{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>Warning</p> <p>ICER is upgrading the HPCC operating system to Ubuntu. Please see our documentation to help you transition to the new operating system.</p>"},{"location":"#what-is-icer","title":"What is ICER?","text":"<p>The Institute for Cyber-Enabled Research (ICER) provides the cyberinfrastructure for researchers from across academia and industry to perform their computational research. ICER supports multidisciplinary research in all facets of computational sciences. ICER continually works to enhance MSU\u2019s national and international presence and competitive edge in disciplines and research that rely on advanced computing. The High Performance Computing Center (HPCC) is ICER's computational facility.</p>"},{"location":"#getting-access-to-the-hpcc","title":"Getting Access to the HPCC","text":"<p>For potential users with an MSU NetID, accounts must be requested by a MSU tenure-track faculty member. Researchers at partner institutions should use the mechanism specified by their institution's  agreement with MSU. For more information, see: Obtain an HPCC Account.</p>"},{"location":"#new-the-data-machine","title":"NEW! The Data Machine","text":"<p>ICER will soon be offering access to a new resource for data-intensive computing for use in research and in the classroom. We are seeking users willing to experiment with the machine and offer feedback. Read more here.</p>"},{"location":"#cpu-and-gpu-time-limits","title":"CPU and GPU Time Limits","text":"<p>Non-buyin users are limited to\u00a0500,000 CPU hours (30,000,000 minutes) and 10,000\u00a0GPU hours (600,000 minutes)\u00a0every year (from January 1st to December 31st). More information is available at Job Policies.</p>"},{"location":"#buy-in-options","title":"Buy-in Options","text":"<p>With each cluster purchase, ICER offers researchers the oppportunity to purchase buy-in nodes for priority access to the cluster.</p>"},{"location":"#questions","title":"Questions?","text":"<p>If you have questions please check our Frequently Asked Questions or use the search bar above. If you still can't find answers in this documentation, please submit a ticket.</p>"},{"location":"#online-helpdesk-hours","title":"Online Helpdesk Hours","text":"<p>Monday and Thursday, 1-2pm at our ICER Public Help Desk Channel . More information about virtual support is available.</p>"},{"location":"#hpcc-workshops-and-training","title":"HPCC Workshops and Training","text":"<p>Our monthly workshops and our D2L training materials cover introductions to Linux, HPCC, and some popular software tools. Check out our training calendar for scheduled events and our course content available on Desire2Learn.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>We encourage HPCC users to acknowledge ICER and MSU in publications based on work that was done with HPCC resources. A sample statement:\u00a0\"This work was supported in part through computational resources and services provided by the Institute for Cyber-Enabled Research at Michigan State University.\" </p> <p>Let us know that we have been referenced, and we will link to your publication on our\u00a0publication site,\u00a0which will further increase the visibility of your work.</p>"},{"location":"2022-06-27_LabNotebook_AntiSmash_installation/","title":"(2022-06-27) Lab Notebook: AntiSMASH installation with Conda","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Conda","AntiSMASH"]},{"location":"2022-06-27_LabNotebook_AntiSmash_installation/#lab-notebook-installing-antismash-on-hpcc-using-conda-2022-06-27","title":"Lab Notebook --- Installing AntiSMASH on HPCC using Conda (2022-06-27)","text":"<p>AntiSMASH (The antibiotics &amp; Secondary Metabolite Analysis Shell) is bioinformatics program for identifying genes belonging to secondary metabolite pathways, particularly in plant, fungi and bacteria species. Documentation for AntiSMASH can be found at https://docs.antismash.secondarymetabolites.org, but you will need to follow the instruction below to install AntiSMASH in your home directory on HPCC.</p> <p>Small analyses can be done through their webportal (see https://docs.antismash.secondarymetabolites.org/website_submission/), however if your analysis requires running AntiSMASH on the HPCC, please see the instruction below for how to install this program localy using Anaconda</p> <p>If you have not installed Anaconda in you home directory, see https://docs.icer.msu.edu/Using_conda/</p> <p>Installing AnitSMASH</p> <pre><code># Clear modules and load Anaconda\nmodule purge\nmodule load Conda/3\n\n# Add biconda to your Anaconda install\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\n\n# Create a cona environemnt for antimash and its depdencies\nconda create -n antismash\nconda activate antismash \n\n# As of version 6.1, you need to install each of these \n# programs first before install antismash or it never resolves\n# Seems to be an issue with the conda recipie, not AntiSMASH itself\nconda install hmmer2\nconda install hmmer\nconda install diamond\nconda install fasttree\nconda install prodigal\nconda install blast\nconda install muscle\nconda install glimmerhmm\n\n# Once the above installs have complete, this command will\n# install antismash and update the versions of the above programs # as needed. \n# \n# You may be asked to 'DOWNGRADE' diamond, hmmer, muslce, \n# and some perl libraries; this is normal \nconda install antismash\n\n# Test your install\nantismash --check-prereqs\nantismash --help\n\n# Download databases\ndownload-antismash-databases\n</code></pre> <p>Once you have completed the above steps, to run AntiSMASH in the future, do:</p> <pre><code>module purge\nmodule load Conda/3\n\nconda activate antismash\n\n# check\nantismash --help\n</code></pre>","tags":["lab notebook","Conda","AntiSMASH"]},{"location":"2022-09-20_LabNotebook_Pymesh_installation_with_conda/","title":"(2022-09-20) Lab Notebook: PyMesh installation with Conda","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Conda","PyMesh"]},{"location":"2022-09-20_LabNotebook_Pymesh_installation_with_conda/#lab-notebook-installing-pymesh-on-hpcc-using-conda-2022-09-20","title":"Lab Notebook --- Installing PyMesh on HPCC using Conda (2022-09-20)","text":"<p>PyMesh is a code base developed by Qingnan Zhou for his PhD research at New York University. It is a rapid prototyping platform focused on geometry processing. PyMesh is written with both C++ and Python, where computational intensive functionalities are realized in C++, and Python is used for creating minimalistic and easy to use interfaces. Documentation for AntiSMASH can be found at https://github.com/PyMesh/PyMesh, but you will need to follow the instruction below to install AntiSMASH in your home directory on HPCC.</p> <p>If you have not installed Anaconda in you home directory, see https://docs.icer.msu.edu/Using_conda/</p> <p>Installing Pymesh</p> <pre><code># Clear modules and load Anaconda\nmodule purge\nmodule load Conda/3\n\n# You need to create a conda environment with an older version of python\nconda create -n pymesh python=3.6\n\n# Activate the environment\nconda activate pymesh\n\n# Use the the 0.2.1 version because the 0.3 version on conda forge required\n# GLIBC 2.18 which is incompatible with the version (2.17) on HPCC (Centos7 thing I think)\nconda install pymesh2=0.2.1\n\n# Once done, test by trying to do 'import pymesh' in python\n</code></pre> <p>Why not Pymesh 0.30</p> <p>The version of Pymesh 0.3 on conda forge produces the following error when I try to import pymesh</p> <pre><code>ImportError: /lib64/libc.so.6: version `GLIBC_2.18' not found (required by /mnt/home/panchyni/anaconda3/envs/pymesh/lib/python3.6/site-packages/pymesh/lib/libstdc++.so.6)\n</code></pre> <p>As far as I understand it, the GLIBC version is tied to the OS, so its not a s simple as importing another library. If for some reason version 0.3 is required, there are a couple of options, though they may be time consuming:</p> <ul> <li>Build PyMesh from source using the files from the github. There are a number of dependencies, so you might want to see if you can use a conda environment to install the dependencies</li> <li>Create a conda environment within an Singularity container of Centos8</li> </ul>","tags":["lab notebook","Conda","PyMesh"]},{"location":"2022-09-21_LabNotebook_VisIt/","title":"(2022-09-21) Lab Notebook: VisIt on HPCC","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","VisIt"]},{"location":"2022-09-21_LabNotebook_VisIt/#lab-notebook-using-visit-on-hpcc-2022-09-21","title":"Lab Notebook --- Using VisIt on HPCC (2022-09-21)","text":"<p>VisIt is an Open Source, interactive, scalable, visualization, animation and analysis tool (see the  VisIt Docs for more detail)</p> <p>Currently, there are two challenges using VisIt on HPCC (1) failed installation due to conflicts with GLIBC and (2) running the GUI without it crashing immediately after loading.</p>","tags":["lab notebook","VisIt"]},{"location":"2022-09-21_LabNotebook_VisIt/#easy-solution-using-an-older-visit","title":"Easy Solution, using an older VisIt","text":"<p>To address the install issues, the easiest solution is to use an older (before 3.x) version of VisIt which can be found here. Use the version titled:</p> <pre><code>Linux - x86_64 64 bit\nRedhat Enterprise Linux 7.5, 4.18.9-1.el7.elrepo.x86_64 #1 SMP, gcc 4.8.5\n</code></pre> <p>Or use the following command on HPCC:</p> <pre><code>wget https://github.com/visit-dav/visit/releases/download/v2.13.3/visit2_13_3.linux-x86_64-rhel7.tar.gz\n</code></pre> <p>Once this file is donwloaded, unpack using tar -xzf and then begin and then begin an Interactive Desktop session via OnDemand. This step is necessary because running VisIt from the command line will cause the GUI interface to crash immediately after loading (tested with XQuartz on Mac and MobXterm on Windows). Instead, after creating an Interactive Desktop,navigate to the unpacked VisIt folder, enter the bin folder within, and click on the fie named 'visit'. This should start VisIt within the Interactive Desktop.</p>","tags":["lab notebook","VisIt"]},{"location":"2022-09-21_LabNotebook_VisIt/#intermediate-solution-using-rhel7-visit-33-with-modules","title":"Intermediate Solution, using RHEL7 Visit 3.3 with modules","text":"<p>Following from the above solution, I realized that the new (as of writing) version of RHEL7 should work on HPCC without the GLIBC issue, but it doesn't out of the box. However, the fix wasn't difficult once I figured out that right module to load on HPCC, though the combinaton with the need to use OnDemand makes things a little clunky.</p> <p>First, get the RHEL7 3.3 version of VisIt with Mesa from their website, or use the following command:</p> <pre><code>wget https://github.com/visit-dav/visit/releases/download/v3.3.0/visit3_3_0.linux-x86_64-rhel7-wmesa.tar.gz\n</code></pre> <p>Once this file is downloaded, unpack using tar -xzf and then begin an Interactive Desktop session via OnDemand. This step is STILL necessary because running this version of VisIt from the command line ALSO will cause the GUI interface to crash though this time it waits until you try to draw something (tested with XQuartz on Mac).</p> <p>Because we need to use some module, once the Interactive Desktop is started, open a terminal and navigate to the folder where VisIt was unpacked and go to the 'bin' directory. There, run the following commands:</p> <pre><code>module load GCCcore/11.2.0\nmodule load PCRE2/10.37\n</code></pre> <p>Then, if you are in the bin folder, you can start Visit using:</p> <pre><code>./visit\n</code></pre> <p>I think this need to be done in the same terminal window for VisIt to start in the right environment (i.e. with the modules loaded)</p>","tags":["lab notebook","VisIt"]},{"location":"2022-09-21_LabNotebook_VisIt/#future-getting-centos8-visit-to-run-on-hpcc","title":"Future: Getting CentOS8 Visit to run on HPCC","text":"<p>I've encountered several problem trying to getting any version VisIt 3.x to work on HPCC (tested 3.0 and the current as of now 3.3), though it turns out this was because I was using the CentOS8 version and RHEL7 is close enough to what we use that the workaround is not bad (see above). Using the prebuilt CentOS8 binary files will not work because they are looking for a version of GLIBC later than version 2.17 which is built into CentOS7. </p> <p>I am looking into this becaue I have encountered this issue before with other easily accessible installs of user requested software (see Pymesh on Conda) and might be a useful method to test software prior to a future OS update on HPCC. As such, I have been using Visit to explore ways to try to install CentOS8 built software on HPCC. Currently, the idea is to use a prebuilt binary on a Singualirty container of Centos8, though when I do this it cannot find the Qt5 library that comes with VisIt or, sometimes, an X11 library(might be able to fix this by messing aroubd with library paths within Singularity)</p>","tags":["lab notebook","VisIt"]},{"location":"2022-09-21_LabNotebook_VisIt/#why-no-just-build-visit-from-source-wouldnt-that-be-easier","title":"Why no just build Visit from source, wouldn't that be easier?","text":"<p>Short answer, no.</p> <p>Long answer, when I try to build VisIt 3.x from source, it fails to build its own version of Qt5 (cannot find the xkbcommon library on the system even though its there) and will not recognize Qt5 on the system. Might be related to the library issues trying to work in Singularity, not sure.</p>","tags":["lab notebook","VisIt"]},{"location":"2022-09-23_LabNotebook_Singularity_Using_an_alternate_OS_/","title":"(2022-09-23) Lab Notebook: Singularity --- Using an alternate OS","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Singularity","CentOS 8"]},{"location":"2022-09-23_LabNotebook_Singularity_Using_an_alternate_OS_/#lab-notebook-using-an-alternate-os-via-singularity-2022-09-23","title":"Lab Notebook --- Using an alternate OS via Singularity (2022-09-23)","text":"<p>Occasionally, I run into a peice of software which needs a newer version of GLIBC (&gt; 2.17) to be installed. Rather than try to upgrade GLIBC (which seems complicated and can go very poorly, see this thread for a general idea), its alot easier to run a different OS image in a container and run the software there. For this, we will use Singularity, which functions similarly to Docker, but is available on HPCC because unlike Docker it doesn't give you root privledges on the host system. Futhremore, its alot easier to work with than a virtual box (used them in the past, they can be fun but a lot of overhead + issues sharing files). Finally, you can run environment managers like Conda with Singularity to layer package management on top of the alternate OS, or do things like work with a downgraded python, all without affecting any system level install.</p> <p>Below, I will go through obtaining a CentOS8 image which has an updated GLIBC, building an overlay, and installing conda in that overlay, and building a conda environment for pymesh (version 0.3) as an example of this process</p> <p>Note</p> <p>This is very brief and direct explanation of installing something in a Singularity image. A more general explanation of Singularity images, overlay, etc will be added in the future (probably as  another Notebook)</p>","tags":["lab notebook","Singularity","CentOS 8"]},{"location":"2022-09-23_LabNotebook_Singularity_Using_an_alternate_OS_/#getting-a-centos8-image","title":"Getting a CentOS8 image","text":"<p>You can pull a Docker built image directly using Singularity. The code below will grab a CentOS8 image with a new GLIBC which is needed from the pymesh 0.3 version on conda.</p> <pre><code># This works for now but\n# Probably want to update/find different OS since centos8 hit end of life at the end of 2021, probably centOS9\n# As of current, most similar OS with GLIBC &gt; 2.17\nsingularity pull centos8.sif docker://centos:8.3.2011\n</code></pre> <p>I keep my images in a folder called \"singularity_pull_images\" so, below replace \"../singularity_pull_images\" with whever your images are kept</p>","tags":["lab notebook","Singularity","CentOS 8"]},{"location":"2022-09-23_LabNotebook_Singularity_Using_an_alternate_OS_/#pymesh-03-in-centos8-container","title":"Pymesh 0.3 in CentOS8 Container","text":"<p>Now what we have an image, we can make an 'overlay' which is basically a 'filesystem inside a singe file'. This both cuts down on the number of files used on your home directory AND help silo off environments/installs/etc that are associated with a particular Singularity image (although this last step isn't really necessary, but it helps). Also, you generally have more acess to the filesystem in the overlay, meaning you can install things outside of your home directory in the overlay.</p> <p>For now, we will make an overlay, install conda in that overlay and then use the overlay to contain all the files for pymesh and its dependencies</p>","tags":["lab notebook","Singularity","CentOS 8"]},{"location":"2022-09-23_LabNotebook_Singularity_Using_an_alternate_OS_/#setup-singularity-container","title":"Setup Singularity Container","text":"<pre><code># The following commands are in powertools, which should be loaded by default; otherwise you will need to run 'module load powertools'\noverlay_build 5 overlay.img\noverlay_install_conda overlay.img https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh ../singularity_pull_images/centos8.sif \n\n# Now we access our overlay within our CentOS8 image\noverlay_start overlay.img ../singularity_pull_images/centos8.sif \n\n# After this, your prompt should be 'Singularity&gt;'\n</code></pre>","tags":["lab notebook","Singularity","CentOS 8"]},{"location":"2022-09-23_LabNotebook_Singularity_Using_an_alternate_OS_/#install-pymesh-within-the-container","title":"Install Pymesh within the container","text":"<p>Now that we are inside of our container, we can install pymesh. Using the CentOS8 image, we shouldn't get a GLIBC error, but we still need an older version of python (CentOS8 uses 3.9, pymesh needs 3.6). So, we will make conda enviroment with an older python (which is alot easier that trying to downgrade the system python of the Singularity image)</p> <pre><code># Need an older version of python for pymesh 0.3\nconda create -n pymesh2 python=3.6 pymesh2=0.3    \n\n# Now, activate our new environmet\nconda init\nsource ~/.bashrc                                  # Need to manually update shell post conda init\nconda activate pymesh2\n\n# After this your prompt should be '(pymesh2) Singularity&gt;'\n</code></pre>","tags":["lab notebook","Singularity","CentOS 8"]},{"location":"2022-09-23_LabNotebook_Singularity_Using_an_alternate_OS_/#check-install-using-nose","title":"Check install using nose","text":"<p>Now, within our conda environment, within our Singualrity container, lets install the nose package (unit testing in python) and run the test from the pymesh github (https://github.com/PyMesh/PyMesh)</p> <pre><code># From within the pymesh2 environment\nconda install nose\npython -c \"import pymesh; pymesh.test()\"\n\n# You should get a couple warning and a couple S (skipped) tests, pretty sure this is fine\n</code></pre>","tags":["lab notebook","Singularity","CentOS 8"]},{"location":"2022-09-23_LabNotebook_Singularity_Using_an_alternate_OS_/#endnote","title":"Endnote","text":"<p>This solution might seem (overly) complicated, because at the end we are running a conda environment inside of Singularity container. However, this allows us to use an older python version (via conda) and an alternative OS/GLIBC (via Singularity), either of which is an issue which is difficult to solve (downgrade python/upgrade GLIBC) and which it is not difficult to really screw things up by attempting</p>","tags":["lab notebook","Singularity","CentOS 8"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/","title":"(2022-09-24) Lab Notebook: Singularity Overlays","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#singularity-overlays-2022-09-24","title":"Singularity Overlays (2022-09-24)","text":"<p>Singularity is a versatile tool to give researchers more flexibility installing software and running their workflows on the HPC.   Most workflows don't need Singularity but it can be extremely helpful to solve certain weridly difficult problems.  Some common examples for researchers using singularity on the HPC include:</p> <ul> <li>Installing software that needs a special/different base operating system.</li> <li>Installing software that requires administrator privileges (aka root, su and/or sudo). </li> <li>Installing complex dependency trees (like python and R)</li> <li>Using existing software inside a pre-built virtual machine.</li> <li>Working with lots of tiny files on the HPC filesystems which are better designed for smaller numbers of big files. </li> <li>Building workflows that can easily move between different resources.</li> </ul> <p>NOTE This overview is specific to the High Performance Computing Center (HPCC) at Michigan State University (MSU).  For a complete tutorial see the Singularity documentation.  This overview assumes that you have an HPCC account and know how to navigate to and use a development node. </p> <p>The remainder of this overview will walk you through the first steps in using Singularity. However, if you want to skip the details just try running the following three powertools commands.  The first two will create a read/writable overlay and install miniconda. The third one will start this overlay inside a CentOS singularity image.  You will only need to run the first two commands once to build the overlay file (with conda), then you can just use the third command anytime you want to start the overlay:</p> <pre><code>overlay_build\noverlay_install_conda\noverlay_start\n</code></pre> <p>To exit singularity just type <code>exit</code>.</p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#step-1-get-a-singularity-image","title":"Step 1: Get a singularity image","text":"<p>As a starting point we need a singularity image, also known as a container or virtual machine.  You can think of a singularity image as a \"software hard drive\" that contains an entire operating system in a file. There are three main ways to get these images:</p> <ol> <li>Use one of the Singularity images already on the HPCC. </li> <li>Download an image form one of the many online libraries.</li> <li>Build your own image.</li> </ol> <p>If you don't know which one of the above to use, I recommend that you pick number 1 and just use the singularity image we already have on the system.</p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#1-use-one-of-the-singularity-images-already-on-the-hpcc","title":"1. Use one of the Singularity images already on the HPCC","text":"<p>For this introduction, we can keep things simple and just use one of the Singularity images already on the HPCC. This image runs CentOS 7 Linux and is a good starting point.  Use the following command to start singularity in a \"shell\" using the provided image:</p> <pre><code>singularity shell --env TERM=vt100 /opt/software/CentOS.container/7.4/bin/centos\n</code></pre> <p>Once you run this command you should see the \"Singularity\" prompt which will look something like the following:</p> <pre><code>Singularity&gt;\n</code></pre> <p>You did it!  You are running a different operating system (OS) than the base operating system.  All of the main HPCC folders are still accessible from this \"container\" (ex. /mnt/home, /mnt/research, /mnt/scratch/, etc) so it shouldn't look much different than before (except for the different prompt and you no longer have access to some of the base HPCC software). </p> <p>At this point, if you know what you need, you should be able use files in your home directory and it will compile/run using the singularity OS instead of the base OS.</p> <p>NOTE: You can just install software in your <code>/mnt/home/$USER</code> and/or <code>/mnt/research</code> folders. The software you install will probably only work from \"inside\" this singularity image. However, you will also be able to see and manipulate the files from within your standard HPC account.  This is fine for many researchers but I recommend you jump down to \"Step 3: Overlays\" to make Singularity even more flexible. </p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#2-download-an-image-form-one-of-the-many-online-libraries","title":"2. Download an image form one of the many online libraries","text":"<p>Many people publish singularity images and post them on public \"libraries\" for easy install.  Here is a list of online libraries you can browse (this section of the tutorial may need more work, not all of these may work on the HPCC):</p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#sylabs-library","title":"Sylabs Library","text":"<p>Link to Browse Sylabs example:</p> <pre><code>singularity pull alpine.sif library://alpine:latest\n\nsingularity shell alpine.sif\n</code></pre>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#docker-hub","title":"Docker Hub","text":"<p>Link to Browse Docker Hub example:</p> <pre><code>singularity pull tensorflow.sif docker://tensorflow/tensorflow:latest\n\nsingularity shell tensorflow.sif\n</code></pre>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#singularity-hub-aka-shub","title":"Singularity Hub (aka shub)","text":"<p>Link to Browse Singularity Hub example:</p> <pre><code>singularity pull shub_image.sif shub://vsoch/singularity-images\n\nsingularity shell shub_image.sif\n</code></pre>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#3-build-your-own-image","title":"3. Build your own image","text":"<p>This one is more complex and outside the scope of the overview. However, if you are interested I recommend you try using the build command with a Docker image since it is fairly easy to install on your personal computer. Here is a link to how to use docker to make a singularity image:</p> <ul> <li>Link to singularity build command</li> </ul>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#step-2-running-commands-in-singularity","title":"Step 2: Running commands in Singularity","text":"<p>In Step 1 we showed you how to start a singularity \"shell\".  You can also just \"execute\" a command inside the singularity image and return the results.  For example, to run </p> <pre><code>singularity exec /opt/software/CentOS.container/7.4/bin/centos &lt;&lt;COMMAND&gt;&gt;\n</code></pre> <p>Where you replace <code>&lt;&lt;COMMAND&gt;&gt;</code> with whatever command you need to run.  This option will become very helpful when you want to run singularity inside a submission script \"See Step 4\" below.</p> <p>For example, the <code>df -hT</code> command will report file system disk space usage. So running the <code>df -hT</code> will give a different result running inside or outside a singularity image. You can test this using the following commands:</p> <pre><code>df -hT\n\nsingularity exec /opt/software/CentOS.container/7.4/bin/centos df -hT\n</code></pre>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#step-3-overlays","title":"Step 3: Overlays","text":"<p>One problem we often encounter on the HPCC is \"lots-of-small-files\" (hundreds of files where each one is &lt; 50MB).  The filesystem is optimized for large files.  Lots of small files end up \"clogging\" things up which can slow things down for everyone.  One useful trick of singularity is you can make a single large file called an \"Overlay\" which can be attached to a singularity session. You can use an Overlay as a \"filesystem inside a single file\" where you can store lots of the small files inside the single overlay file. From the user point of view you can have as many small files as you want accessible from the singularity image (within reasonable limits). However, these small files act as a single file from the HPCC point of view and doesn't clog things up.</p> <p>This technique is really helpful if you are using complex software installs such as lots of Python, R or Conda installs.  It can also be helpful if your research data is lots of small files. </p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#make-your-overlay-file","title":"Make your overlay file","text":"<p>Making an overlay is not hard but takes multiple steps. For details on how to make an overlay we recommend viewing the singularity overlay documentation.</p> <p>Fortunately the HPCC has a \"powertool\" that can make a basic overlay for you.  All you need to do is run the following command:</p> <pre><code>overlay_build\n</code></pre> <p>This overlay can be applied to a singularity image using the <code>--overlay</code> option as follows:</p> <pre><code>singularity shell --overlay overlay.img --env TERM=vt100 /opt/software/CentOS.container/7.4/bin/centos\n</code></pre> <p>If you have an overlay called <code>overlay.img</code> in your current directory you can use the following powertool shortcut to run it inside the CentOS image:</p> <pre><code>overlay_start\n</code></pre> <p>You can also view the amount of filespace available on an overlay (using the <code>df -hT</code> command we used above) by using the following powertool:</p> <pre><code>overlay_size\n</code></pre>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#writing-to-your-overlay","title":"Writing to your overlay","text":"<p>Once you are in the singularity shell you can now write to the overlay as if you were adding files to the \"root\" directory (/).  For example, running the following commands from inside of your singularity image should allow you to install miniconda3: </p> <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.12.0-Linux-x86_64.sh \n\n./Miniconda3-py39_4.12.0-Linux-x86_64.sh -b -p /miniconda3/\n</code></pre> <p>Since we install miniconda3 a lot there is yet another powertool that will do this installation for you. Just run the following command:</p> <pre><code>overlay_install_conda\n</code></pre> <p>Once miniconda is installed in the <code>/miniconda3/</code> directory you need to add the folder <code>/miniconda3/</code> to the path with the following command:</p> <pre><code>export PATH=/miniconda3/bin:$PATH\n\nconda --init\n</code></pre> <p>Or, just use the powertool from before and it will automatically add <code>/miniconda3</code> to your path:</p> <pre><code>overlay_start\n</code></pre> <p>At this point you can use <code>pip</code> and <code>conda</code> installs as much as you like.  These generate hundreds of small files but it doesn't matter because everything will be stored in the overlay.img file as one big file.  </p> <p>To exit singularity just type <code>exit</code>. To start your overlay image just type <code>overlay_start</code></p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#step-4-submitting-jobs","title":"Step 4: Submitting Jobs","text":"<p>Once we have our image and our conda overlay working in a development node we can execute a script inside of the singularity image, \"in batch mode\" using the <code>exec</code> from above. For example, this script uses our miniconda installed overlay and runs the python3 script called \"mypython.py\" which is stored in my home directory on the HPCC.</p> <pre><code>singularity exec --overlay overlay.img /opt/software/CentOS.container/7.4/bin/centos python3 /mnt/home/$USER/mypython.py\n</code></pre> <p>Once the above is running on a development node we can just submit this as a job to the HPCC using the following submissions script:</p> <pre><code>#!/bin/bash\n#SBATCH --walltime=04:00:00\n#SBATCH --mem=5gb\n#SBATCH -c 1\n#SBATCH -N 1\n\nsingularity exec --overlay overlay.img --env PATH=/miniconda3/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/sysbin/ /opt/software/CentOS.container/7.4/bin/centos python3 /mnt/home/$USER/mypython.py\n</code></pre> <p>Again, we have a powertool to help clean this up for common workflows.  Using the <code>overlay_exec</code> command you can simply the above submission script using the following:</p> <pre><code>#!/bin/bash\n#SBATCH --walltime=04:00:00\n#SBATCH --mem=5gb\n#SBATCH -c 1\n#SBATCH -N 1\n\noverlay_exec python3 /mnt/home/$USER/mypython.py\n</code></pre>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#job-arrays","title":"Job Arrays","text":"<p>If you need to have multiple jobs running the same software (such as for a job array). You can't have them all writing to the same overlay file. To resolve this issue, there are two steps:</p> <ul> <li>embedding the overlay into the Singularity image</li> <li>and loading the image as temporarily writable.</li> </ul> <p>As a result, the changes you make to the system files (like <code>/miniconda3/</code>) will not persist after you are finished. You should make sure that any changes you want to keep (like important output files) are stored in a <code>/mnt/</code> directory that's shared with the HPCC, like <code>/mnt/home/$USER</code> or a <code>/mnt/research</code> space.</p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#embedding-the-overlay","title":"Embedding the overlay","text":"<p>First, we will embed the overlay into the image file. This links the image and with a copy of the overlay, so that any time the image is used the overlay copy will be brought along without needing to be specified.</p> <p>To do this, you first need a copy of the image file that you want to use with your overlay. In the case above where you are using the CentOS container that is already on the HPCC, you can create a copy called <code>centos7.sif</code> in your current working directory with</p> <pre><code>singularity build centos7.sif /opt/software/CentOS.container/7.4/bin/centos\n</code></pre> <p>Now, you should create an overlay or use the one created in the steps above:</p> <pre><code>overlay_build\n</code></pre> <p>Note</p> <p>After this step, many of the powertools like <code>overlay_exec</code> and <code>overlay_start</code> will not work correctly since they automatically use the <code>/opt/software/CentOS.container/7.4/bin/centos</code> image. In most cases, you can specify arguments to the powertools to use a desired overlay file or image, but this will not work properly with embedded overlays below.</p> <p>To embed the overlay into the container, you can use the (not very user-friendly) command:</p> <pre><code>singularity sif add --datatype 4 --partfs 2 --parttype 4 --partarch 2 --groupid 1 centos7.sif overlay.img\n</code></pre> <p>If you are using a different container or a different overlay, make sure to change the filenames at the end.</p> <p>Nothing looks any different, but your <code>centos7.sif</code> image will have a copy of the overlay embedded into it. Anything that was installed in the overlay (e.g., a Minicoda installation created using <code>overlay_install_conda</code>) is now available in the image.</p> <p>Note</p> <p>The original <code>overlay.img</code> overlay is now entirely disconnected from the one embedded into the <code>centos7.sif</code> image. Any changes made to the overlay will not be reflected in the image. And likewise, any changes to the overlay embedded in the image will not affect the original overlay file.</p> <p>However, you can always run your image with the original overlay file to ignore the embedded one: <pre><code>singularity shell --overlay overlay.img centos7.sif\n</code></pre></p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#running-the-embedded-overlay","title":"Running the embedded overlay","text":"<p>You can now run your image like normal:</p> <pre><code>singularity shell centos7.sif\n</code></pre> <p>You will see any changes that were in the original <code>overlay.img</code>. But if you try to make a change to a system file, you will get an error:</p> <pre><code>Singularity&gt; mkdir /hello\nmkdir: cannot create directory \u2018/hello\u2019: Read-only file system\n</code></pre> <p>To be able to make changes, you need to start the image with the <code>--writable</code> flag:</p> <pre><code>singularity shell --writable centos7.sif\nSingularity&gt; mkdir /hello\nSingularity&gt; ls -d /hello\n/hello\nSingularity&gt; exit\n</code></pre> <p>If you exit the container and restart it, you will still see the <code>/hello</code> directory: </p> <pre><code>singularity shell centos7.sif\nSingularity&gt; ls -d /hello\n/hello\n</code></pre> <p>However, this means that you still cannot use this container in multiple jobs at once since they will all try to get full access to the embedded overlay.</p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#running-the-embedded-overlay-in-multiple-jobs","title":"Running the embedded overlay in multiple jobs","text":"<p>To fix the problem in the previous section, you need to load the overlay as temporarily read-writable. This loads the filesystem temporarily in a way that multiple jobs can use it at once:</p> <pre><code>singularity shell --writable-tmpfs centos7.sif\nSingularity&gt; mkdir /hello2\nSingularity&gt; ls -d /hello2\n/hello2\nSingularity&gt; exit\n</code></pre> <p>Any changes you make are discarded, so make sure your important files are somewhere accessible on the HPCC like your home or research space.</p> <pre><code>singularity shell --writable-tmpfs centos7.sif\nSingularity&gt; ls -d /hello2\nls: cannot access /hello2: No such file or directory\n</code></pre> <p>In a script with a job array, this might look something like</p> <pre><code>#!/bin/bash\n#SBATCH --walltime=04:00:00\n#SBATCH --mem=5gb\n#SBATCh --array 1-10\n#SBATCH -c 1\n#SBATCH -N 1\n\nsingularity exec --writable-tmpfs centos7.sif python3 /mnt/home/$USER/mypython.py $SLURM_ARRAY_ID\n</code></pre> <p>In this case <code>/mnt/home/$USER/mypython.py</code> should use the <code>$SLURM_ARRAY_ID</code> to do some analysis and write the output to somewhere like <code>/mnt/home/$USER/results</code> so it will be remain after the temporary filesystem is erased.</p> <p>This overview of singularity was initially written by Dirk Colbry.  Please contact the ICER User Support Team if you need any help getting your workflow up and running.</p> <p>link to ICER User Support Team online contact form</p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/","title":"(2022-09-28) Lab Notebook: HPCC via RStudio and GitHub","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p> <p>The following is a tutorial by Wendy Leuenberger (with input from Alex Wright and Erin Zylstra) in the Zipkin Quantitative Ecology Lab.  Wendy wrote this tutorial for her lab group and agreed to share it here in case other groups find it useful.  If you or your team have similar tutorials we would be delighted to highlight them here as an ICER Lab Notebook.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#hpcc-via-rstudio-and-github-2022-09-28","title":"HPCC via RStudio and GitHub (2022-09-28)","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#wendy-leuenberger","title":"Wendy Leuenberger","text":"<p>This document is a guide to using MSU's HPCC. I access the HPCC via the Terminal in RStudio, and I move files from my computer to the HPCC using GitHub. I used the website https://happygitwithr.com/ to set up this system. I reference pages of that guide for instructions or additional reading. This method takes a decent amount of setting up but is convenient once in place.</p> <p>Alex Wright and Erin Zylstra contributed to the sections about using the HPCC.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#software-and-account-requirements","title":"Software and Account Requirements","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#rstudio","title":"RStudio","text":"<p>You need RStudio and R installed on your computer.</p> <p>RStudio: https://www.rstudio.com/products/rstudio/download/.</p> <p>R: https://cran.r-project.org/bin/windows/base/</p> <p>Additional reading on versions: https://happygitwithr.com/install-r-rstudio.html</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#git","title":"Git","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#github-account","title":"GitHub Account","text":"<p>You need a GitHub account for this method. You can make one here: https://github.com/. Here's some additional reading on making an account: https://happygitwithr.com/github-acct.html</p> <p>You will need to know your username, password, and the email associated with your account for later steps.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#gitbash","title":"GitBash","text":"<p>Make sure that RStudio is running GitBash when it opens the Terminal. To do so, click Tools -&gt; Global Options -&gt; Terminal -&gt; New terminals open with GitBash. If it doesn't say GitBash, change the drop down menu so that it does. It GitBash is not an option, you'll need to download Git. Here's reading on how to install Git for different operating systems: https://happygitwithr.com/install-git.html</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#check-installation","title":"Check installation","text":"<p>Go to the Terminal in RStudio. It's one of the tabs next to the Console. (Or use key command Shift+Alt+M) Type <code>which git</code> to make sure Git is installed.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#hpcc-set-up","title":"HPCC set up","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#account","title":"Account","text":"<p>You need an account for the HPCC. A PI can request one. Instructions are here: https://icer.msu.edu/users/getting-started</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#sign-in","title":"Sign-in","text":"<p>Make sure you can log in. Go to the Terminal window in RStudio. Type <code>ssh -XY MSUNetID@hpcc.msu.edu</code> and press enter. (Example: I type <code>ssh -XY leuenbe9@hpcc.msu.edu</code>)</p> <p>Type your password and press enter. Sometimes it makes you do it twice.</p> <p>If it doesn't work, contact ICER to see why.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#map-the-drive","title":"Map the drive","text":"<p>This method works best if the HPCC drive is mapped to your computer. Follow the instructions here to do so. If it doesn't work or you get weird results, contact ICER. https://docs.icer.msu.edu/Mapping_HPC_drives_with_SSHFS/</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#github-and-rstudio","title":"GitHub and RStudio","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#git-configuration","title":"Git configuration","text":"<p>Introduce yourself to Git using your GitHub user name and the email used for your GitHub account (may or may not be your MSU one). Use the Terminal in RStudio or the R package <code>usethis</code> to do so.</p> <p>Using the Terminal:</p> <p><code>git config --global user.name 'Jane Doe'</code></p> <p><code>git config --global user.email 'jane@example.com'</code></p> <p><code>git config --global --list</code></p> <p>Using <code>usethis</code>:</p> <p><code>## install if needed (do this exactly once):</code></p> <p><code>## install.packages(\"usethis\")</code></p> <p><code>library(usethis)</code></p> <p><code>use_git_config(user.name = \"Jane Doe\", user.email = \"jane@example.org\")</code></p> <p>For further reading: https://happygitwithr.com/hello-git.html. If you want GitHub to use 'main' instead of 'master' for the branch names, there is code for that on this page.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#personal-access-tokens","title":"Personal Access Tokens","text":"<p>Personal Access Tokens (PAT) are the way that GitHub authenticates your computer's identity. You need one for each computer that you work from. I use https tokens, though there are also ssh tokens. Read here and the following chapter if you want more information (There's a ton of information on this step): https://happygitwithr.com/https-pat.html</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#create-token","title":"Create token","text":"<p><code>usethis::create_github_token()</code> This line of code will open GitHub and begin the process of generating a token. You will need your password. The default settings that open up for the token have been fine by me so far. I usually name the token 'MSU laptop 9/22' or something similar to designate which computer it's for. They're easy enough to create that I leave the default 30 day expiration date, but you can change it if you wish. Click <code>Generate token</code>. Copy the string of numbers and characters that comes up. It'll be something like <code>ghp_LettersNumbersHere231</code>. This is the token.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#save-token","title":"Save token","text":"<p>Install the <code>gitcreds</code> package if necessary (<code>install.packages('gitcreds')</code>). Run <code>gitcreds::gitcreds_set()</code>. Follow the prompts. If this is your first token, I think it just asks you to enter the token. Paste the token when prompted. If you already have an active token, select 2 when prompted to replace the token, and enter the new token</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#github-repository","title":"GitHub Repository","text":"<p>There are some good chapters on testing out repositories on the web guide. I'm going to skip them so that this set-up is more streamlined. We may have to revisit them if we run into any trouble. Here's the chapters in question: https://happygitwithr.com/push-pull-github.html and https://happygitwithr.com/rstudio-git-github.html</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#create-a-repo","title":"Create a repo","text":"<p>We're going to create a new repository for a project. This is the easiest way to set it up. Here's the detailed reading: https://happygitwithr.com/new-github-first.html.</p> <p>There are also instructions for existing projects, either with GitHub first https://happygitwithr.com/existing-github-first.html or last https://happygitwithr.com/existing-github-last.html</p> <p>Go to your GitHub page. Click <code>New</code> next to your repositories, or on your profile, navigate to Repositories and then click <code>New</code>. Give your repo a name ('testrepo'). All other defaults are fine to start. You can add a description and a readme file if you want. The readme files are needed for the Zipkin lab and they can help keep things organized, but they're not required. If you work with proprietary data, you need to make sure the repo is set to private. I usually set my repos to private unless they're complete and ready to be shared. Once you're done with the settings, click <code>Create repository</code>.</p> <p>Click <code>Code</code> on the repo page. Copy the https URL (example: <code>https://github.com/wleuenberger/test-repo.git</code>)</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#link-github-and-rstudio","title":"Link GitHub and RStudio","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#create-rstudio-project-based-on-github","title":"Create RStudio project based on GitHub","text":"<p>There are many different ways to do this. I create RStudio projects for each of my Repos. I create one using the RStudio GUI. Click File -&gt; New Project -&gt; Version Control -&gt; Git. Enter the https URL for the repo. Click Browse to choose where to save the repository and R project file. I have a folder on OneDrive for GitHub projects. I also have some in other folders for research. As long as you can find it again, it's fine. Click Create Project.</p> <p>Additonal approaches and information is in the same chapter as creating repos: https://happygitwithr.com/new-github-first.html</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#github-via-rstudio-terminal","title":"GitHub via RStudio terminal","text":"<p>I find the command line really helpful for the RStudio workflow. There's only a couple of commands that are needed. There is also a Git GUI built into RStudio if you prefer that approach.</p> <p>In your R project that is linked to your GitHub repository, navigate to the Terminal (separate tab next to the Console, or Alt+Shift+M). Type <code>git status</code>. The output will show you tracked files (ones that have been uploaded to Git in a previous version but have changes) and untracked files (not on GitHub yet). Files uploaded to GitHub and haven't been changed don't show up. This output is a helpful summary to see what files might need attention. I usually do this first.</p> <p>Before you start making any changes, type <code>git pull</code>. I've found that it's helpful to do that first as a habit to prevent any merge problems if someone else on a shared repo uploaded something, or if you have files on the HPCC that got changed.</p> <p>Add a file to the folder on your computer that is linked to your repo. I created an R code file called <code>AddMe.R</code></p> <p>Go back to your Terminal in RStudio. Type <code>git status</code>. You can now see your new file in the Untracked files section. Type <code>git add AddMe.R</code> (or whatever you called your file). Type <code>git status</code>. Now <code>AddMe.R</code> is in the section of Changes to be committed. You've told GitHub that you want this file, but it's not ready yet. All changes to your repos have to be committed and then pushed to get them on the repo.</p> <p>Type <code>git commit -m \"Add my first file to GitHub\"</code>. Commits are kinda like a save point. These can be incremental points and updates. For example, if I get one new piece of code working, I'll commit it with a note of what part got updated. You have to commit before you can upload. Type <code>git status</code> and you can see that your local branch is ahead of your GitHub repo by one commit.</p> <p>To upload, type <code>git push</code>. This will upload the file to GitHub. You can see online that it's now present. Type <code>git status</code>. Your local computer is now up to date with the main GitHub.</p> <p>If you make a change from another source (the HPCC, or if a collaborator makes a change), then you'll want to pull those changes with <code>git pull</code>.</p> <p>These are pretty much all the functions I do with GitHub. There is more reading online https://happygitwithr.com/git-commands.html, as well as other chapters about how Git/GitHub work.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#three-terminals","title":"Three Terminals","text":"<p>All of these pieces result in being able to do everything from the Terminal in RStudio. You can open multiple terminals. Once you've navigated to the Terminal, there's a dropdown menu right under the word 'Terminal' that is probably labeled 'Terminal 1'. Use the 'New Terminal' option to open two additional Terminal windows.</p> <p>Terminal 1 saves and uploads files, Terminal 3 downloads files, and Terminal 2 is where you run the files on the HPCC.</p> <p>Terminal 1 is where you save and upload files to your repository. This Terminal's working directory should be your GitHub Repository. This on should already be in the correct place if you're working from an RStudio project that's linked to your GitHub. You can rename this Terminal if you want. Mine is labeled 'Laptop'.</p> <p>Terminal 2 is your HPCC account. This is where you run your code. I label this one 'HPCC'. On this terminal, log into your HPCC account with the <code>ssh -XY MSUNetID@hpcc.msu.edu</code> and your password. To do work on the HPCC, you need to navigate to a development node (ex. <code>dev-intel18</code>). There are multiple. It doesn't matter which one you use. I always use one with low current usage. Type <code>ssh dev-intel18</code> (or whichever <code>dev-intel</code> you want) to enter the node.</p> <p>Terminal 3 is your mapped HPCC drive. This is where you download your files from your repository. I have this one labeled 'Y Drive'. If you are off campus/not wired in, you need to be remotely connected to the BIG-IP Edge Client. You also need to be logged into your HPCC account. Find your mapped drive in your files and click on it. You may need to enter your password again. Then go to your third terminal Type <code>cd /y</code> (or whatever letter you picked) to set your mapped drive as the working directory for this terminal. This terminal window can be a bit slower than your personal computer.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#get-github-repo-on-the-mapped-drive","title":"Get GitHub Repo on the mapped drive","text":"<p>The repo needs to be on your mapped drive/HPCC account. You can copy and paste it into your mapped drive. Or you can add it using the <code>git clone</code> command. Type <code>git clone</code> and the https URL for that repo (ex <code>git clone https://github.com/wleuenberger/test-repo.git</code>). Check that it's working via the <code>git status</code> command.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#implementing-this-method-and-using-the-hpcc","title":"Implementing this method and using the HPCC","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#navigate-to-your-repository","title":"Navigate to your repository","text":"<p>Make sure that you are in your repository on all three terminals. Use the <code>cd</code> command to do. Just <code>cd</code> will put you at the base of your file structure (i.e.\u00a0the <code>c/</code> drive). <code>cd ..</code> will move you up one folder. <code>ls</code> shows you the contents of your current working directory. <code>cd test-repo/</code> would let me enter the test-repo repository if that were a folder in my current working directory. <code>cd GitHub/test-repo/</code> would enter the GitHub folder and then the test-repo Repository within the GitHub folder. <code>cd y/</code> would put me on the Y drive while <code>cd c/</code> would put me on the C drive. If <code>cd y/</code> (or whatever letter you used for your mapped drive) doesn't work, make sure you are logged into the HPCC, the BIG-IP Edge Client if necessary (if on WiFi), and can open the Y drive in your finder window.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#push-and-pull","title":"Push and pull","text":"<p>I do all of my changes from my computer itself. I regularly commit my changes and push whenever I want to move files to the HPCC. Navigate to the mapped drive terminal and pull those changes. Then switch to the HPCC and submit the jobs using the sbatch files (see Lab-Resources GitHub) or otherwise use the files on the HPCC. You then add, commit, and push any model output from the HPCC back to your computer.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#necessary-files","title":"Necessary files","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#r-code","title":"R code","text":"<p>If you are using R, do not set a working directory in your code. Your working directory can be set in the <code>.sb</code> file or can be the directory that you submit the job from.</p> <p>Make sure that you save any output from your code. You can save R objects as <code>.RData</code>. Here's an example of saving the <code>Out</code> object to the <code>Output</code> folder as <code>Out.RData</code></p> <p><code>save(list = Out, file = file.path('Output', 'Out.RData')))</code></p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#data-and-models","title":"Data and models","text":"<p>Any data you use in your R code must be on the HPCC. It can be in your working directory or in a subfolder. The file path must be given when uploading it if it's not in the working directory. Any <code>.stan</code> text files or other files that are referenced must also be available</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#sb-file","title":"<code>.sb</code> file","text":"<p>The sbatch file contains information needed to submit a job. Here is an example for a model run with Stan. You will need to adapt, and some programs (JAGS) require different settings. If you are running code on the development node and not submitting a job, you don't need this file.</p> <p><code>#!/bin/bash --login</code></p> <p><code># how long? Can specify up to 7 days in hours</code>\\ <code>#SBATCH --time=168:00:00</code></p> <p><code># how much memory?</code>\\ <code>#SBATCH --mem=20G</code></p> <p><code># specify nodes needed.</code>\\ <code>#SBATCH --ntasks=1</code></p> <p><code># specify CPUs (or cores per task)</code>\\ <code># Match this to your number of chains</code>\\ <code>#SBATCH --cpus-per-task=3</code></p> <p><code># email me</code>\\ <code>#SBATCH --mail-type=FAIL,END</code>\\ <code>#SBATCH --mail-user=leuenbe9@msu.edu</code></p> <p><code># change to current directory or specify file path</code>\\ <code>cd $SLURM_SUBMIT_DIR</code></p> <p><code># export R_LIBS_USER=/mnt/home/leuenbe9/R_Lib/4.0.2-X11-20200622</code></p> <p><code># add necessary modules</code>\\ <code>module purge</code>\\ <code>module load GCC/11.2.0</code>\\ <code>module load OpenMPI/4.1.1</code>\\ <code>module load R/4.1.2</code>\\ <code>R --no-environ</code></p> <p><code># run R commandline with the Rscript command</code>\\ <code>Rscript Code/R/TryModel_ISM.R --vanilla</code></p> <p><code>squeue -l $SLURM_JOB_ID</code></p> <p><code>scontrol show job $SLURM_JOB_ID</code></p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#line-endings","title":"Line endings","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#gitattributes","title":".gitattributes","text":"<p>The HPCC requires LF line breaks to run. It cannot run with the standard Windows line breaks (CRLF). You can change each file through command line, but it's often easier to tell git to change it for you. To do so, create a text file call <code>.gitattributes</code>. Set a default, and then state which file types need to be converted. A * indicates any text file, <code>*.R</code> means that any file with the extension <code>.R</code> will be converted to LF line endings when uploaded to GitHub. It won't change the original file on your computer. You will need this <code>.gitattributes</code> file in every repository.</p> <p>Here's example text for the <code>.gitattributes</code> file:</p> <p><code># Set the default behavior, in case people don't have core.autocrlf set.</code>\\ <code>* text=auto</code></p> <p><code># Declare files that should always have LF line endings once uploaded</code>\\ <code># Customize for which types of files you need</code>\\ <code>*.stan text eol=lf</code>\\ <code>*.R text eol=lf</code>\\ <code>*.sb text eol=lf</code></p> <p>Save this file in your repository. Then use <code>git add</code>, <code>git commit</code>, and <code>git push</code> to upload it to your GitHub.</p> <p>Here's additional information and ways to set it globally: https://docs.github.com/en/get-started/getting-started-with-git/configuring-git-to-handle-line-endings</p> <p>I think all you need to do to set it globally is type <code>git config -- global core.autolf</code>. I'm testing it now. We'll see if it works.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#manually-changing","title":"Manually changing","text":"<p>From your working directory, type <code>file *</code>. This command will provide details on each file. You may see <code>with CRLF line terminators</code> on some files. If you do, type <code>dos2unix filename.extension</code> for each file. You can do more than one at once <code>dos2unix filename1.extension filename2.extension</code>.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#running-code","title":"Running code","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#development-node","title":"Development node","text":"<p>If you want to test or troubleshoot, you can run an R on a development node. ( <code>dev-intel16</code>, <code>dev-intel18</code> <code>dev-amd20</code>, accessed by <code>ssh</code> when first logging onto the HPCC) Purge modules and then load the ones you need:</p> <p><code>module purge</code>\\ <code>module load GCC/11.2.0</code>\\ <code>module load OpenMPI/4.1.1</code>\\ <code>module load R/4.1.2</code></p> <p>Then you can either enter R by typing <code>R</code> (type <code>q()</code> to quit), or by running an entire file by typing <code>Rscript FileName.R</code></p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#submit-batch-job","title":"Submit batch job","text":"<p>From your working directory, type <code>sbatch FileName.sb</code> This will submit the job.</p> <p>To check on your job, type <code>sq</code>. It will show as PENDING or RUNNING. If it ran into errors or finished, it will no longer show up.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#out-file","title":".out file","text":"<p>Once your job is running, a <code>SLURM-#######.out</code> file will be created. Don't open it until the job is done. It will contain the output from R and your code, and will have some error messages if the job fails.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#common-errors","title":"Common errors","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#packages-not-loaded","title":"Packages not loaded","text":"<p>Not all R packages or R versions are available on the HPCC. If something you need isn't available, contact ICER via Teams or making a ticket. Include the R version you want to use and the package names. They're pretty fast about getting packages loaded.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#contact-icer-if-needed","title":"Contact ICER if needed","text":"<ol> <li>Office Hours: Monday/Thursday 1-2 PM on Teams (maybe also in     person?)</li> <li>ICER User Manual: https://docs.icer.msu.edu (see     especially the sections labeled: Job Scheduling by SLURM and Job     Management by SLURM)</li> </ol>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-30_LabNotebook_ROS/","title":"(2022-09-30) Lab Notebook: ROS on HPCC","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","ROS"]},{"location":"2022-09-30_LabNotebook_ROS/#the-problem-2022-09-30","title":"The problem (2022-09-30)","text":"<p>The latest version of the Robot Operating System, Humble Hawksbill https://www.openrobotics.org/blog/2022/5/24/ros-2-humble-hawksbill-release, uses a version of <code>libQtCore.so.5</code> that is not compatible with the HPCC Linux kernel version (3.10 as of writing). Specifically, it requires kernel version 3.15+. When the ROS is installed into a Docker container, and used inside a Singularity image with e.g. <code>singularity pull docker://morris2001/humble</code> and <code>singularity run humble_latest.sif</code>, the <code>rqt</code> command inside the Singularity image will result in the error:</p> <pre><code>ImportError: libQt5Core.so.5: cannot open shared object file: No such file or directory\n</code></pre>","tags":["lab notebook","ROS"]},{"location":"2022-09-30_LabNotebook_ROS/#the-solution","title":"The solution","text":"<p>Inside your Dockerfile where you build the ROS container with Ubuntu 22.04, add the command</p> <pre><code>RUN /usr/bin/strip --remove-section=.note.ABI-tag /usr/lib/x86_64-linux-gnu/libQt5Core.so.5\n</code></pre> <p>This is based on discussion here: https://github.com/dnschneid/crouton/wiki/Fix-error-while-loading-shared-libraries:-libQt5Core.so.5</p> <p>This command removes the offending piece of code that is incompatible with the HPCC Linux kernel, and allows <code>rqt</code> to launch successfully.</p>","tags":["lab notebook","ROS"]},{"location":"2022-09-30_LabNotebook_ROS/#opencv","title":"OpenCV","text":"<p>OpenCV is an important Python package for various parts of ROS. Make sure you install it in your Docker container by adding the command</p> <pre><code>RUN apt install libopencv-dev python3-opencv\n</code></pre> <p>to your Dockerfile.</p>","tags":["lab notebook","ROS"]},{"location":"2022-10-03_LabNotebook_Bactopia_installation/","title":"(2022-10-03) Lab Notebook: Bactopia installaion with Conda and Mamba","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Bactopia","Conda"]},{"location":"2022-10-03_LabNotebook_Bactopia_installation/#installing-bactopia-on-the-hpcc-2022-10-03","title":"Installing Bactopia on the HPCC (2022-10-03)","text":"<p>Bactopia is a bacteria genome analysis tool. It can be found at https://bactopia.github.io/v2.1.1/ and has extensive documentation. However, if you follow the instructions for installation on the MSU HPCC you may run into issues with solving the Python environment with Conda.</p>","tags":["lab notebook","Bactopia","Conda"]},{"location":"2022-10-03_LabNotebook_Bactopia_installation/#installation-steps","title":"Installation steps:","text":"<pre><code>module load Conda/3\n</code></pre> <p>Download miniconda https://docs.conda.io/en/latest/miniconda.html</p> <p>Activate the miniconda environment</p> <p>Install mamba</p> <pre><code>conda install -c conda-forge -c bioconda mamba\n</code></pre> <p>Update mamba: </p> <pre><code>conda install -c conda-forge 'mamba&gt;=0.24.*'\n</code></pre> <p>Downloaded and activate bactopia: </p> <pre><code>mamba create -n bactopia -c conda-forge -c bioconda bactopia\nconda activate bactopia\n</code></pre> <p>For further information, see this github issue: https://github.com/bactopia/bactopia/issues/355</p>","tags":["lab notebook","Bactopia","Conda"]},{"location":"2022-10-04_LabNotebook_OnDemand_CantClickInteractiveDesktop/","title":"(2022-10-04) Lab Notebook: OnDemand --- Unable to click icons on the Interactive Desktop","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","OnDemand","interactive desktop"]},{"location":"2022-10-04_LabNotebook_OnDemand_CantClickInteractiveDesktop/#lab-notebook-unable-to-click-icons-on-the-ondemand-interactive-desktop-2022-10-04","title":"Lab Notebook --- Unable to click icons on the OnDemand Interactive Desktop (2022-10-04)","text":"<p>If you start an Interactive Desktop session and find yourself unable to click on any of the icons or buttons on the deskop, you are probably have the 'View Only' option enabled. To fix this, look for icon that looks like a small arrow on the left side of your browser window (see below).</p> <p></p> <p>Click on this arrow to open a menu, which should have 'no VNC' on the top. In this menu, click on the gear symbol (it should be the second from the bottom) to open a setting menu (see below)</p> <p></p> <p>Uncheck the 'View Only' checkbox, and you should now be able to interact with you Desktop again.</p> <p></p> <p>Importantly, OnDemand will remember the whether 'View Only' is checked even if you close out of and reopen the same Interactive Desktop session AND if you close and delete a session and start a new one. If you leave 'View Only' check, close and delete session, and start a new Interactive Desktop session, sometimes your desktop will be confined to the center of browser window (see below)</p> <p></p> <p>To fix this, make sure 'View Only' is unchecked and then toggle 'Remote Resizing' to 'Local Scaling' then back to 'Remote Sizing'</p> <p></p>","tags":["lab notebook","OnDemand","interactive desktop"]},{"location":"2022-10-08_LabNotebook_Praat_Install/","title":"(2022-10-08) Lab Notebook: Praat installation and execution using Singularity Overlays on CentOS8","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Conda","Singularity","OnDemand"]},{"location":"2022-10-08_LabNotebook_Praat_Install/#running-praat-on-hpcc-2022-10-08","title":"Running Praat on HPCC (2022-10-08)","text":"<p>I got Praat working on the system.  It requires a newer version of CentOS and a few tricks but it is working.  To start we need to make a folder with a singularity overlay in it and add conda (for dependencies):</p> <pre><code>mkdir praat\ncd praat\nmodule load powertools\noverlay_build\noverlay_resize 10\noverlay_install_conda\n</code></pre> <p>Next we need to download a CentOS8 image which has the some upgraded libraries that praat needs:</p> <pre><code>singularity pull --arch amd64 library://godloved/secure/centos8:20200805.0.0\nmv centos8_20200805.0.0.sif centos8.sif\n</code></pre> <p>Now we can start singularity with the overlay and install some requirements (actually, mfa is not a requirement but this research team is using it for something else):</p> <p><pre><code>overlay_start overlay.img centos8.sif\nconda install -c conda-forge gtk3\n</code></pre> <pre><code>conda install -c conda-forge pulseaudio\n</code></pre> <pre><code>conda install -c conda-forge montreal-forced-aligner\n</code></pre></p> <p>Finally, we can download praat and copy it to our singularity image:</p> <pre><code>wget https://www.fon.hum.uva.nl/praat/praat6223_linux64.tar.gz\ntar -xzvf praat6223_linux64.tar.gz\nmv ./praat /praat\n</code></pre> <p>That should be it. We now connect to the HPCC using an X11 connection. The easiest way to do that is to start an interactive desktop form OnDemand. Once the desktop is running, open a terminal, change to the praat directory and run the following command:</p> <pre><code>cd praat\nsingularity exec -B /usr/bin/:/sysbin/ \\\n                 --env LD_LIBRARY_PATH=/miniconda3/lib:/.singularity.d/lib \\\n                 --env TERM=vt100 \\\n                 --env PATH=/miniconda3/bin/:/usr/local/sbin:/usr\n/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/sysbin/ \\\n                 --overlay overlay.img centos8.sif /praat\n</code></pre> <p>I also have an OnDemand App working in a basic dev environment but need to do some testing to find an easy way to share overlay images.  </p> <p>This overview of singularity was initially written by Dirk Colbry and is just intended of an example of how to install and run things on the HPCC. Please contact the ICER User Support Team if you need any help getting your workflow up and running.</p> <p>link to ICER User Support Team online contact form</p>","tags":["lab notebook","Conda","Singularity","OnDemand"]},{"location":"2022-10-10_LabNotebook_OnDemand_Interactive_Desktop_error/","title":"(2022-10-10) Lab Notebook: OnDemand Interactive Desktop Error","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","OnDemand"]},{"location":"2022-10-10_LabNotebook_OnDemand_Interactive_Desktop_error/#ondemand-interactive-desktop-error-2022-10-10","title":"OnDemand Interactive Desktop Error (2022-10-10)","text":"<p>A user was having trouble connecting to the Interactive Desktop using OnDemand.  Once OnDemand started the would encounter a blank desktop similar to the following:</p> <p></p> <p>We finnally tracked down the problem to an error someplace in their user dconf file.  To fix the error the user just needs to remove their dconf file using the following command and restart their desktop:</p> <pre><code>rm ~/.config/dconf/user \n</code></pre> <p>We are still not exactly sure how this file got corrupted.  Please give the above a try and open a ticket if you have any trouble (or insight as to the original cause of the problem). </p>","tags":["lab notebook","OnDemand"]},{"location":"2022-10-19_LabNotebook_OpenVDB_installation/","title":"(2022-10-19) Lab Notebook: OpenVDB installaion with Conda","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Conda","OpenVDB"]},{"location":"2022-10-19_LabNotebook_OpenVDB_installation/#lab-notebook-building-openvdb-using-a-conda-environment-2022-10-19","title":"Lab Notebook --- Building OpenVDB using a Conda environment (2022-10-19)","text":"<p>Warning</p> <p>EXTRA WARNING: I'm not even sure I would recommmend this method as of writing. Now that HPCC recognizes the updated (1.21.0) Blosc module all the important modules (Boost, tbb, and blosc) should be available through module load. However, in case that breaks or a conda enviroment is needed, here are the instructions.</p> <p>OpenVDB is an open source C++ library comprising a novel hierarchical data structure and a large suite of tools for the efficient storage and manipulation of sparse volumetric data discretized on three-dimensional grids. For more information see their Github page</p>","tags":["lab notebook","Conda","OpenVDB"]},{"location":"2022-10-19_LabNotebook_OpenVDB_installation/#building-the-library-via-a-conda-enviroment","title":"Building the library via a Conda enviroment","text":"<p>First, to setup you conda environment with the necessary dependencies, do:</p> <pre><code>conda create -n openvdb_dep cmake gcc gxx boost tbb tbb-devel blosc\nconda activate openvdb_dep\n</code></pre> <p>Then, in this environment, run the following commands which are a slight modified the install instructions from https://github.com/AcademySoftwareFoundation/openvdb:</p> <pre><code>git clone https://github.com/AcademySoftwareFoundation/openvdb.git\ncd openvdb\nmkdir build\ncd build\ncmake .. -DZLIB_ROOT=&lt;path_to_your_conda_enviroment&gt; -DCMAKE_INSTALL_PREFIX=&lt;path_to_where_you_want_to_install_openvdb&gt;\nmake -j4 &amp;&amp; make install\n</code></pre> <p>The file paths to the depedencies, <code>&lt;path_to_your_conda_enviroment&gt;</code>, should be something like <code>/mnt/home/your_netid_name/anaconda3/envs/openvdb_dep/</code> </p> <p>The install directory <code>&lt;path_to_where_you_want_to_install_openvdb&gt;</code> can be anywhere in your home directory (but its best to make it a folder called 'openvdb' because make will write the bin, include, and lib folders there), just make sure you know where it is so you can point to it/add it to your path as need.</p>","tags":["lab notebook","Conda","OpenVDB"]},{"location":"2022-10-19_LabNotebook_TractSeg_installation/","title":"(2022-10-19) Lab Notebook: TractSeg installation with Conda","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Conda","TractSeg"]},{"location":"2022-10-19_LabNotebook_TractSeg_installation/#lab-notebook-installing-tractseg-on-hpcc-using-conda-2022-10-19","title":"Lab Notebook --- Installing TractSeg on HPCC using Conda (2022-10-19)","text":"<p>TractSeg is a tool for fast and accurate white matter bundle segmentation from Diffusion MRI (see their GitHub page for further details). The program itself can be installed through pip, but requires a few dependencies, one of which, Mrtrix 3, may appear dauting at first because the link on the TractSeg page takes you to the source build instruction. However, Mrtrix 3 can be installed using Anaconda, which is recommened and enable by the developer (see here). Therefore, we are going to use Anacnoda for the whole install. </p> <p>If you have not installed Anaconda in you home directory, see https://docs.icer.msu.edu/Using_conda/</p> <p>Installing TractSeg</p> <p>The code below will walk you through the process of creating a conda environment of TractSeg and install TactSeg in it:</p> <pre><code># Begin by loading the loading the Conda module:\n\nmodule purge\nmodule load Conda/3\n\n#After that is done, the following command should create a conda enviroment, activate it and install the dependencies:\nconda create -n tractseg\nconda activate tractseg\nconda install -c mrtrix3 mrtrix3\nconda install pytorch\n\n# Now we can install TractSeg with pip\npip install TractSeg\n\n#Finally, to test the install, try to get the help options for TractSeg:\n\nTractSeg --help\n</code></pre> <p>Once you have completed the above steps, to run TractSeg in the future, do:</p> <pre><code>module purge\nmodule load Conda/3\n\nconda activate tractseg\n\n# check\nTractSeg --help\n</code></pre>","tags":["lab notebook","Conda","TractSeg"]},{"location":"2022-10-19_LabNotebook_TractSeg_installation/#install-optional-dependencies","title":"Install Optional Dependencies","text":"<p>TractSeg has a couple extra dependencies which can be installed if needed, but are not necessary. I've tested installing them but not their functionality in TractSeg as I am not sure where pip installs the TractSeg example files nor am I an expert on the software itself.</p>","tags":["lab notebook","Conda","TractSeg"]},{"location":"2022-10-19_LabNotebook_TractSeg_installation/#fsl","title":"FSL","text":"<p>An FSL python install script can be found here with accompanying install information for Linux here. This script WILL try to install to /usr/local, so use the -d option to install in our home directory (i.e., fslinstaller.py -d /mnt/home/your_netid/something/fsl).</p> <p>After finishing the install, you will need to take several setps that depdent on where you installed FSL. So first, let assume that the  pathe to the directory you installed FSL in is something like \"/mnt/home/your_netid/local/fsl\" (replace this with whatever the real path is). To get FSL running you will need to do the following in command line, but PLEASE be careful with the export PATH command:</p> <pre><code>export FSLDIR=/mnt/home/your_netid/local/fsl                  # Set an environment variable for the program\nsource /mnt/home/your_netid/local/fsl/etc/fslconf/fslfsl.sh   # Source the config file\nexport PATH=$PATH:/mnt/home/your_netid/local/fslbin           # Add FSL to your path\n</code></pre> <p>After this you should be able to run FSL from the command line by running 'fsl'</p> <p>If you are going to be using FSL alot, you can adde the above lines to your ~/.bashrc file, but agian, PLEASE be careful.</p>","tags":["lab notebook","Conda","TractSeg"]},{"location":"2022-10-19_LabNotebook_TractSeg_installation/#xvfb","title":"xvfb","text":"<p>Xvfb is a virtual frame buffer for X11 servers. It is library which should be able to bed installed via conda using:</p> <pre><code>conda install xorg-x11-server-xvfb-cos7-x86_64\n</code></pre>","tags":["lab notebook","Conda","TractSeg"]},{"location":"2022-10-27_LabNotebook_SFTP_Mapping_on_HPCC_file_systems/","title":"(2022-10-27) Lab Notebook: SFTP Mapping on HPCC File Systems","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","sftp"]},{"location":"2022-10-27_LabNotebook_SFTP_Mapping_on_HPCC_file_systems/#sftp-mapping-on-hpcc-file-systems-lasted-updated-2022-10-27","title":"SFTP Mapping on HPCC file systems (lasted updated 2022-10-27)","text":"<p>SFTP Net Drive is the software which can map remote HPCC file systems on your local Windows computers via SFTP. (Mac OS is not supported.) Once connected, you can browse and work with files on HPCC as if they were on a hard drive of your local machine. In order words, they do not need to be downloaded and uploaded when users read or modify them using their local computers.</p> <p>HPCC users can use the  download site to get a free version of SFTP Net Drive. Once it is downloaded and executed, users can do its setup. In the main menu (below), you can choose your own <code>Profile</code> name and <code>Drive Letter</code>. Make sure the <code>Server</code> is set to <code>http://rsync.hpcc.msu.edu</code> <code>Username</code> and <code>Password</code> are the same as your HPCC login.</p> <p></p> <p>For more advanced setting, please click on the <code>Advanced...</code> button. Three setting menus: <code>Connection</code>, <code>Protocol</code> and <code>Drive</code> can be modified. For <code>Connection</code>, the value on <code>Port</code> has to be 22. A longer initiation time can be adjusted on <code>Timeout</code>. You can also set up <code>reconnect times</code> in case the connection is dropped and <code>Send keep-alive</code> to prevent disconnection.</p> <p></p> <p>For <code>Protocol</code>, you may just use the default setting:</p> <p></p> <p>For <code>Drive</code>, if you would like, you may set up a different <code>Root folder</code> to start with other than your home folder. You might want to click on <code>Handle case-sensitive filenames</code> since the HPCC file system is case-sensitive. If you would like to show hidden files, you can click on <code>Show files started with dot</code>.</p> <p></p> <p>Once all of them are set, you can click on <code>Connect</code> button in the main menu. If it is successfully connected, the <code>Connect</code> button will become <code>Disconnect</code>:</p> <p></p> <p>Now, open the file explorer and click on <code>This PC</code>. You should find the HPCC file system shown in the <code>Network locations</code> area with the <code>Profile</code> name and the <code>Drive Letter</code> of your input:</p> <p></p>","tags":["lab notebook","sftp"]},{"location":"2022-10-27_LabNotebook_SFTP_Mapping_on_HPCC_file_systems/#mapping-hpcc-drive-with-more-spaces","title":"Mapping HPCC Drive with More Spaces","text":"<p>If you would like to do SFTP mapping with more than one space in HPCC, the better way is to set static links in the <code>Root folder</code> of your setting. For example, you want to map your home space and research space on your local Windows computer. You can set the <code>Root folder</code> to be your home folder as the setup above. Use\u00a0a ssh client to connect to the HPCC or Web Site Access to HPCC to run a command line on a dev node:</p> <pre><code>[username@dev-intel18 ~]$ ln -s /mnt/research/&lt;Research Name&gt; &lt;Research Name&gt;\n</code></pre> <p>where a static link to your research space is set in your home folder. Once it is done, run <code>ls</code> command and the static link <code>&lt;Research Name&gt;</code> should be shown with the light blue color. To access the research space through your local computer, just click on the HPCC drive of your setting in the <code>Network locations</code> area as mentioned in the upper section. Look for the space link <code>&lt;Research Name&gt;</code> and click on it. You can now see the files in the research space. If you would like to map more spaces, such as your scratch spaces or other research spaces, just create more static links in your home folder by the same way.</p>","tags":["lab notebook","sftp"]},{"location":"2022-12-07_LabNotebook_OnDemand_BadRequest/","title":"(2022-12-07) Lab Notebook: OnDemand --- Bad Request from browser","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","OnDemand","Debugging"]},{"location":"2022-12-07_LabNotebook_OnDemand_BadRequest/#bad-request-in-ondemand-2022-12-07","title":"Bad Request in OnDemand (2022-12-07)","text":"<p>Some users are reporting the following error:</p> <p></p> <p>We are not exactly sure about all of the causes of this problem. It seems to be related to MSU authentication, the type of browser and if your computer switches networks.  </p> <p>In any case the problem is easy to fix. You can clear out your browsers cookies and/or just exit the browser and log back in.</p> <p>If you are getting this problem consistantly please feel free to open a ticket with ICER and help us track down and further debug the problem.</p>","tags":["lab notebook","OnDemand","Debugging"]},{"location":"2022-12-20_LabNotebook_Anaconda_Cleaning_out_cache/","title":"(2022-12-20) Lab Notebook: Anaconda --- Cleanning out your cache on HPCC","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Conda","many files","clean up"]},{"location":"2022-12-20_LabNotebook_Anaconda_Cleaning_out_cache/#lab-notebook-cleanning-out-your-anaconda-cache-on-hpcc-2022-12-20","title":"Lab Notebook --- Cleanning out your Anaconda Cache on HPCC (2022-12-20)","text":"<p>Warning</p> <p>EXTRA WARNING: This process is MOSTLY safe in that the deafult behavior of conda is to hardlink from enviroments to the packages, so if packages are removed from the conda cache, it will not affect the environment. However, if you have enable softlinking with conda (by setting  allow_softlinks or always_softlinks in your config) or manually soflinked to files/folders in the packages directory, this WILL break those links. For a discussion of why this process is MOSTLY safe, see the following links:</p> <ul> <li>\"Is it safe to manually delete all files in pkgs folder in anaconda python?\" (Stackoverflow)</li> <li>\"Can I delete files in the pkgs directory after installation.\" (Anaconda Google Group)</li> </ul> <p>Anaconda often creates many small files contributing to user's file quota on HPCC. A more permenant solution for this problem can be found using Singularity overlays, but as of the time of writing, Sigularity overlays remains something of a work in progress. Hence, this guide is meant to provide an interm solution by outling ways of cleaning out some of the extraneous files produced by Anaconda.</p>","tags":["lab notebook","Conda","many files","clean up"]},{"location":"2022-12-20_LabNotebook_Anaconda_Cleaning_out_cache/#conda-clean","title":"Conda clean","text":"<p>The command 'conda clean' can be used to clean out uneeded packages in the 'pkgs' cachce folder. Full options for this command can be found here, but the simplest options are to either remove all unused packages with:</p> <p><code>conda clean -p</code></p> <p>Or remove ALL packages with:</p> <p><code>conda clean -a</code></p> <p>Again, this should have no affect on hard-link enviroments installed under normal circumstances, however if there is any doubts consider making a backup of existing, important python environments as .yaml files which can be used to reinstalling enviroments or other options to reduce file count such as archiving old/unsued files and directories.</p>","tags":["lab notebook","Conda","many files","clean up"]},{"location":"2022-12-20_LabNotebook_SSHFS_Mapping_on_Windows/","title":"(2022-12-20) Lab Notebook: SSHFS mapping on Windows","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Windows","sshfs"]},{"location":"2022-12-20_LabNotebook_SSHFS_Mapping_on_Windows/#lab-notebook-mapping-hpcc-drives-with-sshfs-on-windows-2022-12-20","title":"Lab Notebook --- Mapping HPCC Drives with SSHFS on Windows (2022-12-20)","text":"<p>Warning</p> <p>EXTRA WARNING: This information was removed from the main SSHFS documentation after switching rsync.hpcc.msu.edu over to SSH key authentication. This  process is now much more difficult, less stable, and can no longer support convenient software like SSHFS-Win Manager. As such we no longer plan to  update or support these instructions and they mainly exist for archival purposes and for users who prefer this approach. Users are free to use this  approach and their own risk, but we recommend other drive mapping and file transfer options.</p> <ol> <li> <p>Install the latest stable release of <code>winfsp</code>. The .msi installer package is recommended. WinFsp release list</p> <ol> <li>Full upstream documentation: WinFsp Documentation</li> </ol> <p>Note</p> <p>Administrative access is required</p> </li> <li> <p>Install the latest stable release of SSHFS-Win.The .msi installer package is recommended. SSHFS-Win release list </p> <ol> <li>Full upstream documentation: SSHFS-Win Documentation</li> </ol> <p>Note</p> <p>Administrative access is required</p> </li> <li> <p>You will need to generate an authentication key pair by following the directions at SSH Key-Based Authentication. Follow the instructions for Windows as described for uploading your key to HPCC, but additionally you will need to created a folder on your Windows system at C:\\Users\\ called .ssh. Inside this folder use Notepad (NOT Office) to create a file named id_rsa.pub and copy your public key from the MobaKeyGen window into that file. Finally, go to the \"Conversions\" tab in the MobaKeyGene windows and click \"Export OpenSSH Key\" as save that as id_rsa in the same folder (C:\\Users\\.ssh). DO NOT use the \"Save public/private key\" buttons to save the keys in C:\\Users\\.ssh as these will NOT save the key in the proper format for SSHFS.  <li> <p>Mounting of Home and Scratch can be done through the command prompt or with the Windows File Explorer once WinFsp and SSHFS-Win are installed.</p> <ol> <li>Mounting Home directory with the command prompt. Note the '.k' after sshfs (<code>\\\\sshfs.k\\</code>) indicates key authentication should be used<ul> <li>DriveLetter (H: or Z: etc.) is optional, but username is required </li> <li>Command structure <code>C:\\&gt;net use [DriveLetter:] \\\\sshfs.k\\&lt;username&gt;@rsync.hpcc.msu.edu</code></li> <li>example <code>C:\\&gt; net use F: \\\\sshfs.k\\ryanjos2@rsync.hpcc.msu.edu</code></li> </ul> </li> <li>Mounting Scratch directory with the command prompt requires a slightly different command.  Note the addition of '.r' to sshfs (<code>\\\\sshfs.kr\\</code>) that designates to start from the root directory.<ul> <li>DriveLetter (H: or Z: etc.) is optional, but username is required </li> <li>Command structure <code>C:\\&gt;net use [DriveLetter:] \\\\sshfs.kr\\&lt;username&gt;@rsync.hpcc.msu.edu\\mnt\\gs21\\scratch\\&lt;username&gt;</code></li> <li>example <code>C:\\&gt;net use R: \\\\sshfs.kr\\ryanjos2@rsync.hpcc.msu.edu\\mnt\\gs21\\scratch\\ryanjos2</code></li> </ul> </li> <li>Mounting Home or Scratch directory with Microsoft file browser:<ul> <li>Both Home and Scratch use the same steps, just with different network paths<ul> <li>Home: <code>\\\\sshfs.k\\username@rsync.hpcc.msu.edu</code></li> <li>Scratch:  <code>\\\\sshfs.kr\\username@rsync.hpcc.msu.edu\\mnt\\gs21\\scratch\\username</code></li> </ul> </li> <li>In file browser on the 'This PC' page, select 'Map Network Drive' </li> <li>Fill in the appropriate network path (<code>\\\\sshfs.k\\username@rsync.hpcc.msu.edu</code>), select a drive letter and any desired options, and click Finish. </li> </ul> </li> </ol> </li> <li> <p>Mounting a research space required an additional tool, the SSHFS-Win Manager GUI.  However, SSHFS-Win Manager GUI does not work properly with SSH key authentication so this is not longer supported with SSHFS.</p> </li>","tags":["lab notebook","Windows","sshfs"]},{"location":"2023-02-07_LabNotebook_NAMD_on_multiple_nodes/","title":"(2023-02-07) Lab Notebook:  NAMD --- Running on Multile Nodes","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","NAMD"]},{"location":"2023-02-07_LabNotebook_NAMD_on_multiple_nodes/#running-namd-across-multiple-nodes-2023-02-07","title":"Running NAMD across multiple nodes (2023-02-07)","text":"<p>NAMD is a parallel molecular dynamics code. At the time of writing we have 3  major versions installed on the HPCC. This guide is for running NAMD across multiple compute nodes.</p>","tags":["lab notebook","NAMD"]},{"location":"2023-02-07_LabNotebook_NAMD_on_multiple_nodes/#recommended-script-setup","title":"Recommended script setup","text":"<p>Below is a sbatch script that should allow NAMD to be run across multiple nodes.  Note that items in &lt;&gt; should be replaced with appropriate values. </p> <p>Especially note that <code>&lt;NUMBER OF NODES&gt; * &lt;NUMBER OF CPUS&gt;</code> should be  entered as one number e.g. 5 nodes * 50 CPUs = 200. This script assumes one task per core.</p> <pre><code>#!/bin/bash --login\n#SBATCH --nodes=&lt;NUMBER OF NODES&gt;\n#SBATCH --ntasks-per-node=&lt;NUMBER OF CPUS&gt;\n#SBATCH --time=&lt;WALLTIME&gt;\n#SBATCH --mem=&lt;MEMORY REQUEST&gt;\n#SBATCH --output=namd.sb.o%j\n#SBATCH --error=namd.sb.e%j\n\nmodule purge\nmodule load GCC/9.3.0 OpenMPI/4.0.3\nmodule load NAMD/2.14-mpi\n\ncd $SLURM_SUBMIT_DIR\n\ncharmrun +p &lt;NUMBER OF NODES&gt; * &lt;NUMBER OF CPUS&gt; namd2 &lt;CONFIGURATION FILE&gt;\n</code></pre>","tags":["lab notebook","NAMD"]},{"location":"2023-02-07_LabNotebook_WRF_installing_with_EasyBuild/","title":"(2023-02-07) Lab Notebook: WRF installation with EasyBuild","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","WRF","EasyBuild"]},{"location":"2023-02-07_LabNotebook_WRF_installing_with_EasyBuild/#lab-notebook-installing-wrf-on-hpcc-using-easybuild-2023-02-07","title":"Lab Notebook --- Installing WRF on HPCC using EasyBuild (2023-02-07)","text":"","tags":["lab notebook","WRF","EasyBuild"]},{"location":"2023-02-07_LabNotebook_WRF_installing_with_EasyBuild/#the-problem-as-of-easybuild-462","title":"The Problem (as of EasyBuild 4.6.2)","text":"<p>Currently, the most recent versions of WRF (4.1 and later) cannot be successfully built by the most recent version of EasyBuild instlled on HPCC (4.6.2). As best I can tell, although EasyBuild is loads more recent versions of GCC and OpenMPI and uses them to build dependencies such as netCDF, something in WRF itself (I think) is configure so that when attempting to compile tests cases (i.e. \"compile em_b_wave\"), the versions of GCC and OpenMPI the automatically load on HPCC (6.4.0 and 2.1.2 respectively) are used.</p> <p>In my experience, this problem tends to manifiest in one of two ways: either EasyBuild will fail while trying to build one of the test cases due to a mismatch in compiler version with netCDF (\"Fatal Error: Cannot read module file netcdf.mod opened at (1), because it was created by a different version of GNU Fortran\") or elements of the test case will fail to build due to the older GCC complier not recognizing  the flag \"-fallow-argument-mismatch\" (gfortran: error: unrecognized command line option -fallow-argument-mismatch; did you mean -Wlto-type-mismatch?) which is believe is a 10+ flag for GCC (see https://gcc.gnu.org/gcc-10/changes.html). You will also probably see something like:</p> <p>Serial Fortran compiler (mostly for tool generation): which SFC /opt/software/GCCcore/6.4.0/bin/gfortran</p> <p>In the output or log file when trying to compile a test case.</p>","tags":["lab notebook","WRF","EasyBuild"]},{"location":"2023-02-07_LabNotebook_WRF_installing_with_EasyBuild/#current-solution","title":"Current Solution","text":"<p>Currently, the best solution is to use an older version of EasyBuid (4.5.0) which can be loaded via \"module load EasyBuild/4.5.0\". This approach has been tested and works out of the box for WRF 4.1.3, 4.2.2, and 4.3.</p> <p>However, older (3.6.X) versions of WRF may require an even older verion of EasyBuild, but their remains problem with this approach realted to folder accesibility [TBD, consult with Andrew and Xaioge]</p>","tags":["lab notebook","WRF","EasyBuild"]},{"location":"2023-02-07_LabNotebook_WRF_installing_with_EasyBuild/#notes-and-future-considerations","title":"Notes and Future Considerations","text":"<p>I am neither well versed in Fortran nor WRF, so the above is a compilation of my observation of trying to get WRF intalled along with a little reserach along the way. As a Lab Notebook, this primarily serves as a reference point and installing WRF should probably be revisited if and when EasyBuild is updated (4.7.0 was released recently)</p> <p>Update (2-8-2023): Installed EasyBuild 4.7.0, same issues as 4.6.2. need to follow up with additional confgiuration mentioned by Xiaoge</p>","tags":["lab notebook","WRF","EasyBuild"]},{"location":"2023-02-09_LabNotebook_Rclone_onedrive/","title":"(2023-02-09) Lab Notebooks: Rclone --- Example of copying folder from HPCC to MSU OneDrive","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","rclone","files"]},{"location":"2023-02-09_LabNotebook_Rclone_onedrive/#hpcc-to-onedrive-with-rclone-2023-02-09","title":"HPCC to OneDrive with Rclone (2023-02-09)","text":"<p>The documentation to use rclone to copy data from the HPC to their MSU OneDrive account is very complex.   Rclone is a powerful tool with lots of options so it can be frustrating to sift through all of the options to know what you need to do. Since I had never used rclone before I thought I would give it a try and type up what I learned.</p>","tags":["lab notebook","rclone","files"]},{"location":"2023-02-09_LabNotebook_Rclone_onedrive/#step-1-configure-rclone-to-connect-with-onedrive","title":"Step 1: Configure Rclone to connect with OneDrive","text":"<p>First, we need to log on to an interactive desktop using OnDemand. This is required because OneDrive needs to set up an authentication token and it uses a web browser running on the server to do this step.  Just log onto \"http://ondemand.hpcc.msu.edu/\" using your MSU NetID and password.  Then click on \"Interactive apps\" and then click on \"Interactive desktop\"  use the default settings which should be something like the following:</p> <pre><code>Number of hours 4\nNumber of cores per task 1\nAmount of memory 4gb\n</code></pre> <p>Then you can just hit launch and wait for the job to start. You will be taken to a screen with a job ID number and the word \"Queued\" in the upper right corner.  It often doesn't take long the word \"Queued\" will change to \"Starting\" and then \"Running\".  A button will appear that says \"Launch Interactive Desktop\". Click that button and a new window will appear in your browser with the desktop running. </p> <p>Then next step is to run the \"rclone config\" command from a terminal. To get to a terminal go up to the start menu at the stop of the desktop. Click on \"Applications--&gt;System Tools--&gt;Terminal\" and a terminal window will appear.  Type the following command to load rclone in the terminal:</p> <pre><code>module load rclone\n</code></pre> <p>Run the <code>rclone config</code> command (you only need to do this once).  I typed the following:</p> <pre><code>n\nremote\nonedrive\n&lt;&lt;enter&gt;&gt;\n&lt;&lt;enter&gt;&gt;\nn\ny\nonedrive\n0\ny\ny\nq\n</code></pre> <p>During this process a window browser window will pop up which may include a button. Just hit accept. You can close the browser when it says \"Success\". Here is a slightly more detailed breakdown of the command I used when configuring:</p> <pre><code>n (create new remote)\nremote (name of the new remote. We are just calling it remote but you can use whatever name)\nonedrive (storage type of remote. In this case onedrive)\n&lt;enter&gt; (Just using default ClientID of an empty string)\n&lt;enter&gt; (Just using default client_secret of an empty string)\nn (No we don't want to use advanced settings)\ny (Yes we do want to use automatic configuration, this is when the browser window will pop up)\nonedrive (Pick your Onedrive Personal or Bisness account)\n0 (Pick the first onedrive account. This assumes you only have one and it is the first in the list)\ny (Yes we want to use our MSU onedrive which will have a URL similar to https://michiganstate-my.sharepoint.com/personal/userid_msu_edu/Documents)\nq (quit the rclone config)\n</code></pre>","tags":["lab notebook","rclone","files"]},{"location":"2023-02-09_LabNotebook_Rclone_onedrive/#step-2-run-rclone-commands-to-create-and-copy-files-to-onedrive","title":"Step 2: Run Rclone commands to create and copy files to OneDrive","text":"<p>Now that the rclone authentication is configured we can move on to actually using the rclone command. As an example I want to create a directory on my OneDrive (called testdir) and then copy the contents of a directory on the HPC to that directory.  Although I should be able to just run the rclone commands from the terminal I am going to run the command inside a job using the following job script.</p> <pre><code>#!/bin/bash\n#SBATCH --mem=2gb\n#SBATCH --time=4:00:00\nmodule load Rclone\nrclone mkdir remote:testdir\nrclone copy ~/UserCode/colbrydi/see-benchmark/see-segment/Image_data/ remote:testdir\n</code></pre> <p>I named this file \"rclone.sb\" and submitted the job to the cluster using the <code>sbatch rclone.sb</code> command.  Seemed to work for me. Please let us know if you found this helpful.</p> <p>Dirk Colbry Director of User Support Institute for Cyber-Enabled Research  </p>","tags":["lab notebook","rclone","files"]},{"location":"2023-02-16_LabNotebook_Cryosparc_installation/","title":"(2023-02-16) Lab Notebook: CryoSPARC Installation on HPCC","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","CryoSPARC"]},{"location":"2023-02-16_LabNotebook_Cryosparc_installation/#downloading-and-installing-cryosparc-2023-11-22","title":"Downloading and Installing cryoSPARC (2023-11-22)","text":"","tags":["lab notebook","CryoSPARC"]},{"location":"2023-02-16_LabNotebook_Cryosparc_installation/#prepare-for-installation","title":"Prepare for Installation","text":"<ul> <li>Register and obtain the license.  To obtain a License ID for cryoSPARC, go to https://cryosparc.com/download, fill out the form and submit it. Then on approval, you will receive an email with a license ID number. (Store license ID in some safe place in your home space)</li> <li> <p>Log into a development node with GPU on HPCC. (NOTE: Use only dev-amd20-v100 due to the GPU driver version requirement.)</p> </li> <li> <p>Determine where you'd like to install cryoSPARC and create the installation directory.  User should install this software in $HOME or $RESEARCH space. We use CryoSPARC in home directory as the installation directory in this document as an example. <pre><code>mkdir ~/CryoSPARC          # create the installation directory      \ncd ~/CryoSPARC             # go to the install directory\n</code></pre></p> </li> </ul> <p>Note</p> <p>`The installation directory could be any directory under the user's home or research space where use has full access permission. This will be the root directory where all cryoSPARC code and dependencies will be installed.</p>","tags":["lab notebook","CryoSPARC"]},{"location":"2023-02-16_LabNotebook_Cryosparc_installation/#download-software","title":"Download software","text":"<ul> <li>Set environment variable: run <pre><code>export LICENSE_ID=\"&lt;license_id&gt;\"\n</code></pre>     where  is the license ID you received from the registration. <li>Download package to the install directory <pre><code>cd ~/CryoSPARC\ncurl -L https://get.cryosparc.com/download/master-latest/$LICENSE_ID -o cryosparc_master.tar.gz \ncurl -L https://get.cryosparc.com/download/worker-latest/$LICENSE_ID -o cryosparc_worker.tar.gz\n</code></pre></li> <li>Extract the downloaded files:  <pre><code>tar -xf cryosparc_master.tar.gz cryosparc_master             \ntar -xf cryosparc_worker.tar.gz cryosparc_worker\n</code></pre></li> <p>Note</p> <p>After extracting the worker package, you may see a second folder called cryosparc2_worker (note the 2) containing a single version file. This is here for backward compatibility when upgrading from older versions of cryoSPARC and is not applicable for new installations. You may safely delete the cryosparc2_worker</p>","tags":["lab notebook","CryoSPARC"]},{"location":"2023-02-16_LabNotebook_Cryosparc_installation/#installation-of-master","title":"Installation of Master","text":"<ul> <li>Load environment <pre><code>module purge                           # unload previous loaded modules.\nmodule load foss/2022b                 # load the latest compile and dependency.\n</code></pre></li> <li> <p>Master node Installationi <pre><code>cd &lt;dir_master_package&gt;        # go to master package directory.\n\n./install.sh --license $LICENSE_ID \\\n             --hostname &lt;master_hostname&gt; \\\n             --dbpath &lt;db_path&gt; \\\n             --port &lt;port_number&gt; \\\n             [--insecure] \\\n             [--allowroot] \\\n             [--yes] \\\n</code></pre> Example: <pre><code>cd ~/CryoSPARC/cryosparc_master           # go to master package directory.\n./install.sh --license $LICENSE_ID \\\n             --hostname localhost \\\n             --dbpath ~/CryoSPARC/cryoSPARC_database \\\n             --port 45000 \\\n</code></pre></p> </li> <li> <p>Start cryoSPARC: run <pre><code>export CRYOSPARC_FORCE_HOSTNAME=true\nexport CRYOSPARC_MASTER_HOSTNAME=$HOSTNAME\n./bin/cryosparcm start\n</code></pre></p> </li> <li>Create your user account of cryoSPARC: run <pre><code>./bin/cryosparcm createuser --email \"&lt;user email&gt;\" \\\n                      --password \"&lt;user password&gt;\" \\\n                      --username \"&lt;login username&gt;\" \\\n                      --firstname \"&lt;given name&gt;\" \\\n                      --lastname \"&lt;surname&gt;\"\n</code></pre></li> </ul> <p>Note</p> <p>For details of the meaning of the options of above master node installalation steps, See https://guide.cryosparc.com/setup-configuration-and-management/how-to-download-install-and-configure/downloading-and-installing-cryosparc#glossary-reference-1.</p> <p>After completing the above, you are ready to access the user interface.</p> <ul> <li> <p>Access the user interface: navigate your browser to <code>http://&lt;master_hostname&gt;:&lt;port_number&gt;</code></p> <p>If you are physically using the same machine as the master node to interact with the cryoSPARC interface, you can connect to it as: <code>http://localhost:&lt;port_number&gt;</code> See https://guide.cryosparc.com/setup-configuration-and-management/how-to-download-install-and-configure/accessing-cryosparc for more information.</p> </li> </ul>","tags":["lab notebook","CryoSPARC"]},{"location":"2023-02-16_LabNotebook_Cryosparc_installation/#installation-of-worker","title":"Installation of worker:","text":"<ul> <li>Load environment if not yet done it <pre><code>module purge                           # unload previous loaded modules.\nmodule load foss/2022b                 # load the latest compile and dependency.\nmodule load CUDA/12.3.0                # cuda is needed for worker\n</code></pre></li> <li>Worker node Installation</li> </ul> <p><pre><code>cd &lt;install_path&gt; /cryosparc_worker\n\n./install.sh --license $LICENSE_ID \\\n             [--yes]\n</code></pre> Example: using CUDA/12.3.0 module. <pre><code>./install.sh --license $LICENSE_ID\n</code></pre></p> <p>Note</p> <p>For the meaning of the options of worker node installation script, See https://guide.cryosparc.com/setup-configuration-and-management/how-to-download-install-and-configure/downloading-and-installing-cryosparc#worker-installation-glossary-reference.</p> <p>Note</p> <p>Once the master and worker are successfully installed at the dev-node, stop the current cryosparc session using command \" ~/CryoSPARC/cryosparc_master/bin/cryosparcm stop\"</p>","tags":["lab notebook","CryoSPARC"]},{"location":"2023-02-16_LabNotebook_Cryosparc_installation/#start-an-interactive-session-of-cryosparc-using-ondemand","title":"Start an interactive session of CryoSPARC using ondemand","text":"<ul> <li> <p>Request an interactive desktop with the number of GPUS equal to the number of workers. You need to request withthe sufficient resources (CPU, memory, wall time, etc.). Open a terminal on the desktop. </p> </li> <li> <p>Launch cryosparc master:  For the users convenience, create a file named \"cryosparc.sh\" containing commands for setting up environment (shown below). Before launch CryoSPARC, run command \"source cryosparc.sh\" first. <pre><code>#!/bin/bash\n# set up CryoSPARC environment\n#\n\n# Load modules\nmodule purge\nmodule load foss/2022b\n\n# set PATH\nexport PATH=~/cryosparc/cryosparc_master/bin:~/cryosparc/cryosparc_worker/bin:$PATH\nexport CRYOSPARC_FORCE_HOSTNAME=true\nexport CRYOSPARC_MASTER_HOSTNAME=$HOSTNAME\n</code></pre></p> </li> </ul>","tags":["lab notebook","CryoSPARC"]},{"location":"2023-02-16_LabNotebook_Cryosparc_installation/#connect-a-cluster-to-cryosparc","title":"Connect a Cluster to CryoSPARC","text":"<p>Once the cryosparc_worker package is installed, the cluster must be registered with the master process. This requires a template for job submission commands and scripts that the master process will use to submit jobs to the cluster scheduler. To register the cluster, provide cryoSPARC with the following two files and call the cryosparcm cluster connect command: - cluster_info.json - cluster_script.sh</p> <p>The first file (cluster_info.json) contains template strings used to construct cluster commands (e.g., qsub, qstat, qdel etc., or their equivalents for your system). The second file (cluster_script.sh) contains a template string to construct appropriate cluster submission scripts for your system. The jinja2 template engine is used to generate cluster submission/monitoring commands as well as submission scripts for each job.</p> <ul> <li>Create the files  The following fields are required to be defined as template strings in the configuration of a cluster. Examples for SLURM are given; use any command required for your particular cluster scheduler. Note that parameters listed as \"optional\" can be omitted or included with their value as null.</li> </ul> <p>cluster_info.json: <pre><code>name               :  \"cluster1\"\n# string, required\n# Unique name for the cluster to be connected (multiple clusters can be \n# connected)\n\nworker_bin_path    :   \"/path/to/cryosparc_worker/bin/cryosparcw\"\n# string, required\n# Path on cluster nodes to the cryosparcw script\n\ncache_path         :   \"/path/to/local/SSD/on/cluster/node\"\n# string, optional\n# Path on cluster nodes that is a writable location on local SSD on each \n# cluster node. This might be /scratch or similar. This path MUST be the \n# same on all cluster nodes. Note that the installer does not check that \n# this path exists, so make sure it does and is writable. If you plan to \n# use the cluster nodes without SSD, you can omit this field.\n\ncache_reserve_mb   :   10000\n# integer, optional\n# The size (in MB) to initially reserve for the cache on the SSD. This \n# value is 10GB by default, which means cryoSPARC will always leave at\n# least 10GB of space on the SSD free.\n\ncache_quota_mb     :   1000000\n# integer, optional\n# The maximum size (in MB) to use for the cache on the SSD.\n\nsend_cmd_tpl       :   \"{{ command }}\"\n# string, required\n# Used to send a command to be executed by a cluster node (in case the \n# cryosparc master is not able to directly use cluster commands). If your \n# cryosparc master node is able to directly use cluster commands \n# (like qsub etc) then this string can be just \"{{ command }}\"\n\nqsub_cmd_tpl       :   \"sbatch {{ script_path_abs }}\"\n# string, required\n# The command used to submit a job to the cluster, where the job \n# is defined in the cluster script located at {{ script_path_abs }}. This \n# string can also use any of the variables defined in cluster_script.sh \n# that are available when the job is scheduled (e.g., num_gpus, num_cpus, etc.,)\n\nqstat_cmd_tpl      :   \"squeue -j {{ cluster_job_id }}\"\n# string, required\n# Cluster command that will report back the status of cluster job with its id \n# {{ cluster_job_id }}.\n\nqdel_cmd_tpl       :   \"scancel {{ cluster_job_id }}\"\n# string, required\n# Cluter command that will kill and remove {{ cluster_job_id }} from the \n# queue.\n\nqinfo_cmd_tpl      :   \"sinfo --format='%.8N %.6D %.10P %.6T %.14C %.5c %.6z %.7m %.7G %.9d %20E'\"\n# string, required\n# General cluster information command\n\ntransfer_cmd_tpl   :   \"scp {{ src_path }} loginnode:{{ dest_path }}\"\n# string, optional\n# Command that can be used to transfer a file {{ src_path }} on the cryosparc \n# master node to {{ dest_path }} on the cluster nodes. This is used when the \n# master node is remotely updating a cluster worker installation. This is \n# optional; if it is incorrect or omitted, you can manually update the \n# cluster worker installation.\n</code></pre> Along with the above commands, a complete cluster configuration requires a template cluster submission script. The script must send jobs into your cluster scheduler queue and mark them with the appropriate hardware requirements. The cryoSPARC internal scheduler submits jobs with this script as their inputs become ready. The following variables are available for use used within a cluster submission script template. When starting out, example templates may be generated with the commands explained below. <pre><code>{{ script_path_abs }}    # absolute path to the generated submission script\n{{ run_cmd }}            # complete command-line string to run the job\n{{ num_cpu }}            # number of CPUs needed\n{{ num_gpu }}            # number of GPUs needed.\n{{ ram_gb }}             # amount of RAM needed in GB\n{{ job_dir_abs }}        # absolute path to the job directory\n{{ project_dir_abs }}    # absolute path to the project dir\n{{ job_log_path_abs }}   # absolute path to the log file for the job\n{{ worker_bin_path }}    # absolute path to the cryosparc worker command\n{{ run_args }}           # arguments to be passed to cryosparcw run\n{{ project_uid }}        # uid of the project\n{{ job_uid }}            # uid of the job\n{{ job_creator }}        # name of the user that created the job (may contain spaces)\n{{ cryosparc_username }} # cryosparc username of the user that created the job (usually an email)\n</code></pre></p> <p>Note</p> <p>The cryoSPARC scheduler does not assume control over GPU allocation when spawning jobs on a cluster. The number of GPUs required is provided as a template variable. Either your submission script or your cluster scheduler is responsible for assigning GPU device indices to each job spawned based on the provided variable. The cryoSPARC worker processes that use one or more GPUs on a cluster simply use device 0, then 1, then 2, etc. Therefore, the simplest way to correctly allocate GPUs is to set the CUDA_VISIBLE_DEVICES environment variable in your cluster scheduler or submission script. Then device 0 is always the first GPU that a running job must use. </p> <ul> <li> <p>Load script and register the integration.</p> <p>To create or set a configuration for a cluster in cryoSPARC, use the following commands.</p> </li> </ul> <pre><code>cryosparcm cluster example &lt;cluster_type&gt;\n# dumps out config and script template files to current working directory\n# examples are available for pbs and slurm schedulers but others should \n# be very similar\n\ncryosparcm cluster dump &lt;name&gt;\n# dumps out existing config and script to current working directory\n\ncryosparcm cluster connect\n# connects new or updates existing cluster configuration, \n# reading cluster_info.json and cluster_script.sh from the current directory, \n# using the name from cluster_info.json\n\ncryosparcm cluster remove &lt;name&gt;\n# removes a cluster configuration from the scheduler\n</code></pre> <p>Note</p> <p>The command <code>cryosparcm cluster connect</code> attempts reading cluster_info.json and cluster_script.sh from the current working directory.</p>","tags":["lab notebook","CryoSPARC"]},{"location":"2023-02-16_LabNotebook_Cryosparc_installation/#examples-of-cluster_infojson-and-cluster_scriptsh-scripts-for-slurm-on-hpcc","title":"Examples of cluster_info.json and cluster_script.sh scripts for SLURM on HPCC:","text":"<p><code>cluster_info.json</code> <pre><code>{\n\"qdel_cmd_tpl\": \"scancel {{ cluster_job_id }}\",\n\"worker_bin_path\": \"/mnt/home/wangx147/cryoSPARC/cryosparc_worker/bin/cryosparcw\",\n\"title\": \"test_cluster\",\n\"cache_path\": \"/tmp\",\n\"qinfo_cmd_tpl\": \"sinfo --format='%.8N %.6D %.10P %.6T %.14C %.5c %.6z %.7m %.7G %.9d %20E'\",\n\"qsub_cmd_tpl\": \"sbatch {{ script_path_abs }}\",\n\"qstat_cmd_tpl\": \"squeue -j {{ cluster_job_id }}\",\n\"cache_quota_mb\": null,\n\"send_cmd_tpl\": \"{{ command }}\",\n\"cache_reserve_mb\": 10000,\n\"name\": \"test_cluster\"\n}\n</code></pre></p> <p><code>cluster_script.sh</code> <pre><code>#!/bin/bash\n#SBATCH --job-name=cryosparc_{{ project_uid }}_{{ job_uid }}\n#SBATCH --partition=general\n#SBATCH --output={{ job_log_path_abs }}\n#SBATCH --error={{ job_log_path_abs }}\n#SBATCH --nodes=1\n#SBATCH --mem={{ (ram_gb*1000)|int }}M\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task={{ num_cpu }}\n#SBATCH --gres=gpu:{{ num_gpu }}\n#SBATCH --gres-flags=enforce-binding\n\nsrun {{ run_cmd }}\n</code></pre> For more examples, see https://guide.cryosparc.com/setup-configuration-and-management/how-to-download-install-and-configure/cryosparc-cluster-integration-script-examples.</p> <p>Q: Where should these two files be stored?</p> <p>A: Working directory.</p>","tags":["lab notebook","CryoSPARC"]},{"location":"2023-02-27_LabNotebook_HRLDAS/","title":"(2023-02-27) Lab Notebook: HRLDAS","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","HRLDAS"]},{"location":"2023-02-27_LabNotebook_HRLDAS/#hrldas-installation-2023-02-27","title":"HRLDAS installation (2023-02-27)","text":"<p>HRLDAS is the High Resolution Land Data Assimilation System. Its user guide can be found here and its code is located here.</p> <p>To install on ICER, the command sequence is:</p> <p>Clone the repository and submodules <code>git clone --recurse-submodules https://github.com/NCAR/hrldas</code></p> <p>Change to the repository directory  <code>cd ~/hrldas/hrldas</code></p> <p>Configure the setup. For parallel computing, choose option 4. <code>perl configure</code></p> <p>Run these commands in sequence: <pre><code>module purge\nml iccifort/2020.1.217 impi/2019.7.217\nml JasPer/2.0.14\nml netCDF-Fortran/4.5.2\n</code></pre></p> <p>Open the file user_build_options in your favorite editor.</p> <p>In this file, change the following lines:</p> <pre><code>COMPILERF90 = mpiifort\n\nNETCDFMOD = -I/opt/software/netCDF-Fortran/4.5.2-iimpi-2020a/include\n\nNETCDFLIB = -L/opt/software/netCDF-Fortran/4.5.2-iimpi-2020a/lib -lnetcdff -L/opt/software/netCDF/4.7.4-iimpi-2020a/lib64 -L/opt/software/iccifort/2020.1.217/lib/intel64 -lnetcdf -lnetcdf -lm -liomp5 -lpthread\n</code></pre> <p>Run <code>make</code>. The compilation should succeed.</p> <p>Warning</p> <p>This is for Intel MPI compilation. It is likely that other architectures will have performance/instruction issues, especially intel14.</p>","tags":["lab notebook","HRLDAS"]},{"location":"2023-03-14_LabNotebook_Crystal_parallel_submission/","title":"(2023-03-14) Lab Notebook: CRYSTAL23 --- Parallel submission script on the MSU HPCC","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p> <p>This Lab Notebook was adapted from the documentation kindly provided by the Mendoza Group Wiki available at https://sites.google.com/view/mendozagroup/home</p>","tags":["lab notebook","Crystal"]},{"location":"2023-03-14_LabNotebook_Crystal_parallel_submission/#parallel-submission-script-for-crystal23-on-the-msu-hpcc-2023-03-14","title":"Parallel submission script for CRYSTAL23 on the MSU HPCC (2023-03-14)","text":"<p>Usage: Your input file should be called .d12 and be in the same directory as the submission script. The script is submitted by running the command sbatch  <p>Tips: </p> <pre><code>The main parameters you will be changing are the job name (note that it is specified twice in the script), run time (currently set to 30 minutes), nodes/cores (currently 2 nodes, 8 cores per node), memory per cpu, and account (general or mendoza_q). These will depend on the type of job you are running and the amount of resources you would like to allocate.\n\nThere is one #SBATCH line commented out, this is to submit to scavenger queue instead of general or mendoza_q.\n\nThere are two cp lines commented out, these might come in handy to restart calculations.\n</code></pre> <pre><code>#!/bin/bash --login\n#SBATCH --job-name=test\n#SBATCH --account=general\n#SBATCH --time=0-00:30:00\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=8\n#SBATCH --mem-per-cpu=3G\n#SBATCH -C [intel16|intel18]\n\n\nmodule purge\nmodule load CRYSTAL/23\nUCX_TLS=ud,sm,self\n\n\nexport JOB=test\nexport DIR=$SLURM_SUBMIT_DIR\nexport scratch=/mnt/gs21/scratch/$USER/crys23\n\n\nrm -r $scratch/$JOB\nmkdir $scratch/$JOB/\ncp $DIR/$JOB.d12 $scratch/$JOB/INPUT\ncd $scratch/$JOB\n\n\nunset FORT_BUFFERED\nexport I_MPI_ADJUST_BCAST=3\nexport I_MPI_DEBUG=5\nulimit -s unlimited\nexport OMP_NUM_THREADS=1\n\nexport LD_LIBRARY_PATH=/opt/software/UCX/1.12.1-GCCcore-11.3.0/lib64:$LD_LIBRARY_PATH\n\nmpiexec -n $SLURM_NTASKS -genv UCX_TLS ud,sm,self /opt/software/CRYSTAL/23/bin/Pcrystal &gt; $DIR/${JOB}.out 2&gt;&amp;1 $DIR/${JOB}.out\ncp fort.9 ${DIR}/${JOB}.f9\n</code></pre>","tags":["lab notebook","Crystal"]},{"location":"2023-03-14_LabNotebook_VASP/","title":"(2023-03-14) Lab Notebook: VASP","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p> <p>This Lab Notebook was adapted from the documentation kindly provided by the Mendoza Group Wiki available at https://sites.google.com/view/mendozagroup/home</p>","tags":["lab notebook","VASP"]},{"location":"2023-03-14_LabNotebook_VASP/#vasp621-in-hpcc-2023-03-14","title":"VASP/6.2.1 in HPCC (2023-03-14)","text":"<p>If you want an example of all the files needed to run a test calculation, you can download them from here. </p>","tags":["lab notebook","VASP"]},{"location":"2023-03-14_LabNotebook_VASP/#submission-script-parallel-version-hpcc","title":"Submission Script, Parallel version (hpcc)","text":"<p>For a job script example running VASP in parallel, you can use the following script:</p> <pre><code>#!/bin/bash --login\n#SBATCH --ntasks=32\n#SBATCH -N 1-2\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=5G\n#SBATCH --time=7-00:00:00\n#SBATCH -p general-long\n#SBATCH --job-name VASP_parallel\n\nexport JOB=VASP_parallel\nexport DIR=$SLURM_SUBMIT_DIR\nmodule load -* intel/2020b VASP/6.2.1\n</code></pre> <p><code>srun vasp &gt; vasp_new.out</code></p> <p>Besides the memory and the walltime request, you can also change the number of tasks and number of nodes in the first and second #SBATCH lines, respectively.</p>","tags":["lab notebook","VASP"]},{"location":"2023-03-14_LabNotebook_VASP/#submission-script-parallel-version-hpcc-saving-some-files-to-the-scratch","title":"Submission Script, Parallel version (hpcc) - Saving some files to the SCRATCH","text":"<pre><code>#!/bin/bash --login\n#SBATCH --ntasks=32\n#SBATCH -N 1-2\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=5G\n#SBATCH --time=7-00:00:00\n#SBATCH -p general-long\n#SBATCH --job-name IrMn3_test\n#SBATCH --output=job.out  \n\n\nexport JOB=IrMn3_test\nexport DIR=$SLURM_SUBMIT_DIR\nexport scratch=$SCRATCH/vasp\n\n\nml -* intel/2020b VASP/6.2.1\necho \"scratch job directory: \" \necho $scratch/$JOB             \n\n\nmkdir  -p $scratch/$JOB    \ncp $DIR/*  $scratch/$JOB/  \ncd $scratch/$JOB           \n\n\necho -e \"slurm submission directory:\\n$SLURM_SUBMIT_DIR\" &gt; SUBMITDIR  \nsrun vasp &gt; vasp.out\nsucceed=$?\ncp OUTCAR ${DIR}/OUTCAR        \ncp CONTCAR ${DIR}/CONTCAR      \ncp vasp.out ${DIR}/vasp.out    \n\nif [ $succeed -eq 0 ]; then    \n        cd ${DIR}              \n        rm -rf $scratch/$JOB/WAVECAR          \n        echo \"$JOB finished successfully\"     \nfi\n</code></pre> <p>To use it, please use the following command to load the module:</p> <p><code>ml -* intel/2020b VASP/6.2.1</code></p> <p>After that you can use vasp_gam, vasp_ncl, vasp_std and vasp (same as vasp_std) to run VASP.</p> <p>The GPU version of VASP/6.2.1 is also installed</p> <p>[From one of our technicians]</p> <p>I also compiled VASP/6.2.1 with intel and CUDA compilers. You can also load the version with the command:</p> <p><code>ml -* intelcuda/2020b VASP/6.2.1</code></p> <p>After that you can use vasp_gpu, vasp_gpu_ncl, vasp_gam, vasp_ncl, vasp_std and vasp (same as vasp_std) to run VASP.</p> <p>All of the compilations have the interface to Wannier90/3.1.0.</p> <p><code>ml -* intel/2020b VASP/6.2.1</code></p> <p>After that you can use vasp_gam, vasp_ncl, vasp_std and vasp (same as vasp_std) to run VASP. The compilation used the intel compilers to compile VASP. </p> <p><code>ml -* intelcuda/2020b VASP/6.2.1</code></p> <p>After that you can use vasp_gpu, vasp_gpu_ncl, vasp_gam, vasp_ncl, vasp_std and vasp (same as vasp_std) to run VASP. All of the compilations have the interface to Wannier90/3.1.0. </p> <p><code>ml -* intel/2020b VASP/6.2.1-VASPsol</code></p> <p>on a dev node to load the version.</p>","tags":["lab notebook","VASP"]},{"location":"2023-03-14_LabNotebook_VASP/#some-general-points-to-remember-when-running-vasp-calculations","title":"Some General Points to Remember when Running VASP Calculations","text":"<p>To visualize and modify your structures you can use VESTA: https://jp-minerals.org/vesta/en/download.html</p> <p>For more instructions about VESTA you can use: https://www.youtube.com/channel/UCmOHJtv6B2IFqzGpJakANeg/videos</p> <p>As explained below, VASP calculations require 4 input files: INCAR, KPOINTS,  POSCAR, and POTCAR. </p> <p>One needs to create a separate directory for each calculation and copy the input files to the directory. A brief introduction to the files: </p> <p>(1) INCAR: All calculations set ups such as energy and force convergence, single point  energy calculation, optimization, or MD; 2D vs 3D calculations etc. are defined here. Most keywords are explained (commented). If you want to know about a particular keyword you could search for it in VASP WIKI documentation. Just type the word and VASP in google and it will take you to many online sources explaining it. VASP has been popular among researchers for many years. So, it is easy to find the explanations from online groups. </p> <p>(2) POSCAR: We define atomic positions and lattice vectors here. Note that the atomic positions are in crystal units. To convert it, I usually open the file in VESTA and export again as a POSCAR file, but now in angstrom unit. I don't have a script to automate it. But, will be useful when planning to run several calculations in one go.  </p> <p>(3) KPOINTS; Typically I use gamma-point centered high symmetry points in Brillouine zone. Monkhorst-Pack grids. You may change it according to the problem. </p> <p>(4) POTCAR: Potential files are distributed along with the VASP software and license. I use GGA-PBE PAW pseudo potentials. This is saved as default POTCAR file in the VASP pseudopotential directory in hpc. Once you open POTCAR file, you could see the explanation in the first few lines. </p> <p>If the system has more than one unique element, you need to copy the POTCAR file for each element and concatenate all of them in the same order as you have it in POSCAR file. To highlight the difference for a system with more than one unique element, I have attached a file named POSCAR_2. </p> <p>type module show to get the path for vasp installation and pseudopotential (POTCAR) files.</p> <p>I feel PBE PAW pseudopotentials should be good for most purposes. Test different convergence criteria for your system before starting the production run. </p> <p>To concatenate POTCAR files, use the command, cat ~/pot/Al/POTCAR ~/pot/C/POTCAR ~/pot/H/POTCAR &gt;POTCAR (this also shows that you dont need to coy all POTCAR files into the working directory. Instesd, you could directly create the final POTCAR file in one go). </p> <p>(5) I haven't exported the wave functions files from VASP for any specific purpose. So, I cannot help you with that for now. But, you may check online for guidance. </p> <p>VASP creates a lot of scratch files. So, it can use all the memory very quickly. If you need to run many calculations in one go, plan ahead to delete unnecessary files.</p>","tags":["lab notebook","VASP"]},{"location":"2023-03-27_LabNotebook_Jupyter_port-forward_servers/","title":"(2023-06-30) Lab Notebooks: Jupyter --- Port-Forwarding Servers","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Jupyter","ssh"]},{"location":"2023-03-27_LabNotebook_Jupyter_port-forward_servers/#port-forwarding-for-jupyter-notebooks-updated-2023-06-30","title":"Port-Forwarding for Jupyter Notebooks (updated 2023-06-30)","text":"<p>The OnDemand system provides a wonderful way to run Jupyter notebooks on the HPCC, but sometimes it may not perfectly meet our needs. Perhaps you are having difficulty setting up the environment in which Jupyter should run, or you want to use the newer JupyterLab instead of the old-style Jupyter notebook server. </p> <p>If you run <code>jupyter notebook</code> from the command line on the HPCC, you may find that a browser window opens just as if you had run the command in a terminal on your own machine. This browser will be very slow to respond, however, as it has launched via the X Windows System.</p> <p>The solution is to run our Jupyter Notebook (or JupyterLab) server with port-forwarding, such that the server is running on the HPCC but we use the browser on our local machine to connect to it.</p> <p>The following information is modified from instructions Philipp Grete wrote for Brian O'Shea's research group.</p>","tags":["lab notebook","Jupyter","ssh"]},{"location":"2023-03-27_LabNotebook_Jupyter_port-forward_servers/#pre-requisites","title":"Pre-Requisites","text":"<p>These instructions assume you have access to a terminal on your local machine. Users interested in attempting this with MobaXterm should follow these instruction instead.</p> <p>As a prerequisite, set up an SSH Key and SSH Tunneling.</p> <p>Then, add the following to the bottom of your local <code>.ssh/config</code> file (this is the same file edited when setting up SSH tunneling):</p> <pre><code>    Host lac* vim* skl* nvl* amr* nvf* nif* nal* acm*\n    User here_you_put_your_net_id\n    ProxyJump intel18\n</code></pre> <p>The three-letter names correspond to the kinds of nodes available from each cluster. This will make it possible to SSH to compute nodes from your local machine! This is useful if you would like to run a Jupyter server from an interactive job.</p> <p>Note</p> <p>You can only SSH to a compute node if you have job running on that node.</p>","tags":["lab notebook","Jupyter","ssh"]},{"location":"2023-03-27_LabNotebook_Jupyter_port-forward_servers/#setting-up-a-server-on-the-hpcc","title":"Setting up a Server on the HPCC","text":"<p>After preparing your local machine, open a terminal and connect to a development node or compute node with an interactive job.</p> <p>Either from the command line or within a batch script, you should launch your Jupyter server on the HPCC with the following command: </p> <p><code>jupyter notebook --port=12345 --ip=\"*\" --no-browser</code> </p> <p>where <code>12345</code> can be a 5-digit number between 10000 and 65535. If you and someone else happen to be using the same port at the exact same time, issues may arise. Simply cancel the server and restart with a new port number.</p> <p>If using JupyterLab, simply swap <code>notebook</code> for <code>lab</code>.</p>","tags":["lab notebook","Jupyter","ssh"]},{"location":"2023-03-27_LabNotebook_Jupyter_port-forward_servers/#connecting-to-the-remote-server-from-your-local-machine","title":"Connecting to the Remote Server from your Local Machine","text":"<p>Having set up the server on the HPCC, we now need to connect to it from our local machine.</p> <p>In the same terminal that is connected to the HPCC, type <code>~C</code> (do not press Enter). You should see an <code>ssh&gt;</code> prompt appear.</p> <p>Warning</p> <p>These instructions will not work if you use the OnDemand development node terminals to access the HPCC.</p> <p>You'll type the following information without spaces:</p> <p><code>-L&lt;localport&gt;:&lt;remotehost&gt;:&lt;remoteport&gt;</code></p> <p>For example, <code>-L12345:lac-250:12345</code>. Then press Enter.</p> <p>Each piece of information, from right to left, is as follows:</p> <ul> <li><code>&lt;remoteport&gt;</code> is the port you entered when launching your Jupyter server. For this example, it was <code>12345</code>.</li> <li><code>&lt;remotehost&gt;</code> is the name of the host your server is running on. This could be a development node (e.g. <code>dev-amd20</code>) or it could be a compute note (e.g. <code>lac-250</code>). For this example, we'll assume a compute node. This is why setting up SSH Tunneling is required: it's the only way to connect directly to development and compute nodes!</li> <li><code>&lt;localport&gt;</code> is another 5-digit number like <code>&lt;remoteport&gt;</code>. For simplicity, we'll make it the exact same as <code>&lt;remoteport&gt;</code> (but it doesn't have to be).</li> </ul> <p>When you launched the Jupyter server, you should have been given a URL that starts with <code>http://127.0.0.1</code>. This is the URL from which you should access your server; copy and paste it into your browser. It should already have the notebook token as part of the URL, but if you are asked for it, copy the string of numbers and letters that follows <code>?token=</code>.</p>","tags":["lab notebook","Jupyter","ssh"]},{"location":"2023-03-27_LabNotebook_Mathematica_FixingStartupIssues/","title":"(2023-03-27) Lab Notebook: Mathematica --- Fixing Startup Errors","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Mathematica"]},{"location":"2023-03-27_LabNotebook_Mathematica_FixingStartupIssues/#lab-notebook-fixing-startup-errors-in-mathematica-2023-03-27","title":"Lab Notebook --- Fixing Startup Errors in Mathematica (2023-03-27)","text":"<p>If you have errors upong starting the Mathematica GUI or notebook, particular if the error involves a value called \"Persistance\", you may find that some features of the program (such as starting parallel kernels) do not work.</p> <p></p> <p>This issue appears to be related/can be fixed by resetting Mathematica's preferences, caches, and history, as mentioned here. While this process should not cause any data loss, we DON NOT recommend immediately deleting the listed files. Instead rename the .Mathematica and .Wolfram folders in your home directory to .Mathematica.save and .Wolframe.save, respectively (this way, the folders can be reverted if something is lost). Please do not change any of the other folders listed in the link without conctacting us first (as we have not tested changing them).</p> <p>Renaming these folders with the 'mv' command from the command line:</p> <pre><code>mv .Mathematica .Mathematica.save\nmv .Wolfram .Wolfram.save\n</code></pre> <p>Once this is done, you can restart Mathematica, which should create new .Mathematica and .Wolfram folders and this should resolve the issue. If it does not, please open a ticket with us.</p>","tags":["lab notebook","Mathematica"]},{"location":"2023-04-04_LabNotebook_OnDemand_RunningMPIJob/","title":"(2023-04-04) Lab Notebook: OnDemand RunningMPIJobs","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","OnDemand","MPI"]},{"location":"2023-04-04_LabNotebook_OnDemand_RunningMPIJob/#running-mpi-job-throughs-the-ondemand-interactive-desktop-terminal-2023-04-04","title":"Running MPI Job throughs the OnDemand Interactive Desktop Terminal (2023-04-04)","text":"<p>To run an MPI job through the OnDemand Interactive Desktop, you need to make sure that your resource request through OnDemand asks for at least two (2) tasks under \"Advanced Options\" and that the number of cores per task (N) is set to the number that you want to the MPI job with. After starting the Interactive Desktop, go to the Terminal and you should be able to run your job with:</p> <p>srun --ntasks=N --cpus-per-task=1  <p>For example, if your want to run a job with 4 tasks inside of the Interactive Terminal, your resrouce request would look like:</p> <p></p> <p>And then your would be able to run your job with</p> <p>srun --ntasks=4 --cpus-per-task=1 some_job.sh</p>","tags":["lab notebook","OnDemand","MPI"]},{"location":"2023-05-04_LabNotebook_srun_threading_changes/","title":"SLURM --- Changes to <code>srun</code> for jobs with multiple CPUs per task","text":"<p>ICER has recently updated the HPCC's SLURM scheduling system to version 23.02. With this update comes a change to the behavior of <code>srun</code> that affects jobs using multiple cores per task.</p> <p>Take the following SLURM job script as an example. Here we request 5 tasks (or processes) where each tasks is being executed by 2 CPUs, for a total of 10 CPUs: <pre><code>#!/bin/bash/ --login\n\n#SBATCH --time=4:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks=5\n#SBATCH --cpus-per-task=2\n#SBATCH --mem-per-cpu=2G \n\nsrun my_job.exe --arg1 foo --arg2 bar\n</code></pre></p>","tags":["lab notebook","slurm","srun"]},{"location":"2023-05-04_LabNotebook_srun_threading_changes/#how-srun-used-to-work","title":"How <code>srun</code> used to work","text":"<p>Historically, <code>srun</code> would run <code>my_job.exe</code> with 5 tasks and 2 CPUs per task by default. No additional command line arguments need to be specified for <code>srun</code> to execute in this way.</p> <p>This is because SLURM sets a number of environment variables for a job which describe the resources requested. Specifically,</p> <ul> <li>The <code>--ntasks</code> (or <code>-n</code>) request is saved to the <code>SLURM_NTASKS</code> variable</li> <li>The <code>--cpus-per-task</code> (or <code>-c</code>) request is saved to the <code>SLURM_CPUS_PER_TASK</code></li> </ul> <p>Then, <code>srun</code> inherits values from these environment variables.</p>","tags":["lab notebook","slurm","srun"]},{"location":"2023-05-04_LabNotebook_srun_threading_changes/#how-srun-works-now","title":"How <code>srun</code> works now","text":"<p>As of version 22.05, <code>srun</code> no longer reads the variable <code>SLURM_CPUS_PER_TASK</code>.</p> <p>Instead, you must now re-specify the CPUs per task when calling <code>srun</code>. The final line of our example batch script should be replaced with: <pre><code>srun --cpus-per-task=2 my_job.exe --arg foo\n</code></pre> or <pre><code>srun -c 2 my_job.exe --arg foo\n</code></pre></p> <p>The environment variable <code>SLURM_CPUS_PER_TASK</code> is still available to the user, so it is also possible to execute <code>srun</code> by passing the value of this variable to the <code>--cpus-per-task</code>/<code>-c</code> option: <pre><code>srun -c $SLURM_CPUS_PER_TASK my_job.exe --arg foo\n</code></pre> The advantage of this approach is that users only need to adjust the number of CPUs per task at the top of their batch scripts.</p> <p>Alternatively, you can set the new <code>SRUN_CPUS_PER_TASK</code> environment variable: <pre><code>export SRUN_CPUS_PER_TASK=2\nsrun my_job.exe --arg foo\n</code></pre> or <pre><code>export SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK\nsrun my_job.exe --arg foo\n</code></pre></p> <p>Warning</p> <p>If you are using OpenMP, setting OMP_NUM_THREADS will override both the <code>-c</code> option and <code>SRUN_CPUS_PER_TASK</code>.</p>","tags":["lab notebook","slurm","srun"]},{"location":"2023-05-04_LabNotebook_srun_threading_changes/#try-it-for-yourself","title":"Try it for yourself","text":"<p>You can test the behavior of <code>srun</code> with a hybrid MPI/OpenMP example available through <code>getexample</code>:</p> <pre><code>getexample MPI_OpenMP_GPU\n\ncd MPI_OpenMP_GPU\nmake\n</code></pre> <p>Then, request a 30 minute interactive job with 5 tasks and 2 CPUs per task: <pre><code>salloc -n 5 -c 2 -t 00:30:00\n</code></pre></p> <p>Once your interactive job starts, try running: <pre><code>srun hybrid\nsrun -c 2 hybrid\n</code></pre></p> <p>You should see that <code>srun hybrid</code>, without any additional arguments, allocated 10 CPUs to each of the 5 processes. On the other hand, <code>srun -c hybrid</code> has the expected behavior of 2 CPUs for each of the 5 processes.</p> <p>Now, set the <code>SRUN_CPUS_PER_TASK</code> variable: <pre><code>export SRUN_CPUS_PER_TASK=2\nsrun hybrid\n</code></pre></p> <p>You'll see the same behavior as <code>srun -c hybrid</code>.</p>","tags":["lab notebook","slurm","srun"]},{"location":"2023-05-18_LabNotebook_EasyBuild_PostgreSQL_Installation/","title":"(2023-05-18) Lab Notebook: EasyBuild PostgreSQL Installation","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","EasyBuild","PostgreSQL","R"]},{"location":"2023-05-18_LabNotebook_EasyBuild_PostgreSQL_Installation/#lab-notebook-using-easybuild-to-install-a-postgresql-compatible-with-r-2023-05-18","title":"Lab Notebook --- Using EasyBuild to install a PostgreSQL compatible with R (2023-05-18)","text":"","tags":["lab notebook","EasyBuild","PostgreSQL","R"]},{"location":"2023-05-18_LabNotebook_EasyBuild_PostgreSQL_Installation/#problem-setup","title":"Problem setup","text":"<p>A user wanted to use PostgreSQL with the R/4.2.2 module. However, the versions of PostgreSQL installed</p> <pre><code>$ module spider PostgreSQL\n...\nVersions:\n   PostgreSQL/9.6.2-Python-2.7.12\n   PostgreSQL/11.3-Python-3.7.2\n...\n</code></pre> <p>require specific versions of the GCC module to be loaded:</p> <pre><code>$ module spider PostgreSQL/11.3-Python-3.7.2\n...\nYou will need to load all module(s) on any one of the lines below before the \"PostgreSQL/11.3-Python-3.7.2\" module is available to load.\n\n      Core/GCCcore/8.2.0\n      GCCcore/8.2.0\n...\n</code></pre> <p>This conflicts with the version GCC/11.3.0 necessary to load R/4.2.2:</p> <pre><code>$ module spider R/4.2.2\n...\n    You will need to load all module(s) on any one of the lines below before the \"R/4.2.2\" module is available to load.\n\n      Compiler/GCC/11.2.0/OpenMPI/4.1.1\n      Core/GCC/11.2.0  OpenMPI/4.1.1\n      GCC/11.2.0  OpenMPI/4.1.1\n...\n</code></pre> <p>So we need a version of PostgreSQL which is compatible with GCC/11.3.0.</p>","tags":["lab notebook","EasyBuild","PostgreSQL","R"]},{"location":"2023-05-18_LabNotebook_EasyBuild_PostgreSQL_Installation/#solution","title":"Solution","text":"<p>We often use EasyBuild to install software on the HPCC. One of the nice things about EasyBuild is that other users can contribute EasyConfigs which are recipes to build and install different types of software.</p>","tags":["lab notebook","EasyBuild","PostgreSQL","R"]},{"location":"2023-05-18_LabNotebook_EasyBuild_PostgreSQL_Installation/#loading-easybuild","title":"Loading EasyBuild","text":"<p>To get started, we load the EasyBuild module:</p> <pre><code>$ module purge\n$ module load EasyBuild\n</code></pre> <p>We now have access to the <code>eb</code> command and two aliases defined by MSU HPCC staff: <code>ebF</code> to find EasyConfigs and <code>ebS</code> to install software. We can first check our global EasyBuild configuration using</p> <pre><code>$ eb --show-config\n#\n# Current EasyBuild configuration\n# (C: command line argument, D: default value, E: environment variable, F: configuration file)\n#\nbuildpath            (E) = /tmp/grosscra/EASYBUILD\ncontainerpath        (D) = /mnt/home/grosscra/.local/easybuild/containers\ninstallpath          (E) = /opt\ninstallpath-modules  (E) = /opt/modules\ninstallpath-software (E) = /opt/software\nmodule-naming-scheme (E) = MigrateFromEBToHMNS\noptarch              (E) = GENERIC\nrepositorypath       (E) = /mnt/research/helpdesk/EB_Files_4\nrobot-paths          (E) = /mnt/research/helpdesk/EB_Files_4, /opt/software/EasyBuild/4.7.1/easybuild/easyconfigs, /mnt/research/helpdesk/ebfiles\nsourcepath           (E) = /mnt/research/helpdesk/src\n</code></pre> <p>Since I (<code>grosscra</code>) am part of the <code>helpdesk</code> group used by HPCC staff, my <code>--show-config</code> will look different from other user's configuration. In particular, I am set up to install the software into the root directory <code>/opt</code>, with modules going into <code>/opt/modules</code> and the actual software going into <code>/opt/software</code>.</p>","tags":["lab notebook","EasyBuild","PostgreSQL","R"]},{"location":"2023-05-18_LabNotebook_EasyBuild_PostgreSQL_Installation/#a-quick-digression-on-local-modules","title":"A quick digression on local modules","text":"<p>For a user not in <code>helpdesk</code> you will have directories in your <code>$HOME</code> directory (e.g., software in <code>$HOME/software</code> and modules in <code>$HOME/modules</code>). Thus, using EasyBuild, you can build your own software. If you add your module directory to your module path using</p> <pre><code>$ module use $HOME/modules\n$ echo $MODULEPATH\n/mnt/home/grosscra/modules:/opt/software/hpcc/modules:/opt/modules/Core\n</code></pre> <p>you can then load modules that you install using the exact same commands you use on the HPCC (e.g., <code>module load PostgreSQL</code>).</p>","tags":["lab notebook","EasyBuild","PostgreSQL","R"]},{"location":"2023-05-18_LabNotebook_EasyBuild_PostgreSQL_Installation/#finding-our-easyconfig","title":"Finding our EasyConfig","text":"<p>So now that we're happy with and (mostly) understand our <code>eb --show-config</code> results, we can try finding the EasyConfig for PostgreSQL we'd like to use. Our first step is to use the <code>ebF</code> alias:</p> <pre><code>$ ebF PostgreSQL\n\nebF_PATH=/opt/software/EasyBuild/4.7.1/easybuild/easyconfigs\n\n====== $ebF_PATH/__archive__/p/PostgreSQL/\nPostgreSQL-9.3.5-intel-2014b.eb\n\n====== $ebF_PATH/p/PostgreSQL/\nPostgreSQL-10.2-intel-2018a-Python-2.7.14.eb\nPostgreSQL-10.3-foss-2017b-Python-2.7.14.eb\nPostgreSQL-10.3-foss-2018a-Python-2.7.14.eb\nPostgreSQL-10.3-foss-2018b.eb\nPostgreSQL-10.3-intel-2017b-Python-2.7.14.eb\nPostgreSQL-10.3-intel-2018a-Python-2.7.14.eb\nPostgreSQL-11.3-GCCcore-8.2.0-Python-2.7.15.eb\nPostgreSQL-11.3-GCCcore-8.2.0-Python-3.7.2.eb\nPostgreSQL-12.4-GCCcore-9.3.0.eb\nPostgreSQL-13.2-GCCcore-10.2.0.eb\nPostgreSQL-13.3-GCCcore-10.3.0.eb\nPostgreSQL-13.4-GCCcore-11.2.0.eb\nPostgreSQL-14.4-GCCcore-11.3.0.eb\nPostgreSQL-9.4.7-intel-2016a-Python-2.7.11.eb\nPostgreSQL-9.5.2-intel-2016a-Python-2.7.11.eb\nPostgreSQL-9.6.0-intel-2016b-Python-2.7.12.eb\nPostgreSQL-9.6.2-foss-2016b-Python-2.7.12.eb\nPostgreSQL-9.6.2-intel-2016b-Python-2.7.12.eb\n</code></pre> <p>This tells us that there many EasyConfigs available to help us install different versions PostgreSQL under different toolchains.</p> What is a toolchain? <p>A toolchain is a set of software dependencies used to install new software. Most often, this is a compiler like GCC or a compiler/MPI pair like GCC and OpenMPI. The most basic toolchains are just single compilers and are labeled using their software version (like <code>GCCcore-11.2.0</code>).</p> <p>EasyBuild organizes installed modules by toolchain. For example, if you look for the R/4.2.2 module file, it's under <code>/opt/modules/MPI/GCC/11.2.0/OpenMPI/4.1.1/R/4.2.2.lua</code> because it was built using a GCC/OpenMPI toolchain.</p> <p>Some of these are so commonly used that EasyBuild groups dependency software into larger toolchains like \"foss\" and \"intel\" that contain a compiler/MPI pair and a number of other common dependencies. These are labeled by their year and an <code>a</code> or <code>b</code> for the first or second half of the year. You can check what's in them by searching for their EasyConfig and showing it with <code>eb --show-ec</code>:</p> <pre><code>$ ebF foss\n...\n====== $ebF_PATH/f/foss/\nfoss-2016.04.eb  foss-2016b.eb    foss-2018b.eb  foss-2021a.eb    foss-2022b.eb\nfoss-2016.06.eb  foss-2017a.eb    foss-2019a.eb  foss-2021b.eb\nfoss-2016.07.eb  foss-2017b.eb    foss-2019b.eb  foss-2022.05.eb\nfoss-2016.09.eb  foss-2018.08.eb  foss-2020a.eb  foss-2022.10.eb\nfoss-2016a.eb    foss-2018a.eb    foss-2020b.eb  foss-2022a.eb\n...\n$ eb --show-ec foss-2022a.eb\neasyblock = 'Toolchain'\n\nname = 'foss'\nversion = '2022a'\n\nhomepage = 'https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain'\ndescription = \"\"\"GNU Compiler Collection (GCC) based compiler toolchain, including\n OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK.\"\"\"\n\ntoolchain = SYSTEM\n\nlocal_gccver = '11.3.0'\n\n# toolchain used to build foss dependencies\nlocal_comp_mpi_tc = ('gompi', version)\n\n# we need GCC and OpenMPI as explicit dependencies instead of gompi toolchain\n# because of toolchain preparation functions\ndependencies = [\n    ('GCC', local_gccver),\n    ('OpenMPI', '4.1.4', '', ('GCC', local_gccver)),\n    ('FlexiBLAS', '3.2.0', '', ('GCC', local_gccver)),\n    ('FFTW', '3.3.10', '', ('GCC', local_gccver)),\n    ('FFTW.MPI', '3.3.10', '', local_comp_mpi_tc),\n    ('ScaLAPACK', '2.2.0', '-fb', local_comp_mpi_tc),\n]\n\nmoduleclass = 'toolchain'\n</code></pre> <p>We can see that <code>foss</code> includes <code>GCC</code>, <code>OpenMPI</code>, <code>FlexiBLAS</code>, <code>FFTW</code>, <code>FFTW.MPI</code>, and <code>ScaLAPACK</code>.</p> <p>Since we know that R/4.2.2 requires GCC/11.2.0 to load, we look for a PostgreSQL EasyConfig with a compatible toolchain. In this case, we see <code>PostgreSQL-13.4-GCCcore-11.2.0.eb</code>. If a version of PostgreSQL other than 13.4 were required, we would probably need to generate a new EasyConfig, but this version was suitable for the user.</p>","tags":["lab notebook","EasyBuild","PostgreSQL","R"]},{"location":"2023-05-18_LabNotebook_EasyBuild_PostgreSQL_Installation/#fixing-dependency-resolution","title":"Fixing dependency resolution","text":"<p>Now let's try to see how the installation will go. Since I'm writing this after having installed PostgreSQL/13.4, the output will look different than before it was installed. Instead, I'll use <code>PostgreSQL-14.4-GCCcore-11.3.0.eb</code> which as of now is still not installed.</p> <p>We can check to see if we're missing any of this install's dependencies on the system using <code>eb -M</code>:</p> <pre><code>$ eb -M PostgreSQL-14.4-GCCcore-11.3.0.eb\n...\n47 out of 47 required modules missing:\n\n* Core | M4/1.4.19 (M4-1.4.19.eb)\n* Core | Bison/3.8.2 (Bison-3.8.2.eb)\n* Core | OpenSSL/1.1 (OpenSSL-1.1.eb)\n* Core | zlib/1.2.12 (zlib-1.2.12.eb)\n* Core | help2man/1.47.4 (help2man-1.47.4.eb)\n* Core | M4/1.4.17 (M4-1.4.17.eb)\n* Core | Bison/3.0.4 (Bison-3.0.4.eb)\n* Core | M4/1.4.18 (M4-1.4.18.eb)\n* Core | flex/2.6.4 (flex-2.6.4.eb)\n* Core | binutils/2.38 (binutils-2.38.eb)\n* Core | GCCcore/11.3.0 (GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | M4/1.4.19 (M4-1.4.19-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | help2man/1.49.2 (help2man-1.49.2-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | zlib/1.2.12 (zlib-1.2.12-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | Bison/3.8.2 (Bison-3.8.2-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | flex/2.6.4 (flex-2.6.4-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | binutils/2.38 (binutils-2.38-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | groff/1.22.4 (groff-1.22.4-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | expat/2.4.8 (expat-2.4.8-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | ncurses/6.3 (ncurses-6.3-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | bzip2/1.0.8 (bzip2-1.0.8-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | DB/18.1.40 (DB-18.1.40-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | pkgconf/1.8.0 (pkgconf-1.8.0-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | libreadline/8.1.2 (libreadline-8.1.2-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | UnZip/6.0 (UnZip-6.0-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | Perl/5.34.1 (Perl-5.34.1-GCC\n...\n</code></pre> <p>So we're missing everything we need... But this doesn't seem right! It even says we're missing GCC/11.3.0, which is definitely installed:</p> <pre><code>$ module spider GCC/11.3.0\n\n----------------------------------------------------------------------------\n  GCC: GCC/11.3.0\n----------------------------------------------------------------------------\n    Description:\n      The GNU Compiler Collection includes front ends for C, C++,\n      Objective-C, Fortran, Java, and Ada, as well as libraries for these\n      languages (libstdc++, libgcj,...).\n\n\n    This module can be loaded directly: module load GCC/11.3.0\n...\n</code></pre> <p>What's happening is that the way modules are searched for on the HPCC are different than the way EasyBuild searches for them. EasyBuild wants to include \"Core/\" in front of the core module names (i.e., those that aren't installed under a toolchain). But if we look at where modules are searched for,</p> <pre><code>$ echo $MODULEPATH\n/opt/software/hpcc/modules:/opt/modules/Core\n</code></pre> <p>the \"Core\" part is already included in the path. This makes it so you don't need to use <code>module load Core/GCC/11.3.0</code> and can get right to the software name you need.</p> <p>To make things work correctly with EasyBuild's expectations , we can add <code>/opt/modules</code> to our module path and try again:</p> <pre><code>$ module use /opt/modules\n$ echo $MODULEPATH\n/opt/modules:/opt/software/hpcc/modules:/opt/modules/Core\n$ eb -M PostgreSQL-14.4-GCCcore-11.3.0.eb\n...\n1 out of 47 required modules missing:\n\n* Compiler/GCCcore/11.3.0 | PostgreSQL/14.4 (PostgreSQL-14.4-GCCcore-11.3.0.eb)\n...\n</code></pre> <p>Much better! Now we're only missing the module we want to install.</p> <p>In the case where we would actually be missing dependencies, EasyBuild would install those for us, so long as we use the <code>--robot</code> option when installing. This is included in the <code>ebS</code> alias by default.</p>","tags":["lab notebook","EasyBuild","PostgreSQL","R"]},{"location":"2023-05-18_LabNotebook_EasyBuild_PostgreSQL_Installation/#installing","title":"Installing","text":"<p>Now we're ready to install. We just use the <code>ebS</code> alias with our EasyConfig, and hope things go well!</p> <pre><code>$ ebS PostgreSQL-14.4-GCCcore-11.3.0.eb\n\n... Good luck! ...\n</code></pre>","tags":["lab notebook","EasyBuild","PostgreSQL","R"]},{"location":"2023-05-18_LabNotebook_EasyBuild_PostgreSQL_Installation/#checking-the-installation","title":"Checking the installation","text":"<p>Now that we have installed a version of PostgreSQL compatible with the same GCC that R needs, we can try to load them all</p> <pre><code>$ module purge\n$ module load GCC/11.2.0 OpenMPI/4.1.1 R PostgreSQL\n</code></pre> <p>We get a small warning about OpenMPI/4.1.1 being incompatible with intel14 nodes, but other than that, everything loads correctly.</p>","tags":["lab notebook","EasyBuild","PostgreSQL","R"]},{"location":"2023-07-13_LabNotebook_IgnoringLocalPythonLibraries/","title":"(2023-07-13) Lab Notebook: Ignoring Local Python Libraries","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Python","Singularity"]},{"location":"2023-07-13_LabNotebook_IgnoringLocalPythonLibraries/#lab-notebook-ignoring-local-python-libraries-2023-07-13","title":"Lab Notebook --- Ignoring Local Python Libraries (2023-07-13)","text":"<p>Python will automatically populate the internal \"sys.path\" variable with the location of package libraries. One such location is the .local/lib folder of your home directory on HPCC, provided that the python folder there corresponds to python version being used (i.e. python3.6 for running any Python 3.6.x version). This can create conflicts with containerized software (such as Singularity containters) and (potentially) other python environments like conda by causing python to try and load a module from .local/lib instead of the intended source.</p> <p>In principle, this should not cause a problem if .local/lib is low enough on the load order in sys.path, but in practice this is not always the case. Since there is not benefit to having these additional libaries in sys.path from running something Singularity, the issue can be avoided by excluding the .local/lib path from sys.path.</p> <p>To have python ignore the .local/lib folder when populating sys.path, you can set the following environment variable:</p> <pre><code>export PYTHONNOUSERSITE=1\n</code></pre>","tags":["lab notebook","Python","Singularity"]},{"location":"2023-07-13_LabNotebook_IgnoringLocalPythonLibraries/#example-useage-running-alphafold-from-a-singularity-container","title":"Example Useage: Running AlphaFold from a Singularity container","text":"<p>HPCC has versions of AlphaFold available as Singularity containers (such as /opt/software/alphafold/2.0.0/alphafold.sif). These containers run Python 3.8.x and if you have a .local/lib/python38 folder, you may face an error like the following:</p> <p></p> <p>Setting PYTHONNOUSERSITE as above should resolve this issue</p>","tags":["lab notebook","Python","Singularity"]},{"location":"2023-07-18_LabNotebook_AlphaFold2.3.1_WorkInProgress/","title":"(2023-07-18) Lab Notebook: AlphaFold 2.3.1 DEPRECATED","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p> <p>Extra</p> <p>The 2.3.1 version of the AlphaFold singularity image is currently non-functional on HPCC. As of Ocotober, 2023, the image is no longer able to find the current libcusolver share library file.  Please use the 2.3.2 version of the image from this point forward. Please see the new documentation</p>","tags":["lab notebook","AlphaFold","Singularity"]},{"location":"2023-07-18_LabNotebook_AlphaFold2.3.1_WorkInProgress/#lab-notebook-instructions-for-alphafold-version-231-singulairty-2023-07-18depcreated-see-warnings","title":"Lab Notebook --- Instructions for AlphaFold version 2.3.1, Singulairty (2023-07-18)(DEPCREATED, see warnings)","text":"<p>These instructions are a work in progression for running AlphaFold version 2.3.1 using the singularity container found at:</p> <pre><code>/opt/software/alphafold/2.3.1/alphafold_2.3.1_latest.sif\n</code></pre> <p>As with other containers in the <code>/opt/software/alphafold/</code> directory, AlphaFold 2.3.1 can be run via Singularity.</p> <p>Howevever, AlphaFold version after 2.3.0 use a database which is formatted differently than pevious versions. This database is located in <code>/mnt/research/common-data/alphafold/database_230</code>.</p> <pre><code>database_230\n\u2500\u2500 bfd -&gt; ../database/bfd/\n\u251c\u2500\u2500 mgnify\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mgy_clusters_2022_05.fa\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 mgy_clusters.fa -&gt; mgy_clusters_2022_05.fa\n\u251c\u2500\u2500 params\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 LICENSE\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_1_multimer_v3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_1.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_1_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_2_multimer_v3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_2.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_2_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_3_multimer_v3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_3_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_4_multimer_v3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_4.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_4_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_5_multimer_v3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_5.npz\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 params_model_5_ptm.npz\n\u251c\u2500\u2500 pdb70\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 md5sum\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_a3m.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_a3m.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_clu.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_cs219.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_cs219.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_hhm.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_hhm.ffindex\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 pdb_filter.dat\n\u251c\u2500\u2500 pdb_mmcif\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mmcif_files\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 100d.cif\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 101d.cif\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 101m.cif\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 102d.cif\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 102l.cif\n\u2502\u00a0\u00a0 |   ...\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 obsolete.dat\n\u251c\u2500\u2500 pdb_seqres\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 pdb_seqres.txt\n\u251c\u2500\u2500 small_bfd -&gt; ../database/small_bfd/\n\u251c\u2500\u2500 uniprot\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 uniprot.fasta\n\u251c\u2500\u2500 uniref30\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_a3m.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_a3m.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_cs219.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_cs219.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_hhm.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_hhm.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03.md5sums\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 UniRef30_2021_03.tar.1.gz\n\u2514\u2500\u2500 uniref90\n    \u2514\u2500\u2500 uniref90.fasta\n</code></pre> <p>Before running AlphaFold 2.3.1, you need to set</p> <pre><code>export ALPHAFOLD_DATA_PATH=\"/mnt/research/common-data/alphafold/database\"  \nexport ALPHAFOLD_MODELS=\"/mnt/research/common-data/alphafold/database/params\"\n</code></pre> <p>We also recommend disbaling local Python libraries with the following argument to avoid conflicts with the Python installation within the Singularity container</p> <pre><code>export PYTHONNOUSERSITE=1\n</code></pre> <p>Both the database and Python library parameters will be set automatically if you load the module \"alphafold/2.3.1\"</p> <p>To run Alphafold 2.3.1 via SLURM, please use the following template (for more information about options/flags, please refer to the README on Github).</p> <p>In the script, <code>input.fasta</code> is your input data, and\u00a0you need to set up output_dir. Since the command <code>/usr/bin/hhsearch</code> inside the container does not work on intel14 nodes (<code>Illegal instruction</code>), please use the <code>SBATCH</code> option\u00a0<code>--constraint</code> in the job script.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name alphafold-run\n#SBATCH --time=08:00:00\n#SBATCH --gres=gpu:1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=20G\n#SBATCH --constraint=\"[intel16|intel18|amd20]\"\n\n\necho \"Export AlphaFold variables\"\n\nexport ALPHAFOLD_DATA_PATH=\"/mnt/research/common-data/alphafold/database_230\"\nexport ALPHAFOLD_MODELS=\"/mnt/research/common-data/alphafold/database_230/params\"\nexport PYTHONNOUSERSITE=1\nexport CUDA_VISIBLE_DEVICES=0\nexport NVIDIA_VISIBLE_DEVICES=\"${CUDA_VISIBLE_DEVICES}\"\n\necho \"Start Singularity run\"\nsingularity run --nv \\\n-B $ALPHAFOLD_DATA_OLD_PATH \\\n-B $ALPHAFOLD_DATA_PATH:/data \\\n-B $ALPHAFOLD_MODELS \\\n-B .:/etc \\\n--pwd  /app/alphafold /opt/software/alphafold/2.3.1/alphafold_2.3.1_latest.sif \\ \n--data_dir=/data \\\n--output_dir=output_dir \\\n--fasta_paths=input.fasta  \\\n--uniref90_database_path=/data/uniref90/uniref90.fasta  \\\n--uniref30_database_path=/data/uniref30/UniRef30_2021_03 \\\n--mgnify_database_path=/data/mgnify/mgy_clusters.fa \\\n--bfd_database_path=/mnt/research/common-data/alphafold/database/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\\n--template_mmcif_dir=/data/pdb_mmcif/mmcif_files  \\\n--obsolete_pdbs_path=/data/pdb_mmcif/obsolete.dat \\\n--pdb_seqres_database_path=/data/pdb_seqres/pdb_seqres.txt \\\n--uniprot_database_path=/data/uniprot/uniprot.fasta \\\n--max_template_date=2020-05-14   \\ # Update the template date if need be\n--model_preset=monomer \\\n--use_gpu_relax=true\n</code></pre>","tags":["lab notebook","AlphaFold","Singularity"]},{"location":"2023-07-18_LabNotebook_MATLAB_offload_from_localPC_to_HPCC/","title":"(2023-07-18) Lab Notebook: MATLAB offload from localPC to HPCC","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","MATLAB","Parallel Comuting Toolbox"]},{"location":"2023-07-18_LabNotebook_MATLAB_offload_from_localPC_to_HPCC/#lab-notebook-matlab-offload-from-localpc-to-hpcc-2023-07-18","title":"Lab Notebook --- MATLAB offload from localPC to HPCC (2023-07-18)","text":"","tags":["lab notebook","MATLAB","Parallel Comuting Toolbox"]},{"location":"2023-07-18_LabNotebook_MATLAB_offload_from_localPC_to_HPCC/#cluster-configuration","title":"Cluster Configuration","text":"<p>MATLAB allows its users to run MATLAB on their local machine, such as laptop computer and offload the heavy computation to HPCC cluster, and retrieve the results back from the HPCC cluster to local machine. In this document, we</p> <p>provide the instructions for setting up the cluster profile for use in the MATLAB parallel computing toolbox running on a local computer.</p> <p>Following instructions is made from modifying the sample setup from  Configure Using the Generic Scheduler Interface - MATLAB &amp; Simulink specifically for SLURM on HPCC. The diagram illustrating the cluster for the sample system in that document is very similar to our HPCC cluster. </p>","tags":["lab notebook","MATLAB","Parallel Comuting Toolbox"]},{"location":"2023-07-18_LabNotebook_MATLAB_offload_from_localPC_to_HPCC/#requirements","title":"Requirements","text":"<p>The setup must meet the following conditions:</p> <ul> <li>The client node and cluster login node must support <code>ssh</code> and <code>sftp</code>.</li> <li>The cluster login node must be able to call the <code>sbatch</code> command to submit a job to an SLURM scheduler. You can find more about this in the <code>README</code> file in the <code>nonshared</code> subfolder within the installation folder.</li> </ul> <p>Note: HPCC cluster satisfies the requirements. </p>","tags":["lab notebook","MATLAB","Parallel Comuting Toolbox"]},{"location":"2023-07-18_LabNotebook_MATLAB_offload_from_localPC_to_HPCC/#configure-a-cluster-profile","title":"Configure a Cluster Profile","text":"<p>Follow these steps to configure the cluster profile. You can modify any of these options depending on your setup.</p> <ol> <li> <p>Down load provided plug-in script from GitHub repository at (https://github.com/mathworks/matlab-parallel-slurm-plugin.git)  and store it to a location that MATLAB clients (MATLAB on your local machine) can access. This location will be used in the cluster profile setting in step 6(10).</p> <p>Ex.  I saved it on my Mac at the directory: /Users/wangxg/matlab-parallel-slurm-plugin.</p> <pre><code>NOTE: the name of the directory will be used in setting up the profile later in step 6( 10 ).\n</code></pre> </li> <li> <p>Start a MATLAB session on the client host.</p> <p>Ex. I use  MATLAB/ R2022b on my Mac.</p> <pre><code>NOTE: the version of matlab used by client and cluster should be the match.\n\nNOTE: step 3 to 7 are for creating a cluster profile for matlab clients to offload jobs to HPCC. A sample profile is created for users as a reference. Users can import this sample profile ( The name of the sample profile file is \u201cRemote-HPCC-R2022b.mlsettings\u201d) and follow the instructions to create one and/or to customize the sample profile for their own use.\n</code></pre> </li> <li> <p>Start the Cluster Profile Manager from the MATLAB desktop. On the Home tab, in the Environment section, select Parallel &gt; Create and Manage Clusters.</p> </li> <li>Create a new profile in the Cluster Profile Manager by selecting Add Cluster Profile &gt; Generic.</li> <li>With the new profile selected in the list, in the Manage Profile section, select Rename and change the profile name to <code>InstallTest</code>. Press Enter. (Note, you can name it anything you like)</li> <li>In the Properties tab, select Edit and provide settings for the following fields:<ol> <li>Set the Description field to For testing installation.</li> <li>Set the JobStorageLocation to the location where you want job and task data to be stored on the client machine (not the cluster location), for example, <code>C:\\Temp\\joblocation</code>. \\ You must not share JobStorageLocation among parallel computing products running different versions. Each version on your cluster must have its own JobStorageLocation.</li> <li>Set NumWorkers to the number of workers for which you want to test your installation.</li> <li>Set NumThreads to the number of threads to use on each worker (By default, MATLAB worker runs sequentially, that is using 1 thread).</li> <li>Set ClusterMatlabRoot to the installation location of MATLAB to run on the worker machines, which is the MATLAB PATH on the cluster.</li> <li>If the cluster uses online licensing, set RequiresOnlineLicensing to true. For HPCC cluster, we use license server instead. If you are using MSU campus license and offloading to MSU HPCC cluster, you do not need to set this.     </li> <li>If you set RequiresOnlineLicensing to <code>true</code>, enter your LicenseNumber. For MSU campus license users, you do not need to set this.</li> <li>Set OperatingSystem to the operating system of your cluster worker machines. The operating system of HPCC cluster is linux.</li> <li>Set HasSharedFilesystem to <code>false</code>. This setting indicates that the client node (your local machine) and worker nodes (HPCC cluster nodes) cannot share the same data location.</li> <li>Set the PluginScriptsLocation to the location of your plugin scripts ( the location is set at step 1) .</li> <li>To set the connection to a remote cluster, under the AdditionalProperties table, select Add or directly edit the property** <code>ClusterHost</code> . For connect to HPCC cluster, the property with name <code>ClusterHost</code>, value <code>hpcc.msu.edu</code>, and type <code>String. </code></li> <li>To run jobs on a remote cluster without a shared file system, under the AdditionalProperties table, select Add or directly edit the property <code>RemoteJobStorageLocation </code>to specify a directory on the cluster for jobs to store related files. For HPCC cluster users, specify a new property with name <code>RemoteJobStorageLocation</code>, value as the path in your home or research space, for example, <code>/mnt/home/&lt;yourNetid&gt;/remoteJobStorage/</code>, and type <code>String</code>.</li> </ol> </li> <li>Click Done to save your cluster profile settings and changes. </li> </ol>","tags":["lab notebook","MATLAB","Parallel Comuting Toolbox"]},{"location":"2023-07-18_LabNotebook_MATLAB_offload_from_localPC_to_HPCC/#test-examples","title":"Test examples","text":"<ol> <li> <p>To test if the profile \u201cRemote-HPCC-R2022b\u201d is set correctly for HPCC cluster, users can run the validation from local machine. </p> <p>Start MATLAB version R2022b on your local machine. Note that the version should match the version you will use on HPCC (define by the installation path).</p> <p>From HOME  -&gt; Parallel computing options -&gt; Create and Manage Clusters. Select the profile \u201cRemote-HPCC-R2022b\u201d, run \u201cvalidation\u201d. If you can pass the first 4 stages, the profile should work for offloading. You may ignore the last stage of the validation.</p> </li> <li> <p>To further test the profile setting, users can also run some function or script locally and offload to the cluster, and compare the results. For example, a function get_min is saved in file get_min.m show as </p> </li> </ol> <pre><code>function [s, t] =get_min(niter, n, m)\n%function [s,t] = many_parfor_cluster(niter, n, m)\n% This function runs myfunc.m may time in a parfor loop\n% Workers are created and running on compute nodes\n%\n% INPUT\n% niter: number of times to run.\n% n, m: parameters for implicit.m function call.\n%\n% OUTPUT\n% s: maximum value of niter output of myfunc.m\n% t: elapsed time of the function.\n%\ntic;\nS = zeros(niter,1);\nparfor i =1:niter\n    S(i) = max(svd(rand(n, m)));    % each worker will use 1 threads.\nend\ns = max(S);\nt = toc;\n</code></pre> <p>The function get_min can be run on a local machine as</p> <pre><code>&gt;&gt; parpool('local', 4);    % use a local worker pool\nStarting parallel pool (parpool) using the 'Local' profile ...\nConnected to the parallel pool (number of workers: 4).\n\n&gt;&gt; [s, t] = get_min(100,1000,1000)\n\ns =\n     5.010473515126743e+02\nt =\n     7.260608000000000e+00\n</code></pre> <p>The function get_min can also to offload as a job to run on the cluster. User need to create a cluster using the profile \u201cRemote-HPCC-R2022b\". Then create a job using \u201cbatch\u201d to offload the function \u201cget_min\u201d to the cluster \u201cHPCC\u201d with 4 workers. To obtain the jobs output, user can use function \"fetchOutputs\" after the job is completed. </p> <pre><code>&gt;&gt; HPCC = parcluster('Remote-HPCC-R2022b');    % Create cluster HPCC\n&gt;&gt; Job = batch(HPCC, @get_min, 2, {100,1000,1000},'pool', 4);  %create a job for cluster HPCC with 4 workers\n&gt;&gt; wait(Job);      % wait Job to complete before fetching the results.\n&gt;&gt; X = fetchOutputs(Job)\nX =\n\n  1\u00d72 cell array\n\n    {[501.0474]}    {[8.2506]}\n</code></pre> <p>To monitor the jobs, in the local MATLAB, from HOME -&gt; Parallel computing options -&gt; Monitor Jobs to open the job monitor window. Meanwhile, users can also monitor the jobs from a terminal window of the cluster and use SLURM commands to monitor the jobs.</p>","tags":["lab notebook","MATLAB","Parallel Comuting Toolbox"]},{"location":"2023-07-18_LabNotebook_MATLAB_offload_from_localPC_to_HPCC/#references","title":"References:","text":"<ol> <li>Configure for Third-Party Scheduler Cluster Discovery - MATLAB &amp; Simulink</li> <li>Configure Using the Generic Scheduler Interface - MATLAB &amp; Simulink</li> <li>Customize Behavior of Sample Plugin Scripts - MATLAB &amp; Simulink</li> <li>Parallel Computing Toolbox Plugin for Slurm - File Exchange - MATLAB Central</li> </ol>","tags":["lab notebook","MATLAB","Parallel Comuting Toolbox"]},{"location":"2023-08-02_LabNotebook_AlternativeLinuxShells/","title":"(2023-08-02) Lab Notebook: Alternative Linux Shells on HPCC","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p> <p>Extra</p> <p>Changing your shell on HPCC, particularly your login shell, is not recommended. We only  support bash and much of our system, particularly the module system and job scheduler, is set to work with bash. Additionaly, external software (such as VScode) may break with an alternative shell</p>","tags":["lab notebook","Linux","shell","zsh","csh","tcsh"]},{"location":"2023-08-02_LabNotebook_AlternativeLinuxShells/#using-an-alternative-linux-shell-on-hpcc","title":"Using an Alternative Linux Shell on HPCC","text":"<p>HPCC only supports bash, but if you would like to use an alternative shell, you may do so at your own risk. Please keep in mind the above warning before your proceed, and if you require help to restore the regular bash shell, please contact HPCC support</p>","tags":["lab notebook","Linux","shell","zsh","csh","tcsh"]},{"location":"2023-08-02_LabNotebook_AlternativeLinuxShells/#before-you-change-your-shell","title":"Before you change your shell...","text":"<p>While your default login shell on HPCC is bash, this does not mean you are limited to only running bash shell scripts.</p> <p>Shell scripts are simply text files with series of commands that are a mix of programs that are on the system (grep, date, time, a python program, your program, and 'built-ins' specific to the shell) and syntax for control statements like loops and if specific to the shell in which you are running. </p> <p>Shell scripts typically have a program that runs then, known as the interpreter, which could include a shell (bash, zsh, etc) or a language interpreter (python, Rscript, ruby, julia etc).</p> <p>You can run a shell script in (at least) two ways:</p>","tags":["lab notebook","Linux","shell","zsh","csh","tcsh"]},{"location":"2023-08-02_LabNotebook_AlternativeLinuxShells/#1-run-with-an-interpreter","title":"1. Run with an interpreter","text":"<p>Using an interpreter to run you shell script typically follows this syntax <code>&lt;interpereter&gt; &lt;options&gt; &lt;path/to/script&gt;</code></p> <p>A familiar example using python would be:</p> <pre><code>module load python\npython my_python_script.py\n</code></pre> <p>However, the same can be done with different shells:</p> <pre><code>bash my_bash_script.sh\nzsh my_zsh_script.sh\n</code></pre> <p>While HPCC only supports the bash shell for operations as the module system is written for bash,  if you have shell scripts written for the zsh shell, you can still run those using this method on the command line or even inside another bash script (such as a script to launch a job with SLURM).</p>","tags":["lab notebook","Linux","shell","zsh","csh","tcsh"]},{"location":"2023-08-02_LabNotebook_AlternativeLinuxShells/#2-using-the-shebang","title":"2. Using the 'shebang'","text":"<p>At the top of any script you run on a Linux system like the hPC, you may include what's known as the \"shebang\" line (which must be the first line, for more information, see the Wikipedia page).</p> <p>The general syntax for this line is <code>&lt;interpreter&gt; [optional-arg]</code></p> <p>Examples of the 'shebang' in use include:</p> <pre><code>#! /usr/bin/bash  : run the commands using bash\n\n#! /usr/bin/bash -l  : run the commands using bash, but using a login shell, typically used for job/slurm submission scripts to mimic behavior when run on the dev nodes\n\n#! /usr/bin/env python : run this script using the active python interpreter\n\n#! /usr/bin/env Rscript --vanilla : run this script using the activate python\n\n#! /usr/bin/zsh  : run this using zsh, even if launched through bash.\n</code></pre> <p>A further explanation of why you might use <code>/usr/bin/env</code> for your python or R scripts can be found here.</p> <p>To launch a script with a shebang, you must also change the permissions to allow execution ( <code>+x</code> ) if that is not already set with <code>chmod u+x my_shell_script.sh</code> which gives you permission to run the script.</p>","tags":["lab notebook","Linux","shell","zsh","csh","tcsh"]},{"location":"2023-08-02_LabNotebook_AlternativeLinuxShells/#alternative-shells","title":"Alternative shells","text":"<p>If you still find it necessary to change your shell, see below for instructions on how to do so as well as important notes about setting environment variables and making startup scripts for different shells.</p>","tags":["lab notebook","Linux","shell","zsh","csh","tcsh"]},{"location":"2023-08-02_LabNotebook_AlternativeLinuxShells/#changing-your-shell","title":"Changing your shell","text":"<p>Several common shell choices are available on HPCC:</p> <ul> <li><code>bash</code>: a Bourne-shell (sh) compatible shell with many newer advanced features as well </li> <li><code>tcsh</code>: an advanced variant on <code>csh</code> with all the features of modern shells</li> <li><code>zsh</code>: an advanced shell which incorprates all the functionality of <code>bash</code>, <code>tcsh</code>, and <code>ksh</code> combined</li> <li><code>csh</code>: the original C-style shell</li> </ul> <p>You can change you login shell with the <code>chsh</code> command.</p>","tags":["lab notebook","Linux","shell","zsh","csh","tcsh"]},{"location":"2023-08-02_LabNotebook_AlternativeLinuxShells/#assigning-environment-variables","title":"Assigning Environment Variables","text":"<p>To assign a new value to an environment variable in <code>zsh</code>, you use the same format as <code>bash</code>: <code>export &lt;name&gt;=&lt;value&gt;</code></p> <p>To assign a new value to an environment variable in either <code>tcsh</code> or <code>csh</code>: <code>setenv &lt;name&gt; &lt;value&gt;</code></p>","tags":["lab notebook","Linux","shell","zsh","csh","tcsh"]},{"location":"2023-08-02_LabNotebook_AlternativeLinuxShells/#startup-scripts","title":"Startup scripts","text":"<p>A startup script is a shell script which the login process executes. It provides an opportunity to alter your environment. You are free to setup your own startup scripts but be careful to make sure they are set up correctly for both interactive and batch access or it may negatively affect your ability to log in or run batch jobs on the system. Below are startup scripts for each shell:</p> <ul> <li>bash:  <code>~/.bashrc</code></li> <li>tcsh:  <code>~/.chsrc</code> </li> <li>zsh:   <code>~/.zshrc</code> </li> <li>csh:   <code>~/.cshrc</code> </li> </ul>","tags":["lab notebook","Linux","shell","zsh","csh","tcsh"]},{"location":"2023-08-11_LabNotebook_Jupyter_and_mobaxterm_port-forward_servers/","title":"(2023-08-11) Lab Notebooks: Jupyter and MobaXterm--- Port-Forwarding Servers","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Jupyter","ssh","MobaXterm"]},{"location":"2023-08-11_LabNotebook_Jupyter_and_mobaxterm_port-forward_servers/#port-forwarding-for-jupyter-notebooks-with-mobaxterm-updated-2023-08-11","title":"Port-Forwarding for Jupyter Notebooks with MobaXterm (updated 2023-08-11)","text":"<p>The OnDemand system provides a wonderful way to run Jupyter notebooks on the HPCC, but sometimes it may not perfectly meet our needs. Perhaps you are having difficulty setting up the environment in which Jupyter should run, or you want to use the newer JupyterLab instead of the old-style Jupyter notebook server. </p> <p>If you run <code>jupyter notebook</code> from the command line on the HPCC, you may find that a browser window opens just as if you had run the command in a terminal on your own machine. This browser will be very slow to respond, however, as it has launched via the X Windows System.</p> <p>The solution is to run our Jupyter Notebook (or JupyterLab) server with port-forwarding, such that the server is running on the HPCC but we use the browser on our local machine to connect to it.</p> <p>The following information is modified from instructions Philipp Grete wrote for Brian O'Shea's research group.</p>","tags":["lab notebook","Jupyter","ssh","MobaXterm"]},{"location":"2023-08-11_LabNotebook_Jupyter_and_mobaxterm_port-forward_servers/#pre-requisites","title":"Pre-Requisites","text":"<p>These instructions assume you are using Windows and have access to MobaXterm on your local machine. They also assume that you have setup an SSH connection to the HPCC in MobaXterm using these instructions.</p>","tags":["lab notebook","Jupyter","ssh","MobaXterm"]},{"location":"2023-08-11_LabNotebook_Jupyter_and_mobaxterm_port-forward_servers/#setting-up-a-server-on-the-hpcc","title":"Setting up a Server on the HPCC","text":"<p>After preparing your local machine, open a terminal and connect to a development node or compute node with an interactive job.</p> <p>Either from the command line or within a batch script, you should launch your Jupyter server on the HPCC with the following command: </p> <p><code>jupyter notebook --port=12345 --ip=\"*\" --no-browser</code> </p> <p>where <code>12345</code> can be a 5-digit number between 10000 and 65535. If you and someone else happen to be using the same port at the exact same time, issues may arise. Simply cancel the server and restart with a new port number.</p> <p>If using JupyterLab, simply swap <code>notebook</code> for <code>lab</code>.</p>","tags":["lab notebook","Jupyter","ssh","MobaXterm"]},{"location":"2023-08-11_LabNotebook_Jupyter_and_mobaxterm_port-forward_servers/#connecting-to-the-remote-server-from-your-local-machine","title":"Connecting to the Remote Server from your Local Machine","text":"<p>Having set up the server on the HPCC, we now need to connect to it from our local machine. There are two steps to this process.</p>","tags":["lab notebook","Jupyter","ssh","MobaXterm"]},{"location":"2023-08-11_LabNotebook_Jupyter_and_mobaxterm_port-forward_servers/#1-forwarding-the-gateway-node-to-the-development-or-compute-node","title":"1. Forwarding the gateway node to the development or compute node","text":"<p>Open MobaXterm and start a new SSH session to the HPCC gateway. Take note of which gateway node is next to your username when you login. For example</p> <pre><code>[&lt;username&gt;@gateway-02 ~]$\n</code></pre> <p>means that I will use <code>gateway-02</code> in the next step.</p> <p>Instead of logging into a development node directly, you will use an extra command flag to forward the port from where Jupyter is running to the gateway. The template for this is:</p> <pre><code>ssh -L&lt;port&gt;:&lt;jupyter-host&gt;:&lt;port&gt; &lt;dev-node&gt;\n</code></pre> <p>In our case, let's say that we started Jupyter in an interactive session on <code>lac-250</code> with port <code>12345</code>. We can use</p> <pre><code>ssh -L12345:lac-250:12345 dev-intel18\n</code></pre> <p>to forward the port. You can use any development node. If you are running the Jupyter server on a development node, you should use it in between the ports.</p>","tags":["lab notebook","Jupyter","ssh","MobaXterm"]},{"location":"2023-08-11_LabNotebook_Jupyter_and_mobaxterm_port-forward_servers/#2-forwarding-the-local-port-to-the-gateway-node","title":"2. Forwarding the local port to the gateway node","text":"<p>From the MobaXterm's \"Tools\" menu, choose \"MobaSSHTunnel (port forwarding)\". Select \"New SSH tunnel\" from the window that opens.</p> <p>In the next Window, you will fill out three sections:</p> <ol> <li>Between \"Local clients\" and \"My computer with MobaXterm\"<ul> <li>Forwarded port: This can be any port number but to reduce confusion, you   can use the same port number that you started Jupyter with, in our   example, <code>12345</code>.</li> </ul> </li> <li>Between \"My computer with MobaXterm\" and \"SSH server\"<ul> <li>\"SSH server\": <code>gateway-##.hpcc.msu.edu</code>  where <code>gateway-##</code> is the   gateway node you logged into in the previous step. In the example above,   we used <code>gateway-02</code>.</li> <li>\"SSH login\": Your HPCC username</li> <li>\"SSH port\": <code>22</code></li> </ul> </li> <li>Between \"SSH server\" and \"Remote server\"<ul> <li>\"Remote server\": <code>localhost</code></li> <li>\"Remote port\": The port you used to launch the Jupyter server with. In   our case, <code>12345</code>.</li> </ul> </li> </ol>","tags":["lab notebook","Jupyter","ssh","MobaXterm"]},{"location":"2023-08-11_LabNotebook_Jupyter_and_mobaxterm_port-forward_servers/#opening-the-jupyter-notebook-on-your-local-computer","title":"Opening the Jupyter notebook on your local computer","text":"<p>When you launched the Jupyter server, you should have been given a URL that starts with <code>http://127.0.0.1</code>. This is the URL from which you should access your server; copy and paste it into the browser on your local computer. It should already have the notebook token as part of the URL, but if you are asked for it, copy the string of numbers and letters that follows <code>?token=</code>.</p>","tags":["lab notebook","Jupyter","ssh","MobaXterm"]},{"location":"2023-08-11_LabNotebook_Jupyter_and_mobaxterm_port-forward_servers/#reusing-the-port-forwarding-in-the-future","title":"Reusing the port forwarding in the future","text":"<p>MobaXterm will save the local to gateway port forwarding configuration that we set up in step 2 above. You can restart it from the \"MobaSSHTunnel\" menu at any time, however, be aware that you need to match the <code>gateway-##</code> used in this port forwarding with the <code>gateway-##</code> used to forward the port from the compute node to the gateway in step 1. If this gateway node is different than the one saved in MobaXterm, you will need to setup another SSH tunnel in MobaXterm to correspond to that one.</p> <p>You can also setup MobaXterm tunnels for all four gateway nodes (<code>gateway-00</code>, <code>gateway-01</code>, <code>gateway-02</code>, and <code>gateway-03</code>) using the configuration steps above, and choose the corresponding tunnel depending on where your gateway to compute tunnel starts.</p>","tags":["lab notebook","Jupyter","ssh","MobaXterm"]},{"location":"2023-08-30_LabNotebook_SlowdownIncidentReport/","title":"(2023-08-30) Lab Notebooks: Incident Report on System Slowdown due to I/O","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","incident report","I/O"]},{"location":"2023-08-30_LabNotebook_SlowdownIncidentReport/#incident-report-on-system-slowdown-caused-by-excessive-file-io","title":"Incident report on system slowdown caused by excessive file I/O","text":"<p>On the afternoon of August 28th, 2023, ICER staff received reports about slowdown across multiple development nodes on HPCC. After investigating the issue, we identified a series of jobs that were saturating the file systems transaction capacity. Cancelling these jobs resolved the slowdown and further investigation found previous runs of these jobs are correlated with reported slowdown incidents since late July.</p> <p>Based on our own investigation and working with the user to correct the behavior of the causative jobs, we have identified a number of factors which we believe contributed to the slowdown. We will discuss these below both as an explanation of the incident and as reference for HPCC users to review in order to avoid creating jobs with similar issues in the future.</p>","tags":["lab notebook","incident report","I/O"]},{"location":"2023-08-30_LabNotebook_SlowdownIncidentReport/#factors-contributing-to-slowdown-on-hpcc","title":"Factors contributing to slowdown on HPCC","text":"","tags":["lab notebook","incident report","I/O"]},{"location":"2023-08-30_LabNotebook_SlowdownIncidentReport/#1-multiple-io-operations-on-small-files-in-the-same-folder-from-many-nodes","title":"1. Multiple I/O operations on small files in the same folder from many nodes","text":"<p>The cause of the slowdown was an excessive number of file input/output (I/O) operations that saturated the file systems transaction capacity, due to jobs frequently recording their state during the run. However, the impact on the file system was magnified by a number of other issues:</p> <ul> <li>Reading and writing to many small files simultaneously in general requires more transactions than a single, large file</li> <li>Reading and writing to many files in the same directory, especially the same file, creates additional overhead on the file system to lock and check files</li> <li>Reading and writing to the same folder from multiple nodes further increases the overhead </li> <li>Reading and writing to a folder where disaster recovery is enabled (home directories), further increases the overhead again</li> </ul> <p>We emphasize the above points to illustrate how different facets of the HPCC can magnify the effect of I/O operations and would make the following recommendations to users developing and using software on our system:</p> <ul> <li>Organizing input and output files for jobs in multiple folders can help reduce impact on the file system</li> <li>Non-essential/intermediate files can be stored in scratch space which is not backed up in disaster recovery</li> <li>Writing output in larger chunks to fewer files less frequently reduces the overall number of file transactions</li> <li>When checkpointing the state of a job, please consider how frequently the job state must be sampled to track significant changes. For example, for systems that converge to a solution, consider dynamically adjusting when the state is saved to be longer as the system converges</li> </ul>","tags":["lab notebook","incident report","I/O"]},{"location":"2023-08-30_LabNotebook_SlowdownIncidentReport/#2-running-the-maximum-of-jobs-for-long-periods-of-time","title":"2. Running the maximum of jobs for long periods of time","text":"<p>Users can queue up to 1000 and run up to 520 jobs at one time (except the scavenger queue). However, running 500+ jobs constantly for a long period of time increases the risk of compounding problems on HPCC. In this case, running multiple simultaneous jobs contributed both to the overall number of I/O operations and the overhead of operating out of the same folder for all jobs. Additionally, past cases where we have had to put holds on a user's account for overuse or unwanted behavior have often involved running a larger number of jobs with a looping process in each job.</p> <p>We do not want to discourage users from making use of HPCC resources to their fullest, but would have the following recommendations if you plan to queue a larger number of jobs such that you will have the maximum number of jobs running for a long period:</p> <ul> <li>Test and benchmark a single run of your jobs before queuing hundreds of instances. Proper testing and benchmarking will help you accurately estimate the resources you need per instance and avoid submitting hundreds of jobs to the queue only to have them fail.</li> <li>Do some arithmetic for all your jobs: How many files will 1000 instances of this job take? How much disk space will the output occupy? How much of the yearly allotment of your CPU/GPU hours will these jobs use?</li> <li>Consider setting the working directory independently for each job to reduce the overhead on the filesystem. Alternatively, you can copy your input data to /mnt/local within a job, which will store it in a temporary folder on the compute node (NOTE: since this a temporary folder, be careful to COPY not MOVE as files here will not be preserved after the job completes)</li> </ul>","tags":["lab notebook","incident report","I/O"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/","title":"(2023-09-21) Lab Notebooks: Example home directory software install (with links to videos)","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#installing-mfem-on-the-hpcc","title":"Installing MFEM on the HPCC","text":"<p>Our research group is developing a new MFEM module as part of an NSF CISS award.  Since we need to mess around with the installation it doesn't make sense to install globally.   The following are instructions I used to install MFEM in a folder in my home directory. The videos at the end of this post walk through my process to figure out what needed to be done.  </p> <p>I'm not sure many people will need these specific instructions but I also recorded some over-the-shoulder videos of the process. My hope is that some people may learn something watching these (very long) videos. </p> <p>See https://mfem.org/building/ for the official detailed installation instructions.</p> <ul> <li>Dirk</li> </ul> <p>The first thing you need to do is create a n empty working directory and cd into that empty directory</p>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#load-the-needed-modules","title":"Load the needed modules","text":"<pre><code>module swap GCCcore GCCcore/11.2.0\nmodule load mesa\nmodule load SDL2\nmodule load glew\nmodule load freetype\nmodule load fontconfig\nmodule load CMake\n</code></pre>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#or-better-yet-create-a-script-to-load-modules","title":"or, better yet, create a script to load modules","text":"<pre><code>cat &lt;&lt; EOF &gt; setup.sh\nmodule swap GCCcore GCCcore/11.2.0\nmodule load mesa\nmodule load SDL2\nmodule load glew\nmodule load freetype\nmodule load fontconfig\nmodule load CMake\nEOF\n</code></pre> <p>Run the script using the following command: <pre><code>source ./setup.sh\n</code></pre></p>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#download-the-needed-software","title":"Download the needed software","text":"<pre><code>wget https://github.com/mfem/tpls/raw/gh-pages/metis-4.0.3.tar.gz\n\nwget https://bit.ly/mfem-4-5-2 \nmv mfem-4-5.2 mfem-4-5-2.tgz\n\nwget https://bit.ly/glvis-4-2\nmv glvis-4-2 glvis-4-2.tgz\n\nwget https://github.com/hypre-space/hypre/archive/refs/tags/v2.29.0.tar.gz\n</code></pre>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#untar-folders","title":"Untar folders","text":"<pre><code>tar -xzvf glvis-4-2.tgz\ntar -xzvf hypre-v2.29.0.tar.gz\ntar -xzvf metis-4.0.3.tar.gz\ntar -xavf mfem-4-5-2.tgz\n</code></pre>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#build-hypre","title":"Build hypre","text":"<pre><code>cd hypre-2.29.0/src/\n./configure --disable-fortran\nmake -j\ncd ../..\nln -s hypre-2.29.0 hypre\n</code></pre>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#build-metis","title":"Build metis","text":"<pre><code>cd metis-4.0.3\nmake OPTFLAGS=-Wno-error=implicit-function-declaration\ncd ..\nln -s metis-4.0.3 metis-4.0\n</code></pre>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#build-mfem","title":"build mfem","text":"<pre><code>cd mfem-4.5.2\nmake parallel -j\ncd examples\nmake -j\ncd ../..\n</code></pre>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#download-glm","title":"download glm","text":"<pre><code>git clone https://github.com/g-truc/glm.git\n</code></pre>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#build-glvis","title":"build glvis","text":"<pre><code>cd glvis-4.2\nmake MFEM_DIR=../mfem-4.5.2 -j\n</code></pre>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#test-everything","title":"Test everything","text":"<p>Step 1: Start ondemand desktop and open a terminal. Step 2: Run the above module commands or type <code>source ./setup.sh</code>  Step 3: cd to glvis-4.2 directory and run <code>./glivs</code> Step 4: Open a second terminal Step 5: Run the above module commands or type <code>source ./setup.sh</code> Step 6: cd to <code>mfem-4.5.2/examples</code> and run one of the examples</p>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#videos","title":"Videos","text":"<p>The following videos were recorded when figuring out how to install FEM.  These are not edited and demonstrate the debugging process. Consider watching at double speed as not to be board.</p> <ul> <li>MFEM Install Part 1</li> <li>MFEM Install Part 2</li> <li>MFEM Install Part 3</li> <li>MFEM Install Part 4</li> <li>MFEM Install Instruction Write-up</li> <li>MFEM Install Test/Summary</li> </ul>","tags":["lab notebook","software"]},{"location":"2023-11-03_LabNotebook_VSCode_Singularity/","title":"(2023-11-03) Lab Notebook: Connecting to a Singularity container with VS Code","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","VS Code","VScode","Singularity"]},{"location":"2023-11-03_LabNotebook_VSCode_Singularity/#connecting-to-a-singularity-container-with-vs-code-updated-2023-11-03","title":"Connecting to a Singularity container with VS Code (updated 2023-11-03)","text":"","tags":["lab notebook","VS Code","VScode","Singularity"]},{"location":"2023-11-03_LabNotebook_VSCode_Singularity/#problem-setup","title":"Problem setup","text":"<p>A user has a Singularity image containing an installation of a Python installation they use to run their Python code which includes specific scientific libraries. This user also writes code in Visual Studio Code on Windows, and uses things like the integrated debugger, the integrated terminal, and the Python extension for autocomplete.</p> <p>The user would like to connect VS Code to their Singularity container so that VS Code can integrate the environment and Python installation within the container into its IDE features.</p>","tags":["lab notebook","VS Code","VScode","Singularity"]},{"location":"2023-11-03_LabNotebook_VSCode_Singularity/#solution","title":"Solution","text":"<p>This solution is based on the following Github comment and issue.</p>","tags":["lab notebook","VS Code","VScode","Singularity"]},{"location":"2023-11-03_LabNotebook_VSCode_Singularity/#set-up-the-ssh-configuration","title":"Set up the SSH configuration","text":"<p>If you haven't already, follow the instructions to connect to the HPCC through SSH.</p> <p>Press F1 to open the command pallette and begin typing \"Remote-SSH: Open SSH Configuration File...\", press Enter. Select the file <code>C:\\Users\\&lt;Account_Name&gt;\\.ssh\\config</code>.</p> <p>Add a new section to the SSH configuration file with the following lines, making sure to replace the pieces in brackets (<code>&lt;&gt;</code>) with your information:</p> <pre><code>Host &lt;container&gt;~intel18\n    HostName dev-intel18\n    User &lt;netid&gt;\n    ProxyJump &lt;netid&gt;@hpcc.msu.edu\n    RemoteCommand singularity shell &lt;path_to_singularity_image&gt;.sif\n    RequestTTY yes\n</code></pre> <p>Above, change <code>&lt;container&gt;</code> to a shortname for your container, <code>&lt;netid&gt;</code> to your NetID that you use to access the HPCC, and <code>&lt;path_to_singularity_image&gt;.sif</code> to be the location of your Singularity image on the HPCC. For example, I (<code>grosscra</code>) tested this using a Singularity image on the HPCC at <code>~/tensorflow_latest-gpu-jupyter.sif</code> with Tensorflow installed. So the configuration looks like</p> <pre><code>Host tf~intel18\n    HostName dev-intel18\n    User grosscra\n    ProxyJump grosscra@hpcc.msu.edu\n    RemoteCommand singularity shell /mnt/home/grosscra/tensorflow_latest-gpu-jupyter.sif\n    RequestTTY yes\n</code></pre> <p>You are also free to change the development node to whatever node you like in the <code>Host</code> and <code>HostName</code> lines.</p>","tags":["lab notebook","VS Code","VScode","Singularity"]},{"location":"2023-11-03_LabNotebook_VSCode_Singularity/#set-up-vs-code-settings","title":"Set up VS Code settings","text":"<p>We will now set up VS Code so that when it connects to the <code>&lt;container&gt;~intel18</code> host through the \"Remote - SSH\" extension, it will run the <code>RemoteCommand</code> set above to start the container.</p> <p>Open the VS Code Settings JSON files by pressing F1, and type Preferences: Open User Settings (JSON).</p> <p>Important: Make sure you edit the JSON files directly, do not manipulate the settings using VS Code's menu. There is a bug that does not allow you to set the correct settings from the menu.</p> <p>Add the following lines, or if they are already there, ensure that they have these values, replacing <code>&lt;container&gt;</code> with the short name you used in the SSH configuration above and <code>&lt;Windows_Account_Name&gt;</code> with your Windows account:</p> <pre><code>{\n    ...\n    \"remote.SSH.enableRemoteCommand\": true,\n    \"remote.SSH.useLocalServer\": true,\n    \"remote.SSH.serverInstallPath\": {\n        \"&lt;container&gt;~intel18\": \"~/.vs-code-container/&lt;container&gt;\"\n    },\n    \"remote.SSH.configFile\": \"C:/Users/&lt;Windows_Account_Name&gt;/.ssh/config\",\n    ...\n}\n</code></pre> <p>Note that the SSH configuration file path above uses forward-slashes instead of back-slashes.</p> <p>Additionally, if you have a <code>remote.SSH.remotePlatform</code> section in your <code>settings.json</code> with a line corresponding to your <code>&lt;container&gt;~intel18</code> section, remove it. For example, if my <code>settings.json</code> has the lines</p> <pre><code>{\n    ...\n    \"remote.SSH.remotePlatform\": {\n            \"amd20\": \"linux\",\n            \"tf~intel18\": \"linux\"\n        },\n    ...\n}\n</code></pre> <p>I should remove the <code>\"tf~intel18\": \"linux\"</code> line.</p>","tags":["lab notebook","VS Code","VScode","Singularity"]},{"location":"2023-11-03_LabNotebook_VSCode_Singularity/#kill-vs-code-servers","title":"Kill VS Code Servers","text":"<p>To ensure that your new connection does not reuse any old connections, you will need to kill those old connections if they exist.</p> <p>Press F1 and type the command \"Remote-SSH: Kill VS Code Server on Host...\". If the option appears, choose <code>intel18</code> (or whatever development node you're using). Repeat and choose the <code>&lt;container&gt;~intel18</code> option.</p> <p>Press F1 and type the command \"Remote-SSH: Kill Local Connection Server For Host...\". If the option appears, choose <code>intel18</code> (or whatever development node you're using). Repeat and choose the <code>&lt;container&gt;~intel18</code> option.</p>","tags":["lab notebook","VS Code","VScode","Singularity"]},{"location":"2023-11-03_LabNotebook_VSCode_Singularity/#start-the-connection","title":"Start the connection","text":"<p>You are now ready to start the connection. In the future, this is the only step you need to do.</p> <p>Press F1 and type the command \"Remote-SSH: Connect to Host...\". Choose the <code>&lt;container&gt;~intel18</code> option and you should be connected to a VS Code instance running inside your container.</p>","tags":["lab notebook","VS Code","VScode","Singularity"]},{"location":"2023-11-09_LabNotebook_Molpro_Parallel/","title":"(2023-11-09) Lab Notebook: Molpro Parallel","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Molpro"]},{"location":"2023-11-09_LabNotebook_Molpro_Parallel/#using-molpro-in-parallel-mode","title":"Using Molpro in parallel mode","text":"<p>Molpro 2022.3 can be run using multiple node and CPUs on both the development  and compute nodes. More general information can be found here:  https://www.molpro.net/manual/doku.php?id=running_molpro_on_parallel_computers</p> <p>However, on compute nodes you need to use specific commands so that Molpro will recognise your requested CPUs. For example, to run on 4 CPUs per node:</p> <pre><code>#Your other SBATCH options go above here\n#SBATCH --ntasks-per-node=4\n\n#Load Molpro 2022.3\nmodule purge\nmodule load GCC/11.2.0  OpenMPI/4.1.1 Molpro/molpro_2022.3\n\n#Gather the list of CPUs so Molpro can use them. \n#Add a job number to the file name if you need to run multiple jobs at once.\nsrun -s hostname &gt; nodefile\ncat ./nodefile\n\n#Set up your Molpro directories here as desired\nmkdir -p your_directory\n\n#Run Molpro, note the --nodefile option.\nmolpro -d your_directory -n 4 --nodefile nodefile your_molpro_file\n</code></pre>","tags":["lab notebook","Molpro"]},{"location":"2023-11-10_LabNotebook_AlphaFold2.3.2/","title":"(2023-11-10) Lab Notebook: AlphaFold 2.3.2 WorkInProgress","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details</p>","tags":["lab notebook","AlphaFold","Singularity"]},{"location":"2023-11-10_LabNotebook_AlphaFold2.3.2/#lab-notebook-instructions-for-alphafold-version-232-singularity-2023-11-10work-in-progress","title":"Lab Notebook --- Instructions for AlphaFold version 2.3.2, Singularity (2023-11-10)(WORK IN PROGRESS)","text":"<p>Currently, due to certain system limitations, AlphaFold version 2.3.0 and later cannot be installed on HPCC or run as Docker images.</p> <p>Therefore, we are working to make Singularity images created by third-party user available on HPCC (see https://github.com/prehensilecode/alphafold_singularity). As of the writing of this Lab Notebook, the most current image is 2.3.2-1</p> <p>These instructions are a work in progress for running AlphaFold version 2.3.2 using the singularity container found at:</p> <pre><code>/opt/software/alphafold/2.3.2/alphafold_2.3.2-1.sif\n</code></pre> <p>As with other containers in the <code>/opt/software/alphafold/</code> directory, AlphaFold 2.3.2 can be run via Singularity.</p> <p>Howevever, AlphaFold version after 2.3.0 use a database which is formatted differently than pevious versions. This database is located in <code>/mnt/research/common-data/alphafold/database_230</code>.</p> <pre><code>database_230\n\u2500\u2500 bfd -&gt; ../database/bfd/\n\u251c\u2500\u2500 mgnify\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mgy_clusters_2022_05.fa\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 mgy_clusters.fa -&gt; mgy_clusters_2022_05.fa\n\u251c\u2500\u2500 params\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 LICENSE\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_1_multimer_v3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_1.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_1_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_2_multimer_v3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_2.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_2_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_3_multimer_v3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_3_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_4_multimer_v3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_4.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_4_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_5_multimer_v3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_5.npz\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 params_model_5_ptm.npz\n\u251c\u2500\u2500 pdb70\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 md5sum\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_a3m.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_a3m.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_clu.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_cs219.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_cs219.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_hhm.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_hhm.ffindex\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 pdb_filter.dat\n\u251c\u2500\u2500 pdb_mmcif\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mmcif_files\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 100d.cif\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 101d.cif\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 101m.cif\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 102d.cif\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 102l.cif\n\u2502\u00a0\u00a0 |   ...\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 obsolete.dat\n\u251c\u2500\u2500 pdb_seqres\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 pdb_seqres.txt\n\u251c\u2500\u2500 small_bfd -&gt; ../database/small_bfd/\n\u251c\u2500\u2500 uniprot\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 uniprot.fasta\n\u251c\u2500\u2500 uniref30\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_a3m.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_a3m.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_cs219.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_cs219.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_hhm.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_hhm.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03.md5sums\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 UniRef30_2021_03.tar.1.gz\n\u2514\u2500\u2500 uniref90\n    \u2514\u2500\u2500 uniref90.fasta\n</code></pre>","tags":["lab notebook","AlphaFold","Singularity"]},{"location":"2023-11-10_LabNotebook_AlphaFold2.3.2/#runninging-alphafold-232-using-run_singluaritypy","title":"Runninging AlphaFold 2.3.2 using run_singluarity.py","text":"<p>To run AlphaFold 2.3.2, first load the following modules:</p> <pre><code>module load GCC/6.4.0-2.28  OpenMPI/2.1.2 Python/3.6.4 \nmodule load alphafold/2.3.2\n</code></pre> <p>The alphafold/2.3.2 module will set the ALPHAFOLD_DIR, ALPHAFOLD_DATADIR, and ALPHAFOLD_MODELS environment variables for you. </p> <p>We  recommend you use the python script \"run_singularity.py\" (which is also in /opt/software/alphafold/2.3.2/) to work with the Singularity image. This script helps automate many of the more challenging parts of using the image, such as correctly binding paths to your data directories and enabling GPU support. Below is an example of how to run this script:</p> <pre><code>python3 ${ALPHAFOLD_DIR}/run_singularity.py \\ \n    --use_gpu \\ #Use the GPU, which makes the neural network calculations faster\n    --output_dir=$output_dir \\ #Here is where I want to put the result\n    --data_dir=${ALPHAFOLD_DATADIR} \\ #Here is where the AlphaFold data like pdb sequences live\n    --fasta_paths=input.fasta \\ #Here is our input fasta sequence\n    --max_template_date=2020-05-14 \\ #When looking for PDB templates, this is the maximum date we will consider\n    --model_preset=monomer \\ #We are predicting a monomeric protein, change to \"multimer\" for multimer \n    --db_preset=reduced_dbs #Use the reduced database\n</code></pre> <p>Note that any of the flags normally passed to AlphaFold should be able to be passed through this script.</p> <p>If you would like to submit an AlphaFold job to SLURM, we have included an example script below. Note that you will need to adust the resource requests (mainly time) depending on the complexity of your protein.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name 2023alphafold\n#SBATCH --time=04:00:00\n#SBATCH --gres=gpu:1\n#SBATCH -C [nvf|nal|nif] ## We want the good GPUs\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=12G\n#SBATCH -o 2023.log\n\nmodule load GCC/6.4.0-2.28  OpenMPI/2.1.2 Python/3.6.4\nmodule load alphafold/2.3.2\n\necho \"Export AlphaFold variables\"\n# These variables are now set by the module\necho INFO: ALPHAFOLD_DIR=$ALPHAFOLD_DIR\necho INFO: ALPHAFOLD_DATADIR=$ALPHAFOLD_DATADIR\n\nexport output_dir=$SLURM_SUBMIT_DIR/2023 # you chnage this to whatever path you like\n\ncd $SLURM_SUBMIT_DIR\nmkdir -p $output_dir\ntimestamp=$(date)\necho \"Starting AlphaFold at $timestamp\"\n\npython3 ${ALPHAFOLD_DIR}/run_singularity.py \\\n    --use_gpu \\\n    --output_dir=$output_dir \\\n    --data_dir=${ALPHAFOLD_DATADIR} \\\n    --fasta_paths=8IBQ.fasta \\\n    --max_template_date=2023-08-01 \\\n    --model_preset=monomer \\\n    --db_preset=reduced_dbs\n\necho INFO: AlphaFold returned $?\n\ntimestamp=$(date)\necho \"Finishing AlphaFold at $timestamp\"\n</code></pre>","tags":["lab notebook","AlphaFold","Singularity"]},{"location":"2023-11-10_LabNotebook_AlphaFold2.3.2/#running-the-singularity-image-manually","title":"Running the singularity image manually","text":"<p>If for whatever reason you need to manually run AlphaFold from the singularity image, we recommend you still run \"run_singularity.py\" first as this script will print the \"singularity run ...\" command it generates. It will be much easier to work from this command, which should have all of the bind paths properly set for the image, than to try to write your own command from scratch.</p>","tags":["lab notebook","AlphaFold","Singularity"]},{"location":"2023-11-10_LabNotebook_AlphaFold2.3.2/#additional-resources-and-acknowledgement","title":"Additional Resources and Acknowledgement","text":"<p>I would like to thank Dr. Josh Vermaas for helping me troubleshoot this new image and providing the example SLURM script. </p> <p>For additional details about the Singularity image and run_singularity.py script, please see the Github of the original author:</p> <p>https://github.com/prehensilecode/alphafold_singularity</p>","tags":["lab notebook","AlphaFold","Singularity"]},{"location":"2023-11-21_LabNotebook_Git_Research_Spaces_/","title":"(2023-11-21) Lab Notebook: Git in Research Spaces","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p> <p>Git is a powerful version control tool. However, it does  not interact nicely with multiple users accessing and modifying the same  repository in a shared space.</p> <p>Your research group may have software that you all contribute to and use. If you want to version control it with git, we recommend the following practice:</p> <ul> <li>The git repository in a research space should be pull ONLY. This prevents group permissions issues. Periodic pulls should be run to keep the repository up-to-date with the remote.</li> <li>Development of the software should be handled by group members in their home directories using branches  that can be merged and then pulled into the research space repository.</li> <li>If the PI of the research group would prefer control of the research space repository, it can be placed in a directory with explicit read-only group permissions.</li> </ul> <p>In general, be aware of permissions management  on the HPCC to maintain a working repository.</p>","tags":["lab notebook","git"]},{"location":"2024-01-13_LabNotebook_NFSv4ACLs/","title":"(2024-01-13) Lab Notebook: Sharing directories with other HPCC users","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","files"]},{"location":"2024-01-13_LabNotebook_NFSv4ACLs/#overview","title":"Overview","text":"<p>MSU HPCC's GPFS filesystem supports NFS V4 access control lists (ACLs). By modifying ACLs of your directories and files, you will be able to fine control access by other users. This tutorial provides a step-by-step instruction for setting up file sharing through modifying ACLs. We assume that you've put all your files in a single directory and are ready to share that directory with another HPCC user. </p> <p>To deal with ACLs, you will use three  commands: 1) <code>mmgetacl</code> for obtaining existing ACLs; 2) <code>mmputacl</code> for changing ACLs; and 3) <code>mmdelacl</code> for deleting ACLs. Compared with the basic <code>chmod</code> utility, being able to modify ACLs gives users more fine-grained control over file access.</p>","tags":["lab notebook","files"]},{"location":"2024-01-13_LabNotebook_NFSv4ACLs/#viewing-existing-acls","title":"Viewing existing ACLs","text":"<p>To view ACL of a directory or a file, we use the command <code>mmgetacl</code>:</p> <pre><code>$ mmgetacl &lt;my dir&gt;\n$ mmgetacl &lt;my file&gt;\n</code></pre> <p>The output should be mostly self-explanatory; for a complete description of the format of ACL entries, refer to IBM NFS V4 ACL Syntax or IU's supercomputer user's guide.</p>","tags":["lab notebook","files"]},{"location":"2024-01-13_LabNotebook_NFSv4ACLs/#example-of-sharing-a-directory","title":"Example of sharing a directory","text":"<p>Let's say under <code>myuser</code>'s home directory, there is a subdirectory named <code>acl_demo</code>. It contains two files and two directories (each dir has one file inside), as shown below. </p> <pre><code>$ ll acl_demo/\ntotal 17K\ndrwxrwx--- 2 myuser mygroup 8.0K Jan  9 12:38 dir1/\ndrwxrwx--- 2 myuser mygroup 8.0K Jan  9 12:38 dir2/\n-rwxrwx--- 1 myuser mygroup  151 Jan  9 22:33 file1\n-rwxrwx--- 1 myuser mygroup  194 Jan  9 12:37 file2\n\n$ tree acl_demo/\nacl_demo/\n\u251c\u2500\u2500 dir1\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ffile1\n\u251c\u2500\u2500 dir2\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ffile2\n\u251c\u2500\u2500 file1\n\u2514\u2500\u2500 file2\n\n2 directories, 4 files\n</code></pre> <p>Our goal is to share the entire <code>acl_demo</code> directory with our collaborator  <code>collab</code> on the HPCC. Specifically, we will grant read-only access to <code>collab</code>.</p> <p>First, we need to get the original ACLs for the <code>acl_demo</code> directory and one of the files (say, <code>file1</code>) inside it, using <code>mmgetacl</code> that we've used earlier. The original ACL of a directory/file when created is determined by umask set by <code>myuser</code>. We will save their ACLs to two files in <code>~/tmp</code>, since we want to differentiate directories from files when manipulating their ACLs. Make sure you have created a subdirectory <code>~/tmp</code> in your home directory beforehand.</p> <pre><code>cd # go to home dir (~/) \nmmgetacl acl_demo &gt; ~/tmp/acl-dir.txt # ACL of dirs\nmmgetacl acl_demo/file1 &gt; ~/tmp/acl-file.txt # ACL of files\n\nmore ~/tmp/acl-*.txt | cat # take a look at the two ACL files generated\n</code></pre> <p>Then, we will change the ACLs (for directories and files) using the  <code>mmputacl</code>  command. Its usage is simple:</p> <pre><code>mmputacl -i &lt;ACL entries&gt; &lt;file/dir name&gt;\n</code></pre> <p>The above shows that <code>mmputacl</code> takes an input file (<code>-i</code>) containing user-defined ACL entries. It's important to note that, in this example, we will need to use <code>mmputacl</code> to change ACLs for both directories and files.  This also means that we need to prepare two different input files for <code>mmputacl</code>.</p> <p>To give <code>collab</code> read and execute (execute permission is needed to navigate to the directory) permission of a directory, we only need to add an entry to that directory's existing ACL. The same applies to changing a file.  Below we display two files that each contain the additional entry for granting <code>rx</code> (read and execute) access to user <code>collab</code>:</p> <pre><code>$ cat tmp/dir.acl\nuser:collab:r-xc:allow:FileInherit:DirInherit\n (X)READ/LIST (-)WRITE/CREATE (-)APPEND/MKDIR (X)SYNCHRONIZE (X)READ_ACL  (X)READ_ATTR  (X)READ_NAMED\n (-)DELETE    (-)DELETE_CHILD (-)CHOWN        (X)EXEC/SEARCH (-)WRITE_ACL (-)WRITE_ATTR (-)WRITE_NAMED\n\n$ cat tmp/file.acl\nuser:collab:r-xc:allow\n (X)READ/LIST (-)WRITE/CREATE (-)APPEND/MKDIR (X)SYNCHRONIZE (X)READ_ACL  (X)READ_ATTR  (X)READ_NAMED\n (-)DELETE    (-)DELETE_CHILD (-)CHOWN        (X)EXEC/SEARCH (-)WRITE_ACL (-)WRITE_ATTR (-)WRITE_NAMED\n</code></pre> <p>Above, selected permissions are marked with an <code>X</code>; permissions that are not selected are marked with a <code>-</code>. ACLs for directories have two additional flags (<code>FileInherit</code> and <code>DirInherit</code>) as compared to those for files.  These two files (i.e., <code>dir.acl</code> and <code>file.acl</code>) are saved in <code>~/tmp</code> as before. In practice, you can copy the above content down to your own files and make changes as needed, such as user name and/or permission choices.</p> <p>Our next step is to combine the new entry (which corresponds to the modified permission) with existing ACL. To do so, in the code below we append our newly added entry to the current ACL, for both directories and files. In other words, the two previously generated files, <code>acl-dir.txt</code> and <code>acl-file.txt</code> have been updated now (take a look at the two files to make sure everything is in order).</p> <pre><code>cat ~/tmp/dir.acl &gt;&gt; ~/tmp/acl-dir.txt\ncat ~/tmp/file.acl &gt;&gt; ~/tmp/acl-file.txt\n</code></pre> <p>With all the preparation completed, we are now ready to call <code>mmputacl</code>. In order to make sure all the files and directories inside will be accessible to <code>collab</code>, we need to apply ACL changes at all levels, or, recursively. The following commands accomplish this.</p> <pre><code>cd # go to home dir (~/) \nmmputacl acl_demo -i ~/tmp/acl-dir.txt\ncd acl_demo\nfind . -type d -exec mmputacl \\{} -i ~/tmp/acl-dir.txt \\;\nfind . -type f -exec mmputacl \\{} -i ~/tmp/acl-file.txt \\; \n</code></pre> <p>Here is a breakdown of the above code:</p> <ol> <li>The first line applies the change to the very top level directory <code>acl_demo</code>.  This allows <code>collab</code> to traverse into this directory.</li> <li>The second line brings us inside the <code>acl_demo</code> directory.</li> <li>The third line recursively changes ACLs for subdirectories (<code>dir1</code> and <code>dir2</code>), based on permissions defined in  <code>acl-dir.txt</code>. Furthermore, if there are subdirectories within <code>dir1</code> and/or <code>dir2</code>, their ACLs will be changed in the same manner.</li> <li>The fourth line recursively changes ACLs for files (<code>file1</code>, <code>file2</code>, <code>dir1/ffile1</code>, and <code>dir2/ffile2</code>), based on permissions defined in  <code>acl-file.txt</code>.</li> </ol>","tags":["lab notebook","files"]},{"location":"2024-01-13_LabNotebook_NFSv4ACLs/#optional-deleting-acls","title":"Optional: deleting ACLs","text":"<p>If we want to disable access from the collaborator after file sharing is complete,  we can use <code>mmdelacl</code> to delete extended ACLs we've added to <code>acl_demo</code>. Then the ACLs will be reset to their original entries. To perform deletion, we can use the following commands:</p> <pre><code>cd # go to home dir (~/) \nmmdelacl acl_demo\ncd acl_demo\nfind . -type d -exec mmdelacl \\{} \\;\nfind . -type f -exec mmdelacl \\{} \\; \n</code></pre> <p>After deletion, we can run <code>mmgetacl</code> on those files and directories again, to confirm.</p>","tags":["lab notebook","files"]},{"location":"ABySS/","title":"ABySS","text":"<p>ABySS is a de novo, parallel, paired-end sequence assembler. It can run as an MPI job in the HPCC cluster. The latest version currently installed on the HPCC is 2.1.5, which can be loaded by</p> <pre><code>module load ABySS/2.1.5\n</code></pre> <p>You can optionally load other tools as needed, provided that they have been installed under the same toolchain environment as ABySS/2.1.5. For example,</p> <pre><code>module load BEDTools/2.27.1 SAMtools/1.9 BWA/0.7.17\n</code></pre> <p>is valid after you've loaded ABySS.</p> <p>A sample SLURM script is below.</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=abyss_test  \n#SBATCH --nodes=4  \n#SBATCH --ntasks-per-node=2  \n#SBATCH --mem-per-cpu=5G  \n#SBATCH --time=1:00:00  \n#SBATCH --output=%x-%j.SLURMout\n\necho \"$SLURM_JOB_NODELIST\"\n\nmodule load ABySS/2.1.5\n\nexport OMPI_MCA_mpi_warn_on_fork=0  \nexport OMPI_MCA_mpi_cuda_support=0\n\nabyss-pe k=25 name=test in='/mnt/research/common-data/Bio/ABySS/test-data/reads1.fastq /mnt/research/common-data/Bio/ABySS/test-data/reads2.fastq' v=-v np=8 j=2\n</code></pre> <p>This script launches an MPI job by requesting 8 processes; they are distributed on 4 nodes (<code>--nodes=4</code>) with two processes each (<code>--ntasks-per-node=2</code>). Accordingly, in the <code>abyss-pe</code> command line, we specify <code>np=8</code>. Regarding parameter j, the manual states</p> <p>The paired-end assembly stage is multithreaded, but must run on a single machine. The number of threads to use may be specified with the parameter j. The default value for j is the value of np.</p> <p>So, rather than using np as the default value for j, we set j = 2 which is the number of CPUs per node as requested (in this case \"task\" is equivalent to CPU). To submit the job,</p> <p><code>sbatch --constraint=\"[intel16|intel18]\"</code></p> <p>While the job is running, you may look at the SLURM output file, in this example,\u00a0<code>abyss_test-&lt;job ID&gt;.SLURMout</code>, which has a lot of running log, including the following:</p> <p><code>Running on 8 processors</code> <code>6: Running on host lac-391</code> <code>0: Running on host lac-194</code> <code>2: Running on host lac-225</code> <code>4: Running on host lac-287</code> <code>7: Running on host lac-391</code> <code>3: Running on host lac-225</code> <code>1: Running on host lac-194</code> <code>5: Running on host lac-287</code></p>"},{"location":"AMD_Optimizing_CPU_Libraries_and_Compilers/","title":"AMD Optimizing CPU Libraries and Compilers","text":"<p>With the purchase of amd20 cluster, we've installed AOCC (AMD Optimizing C/C++ Compiler) compiler system and AOCL (AMD Optimizing CPU Libraries) on the HPCC system. Here we introduce how to use the installed compilers and libraries.</p> <p>Based on LLVM 10.0 release, AMD compilers use the commands clang, clang++ and flang to compile c, c++ and fortran codes respectively. Users can simply load an AOCC module and use the command directly. To find out the version of AOCC installed in HPCC, please run the following command\u00a0on a dev node. All modules should be able to load directly.</p> <pre><code> module spider AOCC\n</code></pre> <p>AOCL are a set of numerical libraries specifically tuned for the AMD EPYC processor family. The Libraries can work with either AOCC or GCC compilers. Users need to load a version of GCC or AOCC module before AOCL can be loaded. All available versions of AOCL can be found by running the following command\u00a0on a dev node. Using AOCL on the new amd20 nodes is considered to provide better performance than using GCC compiled libraries (such as OpenBLAS, ScaLAPACK ...). User can find out how to use the libraries and the linkers from the  documentation.</p> <pre><code> module spider AOCL\n</code></pre> <p>Besides AOCC and AOCL, a version of OpenMPI compiled with AOCC (version 2.2.0) is also installed in case users would like to test MPI programs with AMD compilers. Users can simply load the <code>aompi</code> toolchain by running the following command\u00a0on a dev node to load both AOCC and OpenMPI (version 4.0.3).\u00a0</p> <pre><code>ml -* aompi\n</code></pre> <p>An\u00a0<code>aoacl</code> toolchain is also available and able to load the triple modules: AOCC, OpenMPI and AOCL at a time.</p> <p>Note</p> <p>We do not currently have the latest AOCL module installed. You must instead install and link from your home directory if you would like to use the latest AOCL with the latest AOCC.</p> <p>For more information, please see AMD's pages on AOCC and AOCL.</p>"},{"location":"ANSYS/","title":"ANSYS","text":"<p>These instructions are for using Ansys on the current HPCC environment that uses the SLURM scheduler.</p>"},{"location":"ANSYS/#license-issues-version-192-and-older","title":"License Issues (version 19.2 and older)","text":"<p>There is an issue with Ansys licensing due to changes made over the many versions HPCC has installed.\u00a0\u00a0 If you have license issues when Using Ansys, here is the work-around:</p> <p>1. Start an interactive desktop session in OnDemand  or start an X11  terminal (with MobaXterm/Windows or XQuartz/Mac) (see Connect to HPCC System ).\u00a0</p> <p>2. SSH Connect to any development node, remembering to add -X options. \u00a0\u00a0</p> <p>3.\u00a0 Start this program on any dev node.\u00a0\u00a0 it launches a GUI </p> <p>/opt/software/ANSYS/19.2/ansys_inc/shared_files/licensing/lic_admin/anslic_admin  </p> <ol> <li>On the left side of the window are three buttons. Click the button \"set License Preferences for User \\&lt;username&gt;\". A new window will open</li> </ol> <p>5. select Release 19.2 in that new window and click OK</p> <p>6. another window will open with tabs across the top and two options in the bottom that are the same for each tab. On the bottom, click the option for \"Use a seperate license for each application\". It doesn't matter which Tab you've slected (Solver/PrePost/etc). That setting should be the same for all tabs.</p> <p>7. click OK, which closes that window.</p> <p>8. In the original Ansys license utility, click File, and then \"exit\" to close it. This modifies the config file in your home directory.</p> <p>9. Close any current sessions in which you running Ansys and start it again on any method (dev node, in 'salloc' interactive job etc). You should now be able to use the features you needed before.</p>"},{"location":"ANSYS/#guidelines-for-scheduling-parallel-mpi-jobs","title":"Guidelines for scheduling parallel (MPI) jobs","text":"<p>Here are some guidelines for requesting resources.\u00a0\u00a0\u00a0</p>"},{"location":"ANSYS/#use-ntasks-instead-of-nodes","title":"Use --ntasks instead of nodes","text":"<p>Note that <code>-N</code>\u00a0 or <code>-\u2013nodes=</code> will request that number of unique computers, but what most users want is the number of tasks across nodes.\u00a0 Use the number of tasks you requested instead of number of nodes for the `-t`` parameter:</p> <p><code>-t $SLURM_NTASKS</code></p>"},{"location":"ANSYS/#dont-forget-to-request-memory","title":"Don't forget to request memory","text":"<p>Request memory per task, and since the default is to have 1 cpu per task, you can request memory using e.g. <code>--mem-per-cpu=1gb</code></p>"},{"location":"ANSYS/#create-a-temporary-file-for-node-list","title":"Create a temporary file for node list","text":"<p>Inside the job, Fluent requires a file of a particular format ,and the SLURM node file doesn't work.\u00a0\u00a0 This seems to work</p> <pre><code># create and save a unique temporary file \nFLUENTNODEFILE=`mktemp`\n# fill that tmpfile with node list Fluent can use\nscontrol show hostnames &gt; $FLUENTNODEFILE\n# in your fluent command, use this parameter\n-cnf=$FLUENTNODEFILE\n</code></pre> <p>Example fluent Job script (using Intel compiler). \u00a0 Increase tasks and memory as needed</p> <p>Ansys/Fluent job</p> <pre><code>#!/usr/bin/bash --login\n# example 1 hour job with ntasks across any number of nodes\n# adjust the ram and tasks as needed\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=1\n#SBATCH --ntasks=10\n#SBATCH --mem-per-cpu=1gb \n\n\n# create host list\nNODEFILE=`mktemp`\nscontrol show hostnames &gt; $NODEFILE\n\n# Load the ansys/cfx v19.2 module\nmodule load ANSYS\n\n\n# The Input file\nDEF_FILE=baseline.def  # this file is something you have to provide!\n\n\ncfx5solve -def $DEF_FILE -parallel -par-dist $NODEFILE -start-method \"Platform MPI Distributed Parallel\"&gt; cfx5.log\n</code></pre> <p>After you have logged into a development nodes with an X11 terminal (or use the OnDemand desktop as described above),\u00a0 You may run ANSYS tools in parallel and interactively as follows.</p> <pre><code># start a approximately 4 hour interactive job with 10 tasks.  Adjust tasks and memory as needed\n# you'll have 4 hours to work. You must be in a X11 terminal for this to work\n\n salloc --ntasks=10 --cpus-per-task=1 --mem-per-cpu=1gb --time=3:59:00 --x11 \n\n# wait for log-in and then...\n\n# load module\nml intel ansys\n\n# this creates a temporary file and fills it with node list Fluent can use\nNODEFILE=`mktemp`\nscontrol show hostnames &gt; $FLUENTNODEFILE\n\n#for example, run the workbench \nrunwb2\n\u00a0\n# after running workbench you can start fluent directly\n# note we are using Intel mpi\nfluent 3ddp -t $SLURM_NTASKS -mpi=intel -cnf=$FLUENTNODEFILE -ssh\n</code></pre>"},{"location":"ANSYS/#cfx5-solver","title":"CFX5 Solver","text":"<p>This solver uses a different hosts file format for the par-dist parameter.\u00a0 The following uses an example Definition file provided by Ansys 19.2.\u00a0</p> <p>The batch script will adapted the par-dist file depending on how you specify tasks and tasks-per-node (the example below does not specify tasks per node).\u00a0\u00a0\u00a0 Code is taken from https://secure.cci.rpi.edu/wiki/index.php?title=CFX.</p> <p>CFX5 Solver Example sbatch</p> <pre><code>#!/usr/bin/bash --login\n# example 1 hour job with ntasks across any number of nodes\n# adjust the ram and tasks as needed\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=1 \n#SBATCH --ntasks=10\n#SBATCH --mem-per-cpu=1gb \n\nmodule load ansys\n\n# codde adapts the hosts file depending on if you use multiple nodes and the tasks-per-node option.   \nsrun hostname -s &gt; /tmp//hosts.$SLURM_JOB_ID\nif [ \"x$SLURM_NPROCS\" = \"x\" ]; then\n    if [ \"x$SLURM_NTASKS_PER_NODE\" = \"x\" ];then\n     SLURM_NTASKS_PER_NODE=1\n     fi\n     SLURM_NPROCS=`expr $SLURM_JOB_NUM_NODES \\* $SLURM_NTASKS_PER_NODE`\nfi\n# use ssh instead of rsh\nexport CFX5RSH=ssh\n# format the host list for cfx\ncfxHosts=`tr '\\n' ',' &lt; /tmp//hosts.$SLURM_JOB_ID`\n\n# example file\nDEF=/opt/software/ANSYS/19.2/ansys_inc/v192/CFX/examples/StaticMixer.def\n# run the partitioner and solver\ncfx5solve -par -par-dist \"$cfxHosts\" -def $DEF -part $SLURM_NPROCS -start-method \"Platform MPI Distributed Parallel\"\n# cleanup\nrm /tmp/hosts.$SLURM_JOB_ID\n\n\n# output will be in a file named like StaticMixer_001.out and StaticMixer_001.res\n</code></pre>"},{"location":"AlphaFold_installed_in_HPCC/","title":"AlphaFold installed in HPCC","text":"<p>The following command can be used to find all versions of AlphaFold installed on HPCC:</p> <pre><code>[UserName@dev-amd20-v100 ~]$ ml spider AlphaFold\n</code></pre> <p>To find how to load a specific AlphaFold version, where <code>&lt;version&gt;</code> is the version of AlphaFold to be load, use:</p> <pre><code>[UserName@dev-amd20-v100 ~]$ ml spider AlphaFold/&lt;version&gt;\n</code></pre> <p>All AlphaFold versions use the same data structure and location\u00a0<code>/mnt/research/common-data/alphafold/database</code> as mentioned in\u00a0Alphafold via Singularity.</p>","tags":["tutorial","AlphaFold"]},{"location":"AlphaFold_installed_in_HPCC/#illegal-memory-address","title":"Illegal memory address","text":"<p>If CUDA_ERROR_ILLEGAL_ADDRESS or an illegal memory access was encountered while running AlphaFold, this is usually due to not enough memory in GPU cards (from the python package \"jax\"). Please try to request the high memory GPU card A100 (79GB) to run your AlphaFold jobs.</p> <p>You can also set the environment variable to see if the allocated memory is enough or not:</p> <pre><code>export XLA_PYTHON_CLIENT_ALLOCATOR=platform\n</code></pre> <p>Please see GPU memory allocation for more information.</p>","tags":["tutorial","AlphaFold"]},{"location":"AlphaFold_installed_in_HPCC/#alphafold-example","title":"AlphaFold example","text":"<p>Users can get an example of AlphaFold to run on HPCC nodes. Log into HPCC and ssh to a dev node with GPU cards, then run the following command to copy the example directory <code>AlphaFold</code> to the current directory:</p> <pre><code>[UserName@dev-amd20-v100 ~]$ getexample AlphaFold\n</code></pre> <p>(If the above command fails, you may need to manually load powertools with <code>module load powertools</code>)</p> <p>After you cd to the directory, you should be able to see the files inside:</p> <pre><code>[UserName@dev-amd20-v100 ~]$ cd AlphaFold\n[UserName@dev-amd20-v100 AlphaFold]$ ls\ndata.fasta  README  slurm_script.sb\n[UserName@dev-amd20-v100 AlphaFold]$ cat slurm_script.sb\n#!/bin/bash\n#SBATCH --job-name AlphaFold\n#SBATCH --time=12:00:00\n#SBATCH --gpus=4\n#SBATCH --cpus-per-task=24\n#SBATCH --mem=90GB\n#SBATCH --constraint=[intel18|amr|nvf|nal|nif]\n\nexport NVIDIA_VISIBLE_DEVICES=\"${CUDA_VISIBLE_DEVICES}\"\n\nml -* fosscuda/2020a AlphaFold/2.0.0\n\nalphafold --fasta_paths=$PWD/data.fasta --output_dir=$PWD --preset=casp14 --max_template_date=2020-05-14 --model_names=model_1\n\nscontrol show job ${SLURM_JOBID}\njs -j ${SLURM_JOBID}\n</code></pre> <p>The job script file <code>slurm_script.sb</code> shows how to load the <code>AlphaFold</code> module and run the command <code>alphafold</code>. Since most of the specifications and variables have been set in the <code>alphafold</code> command script and the module file, five options for the job are  specified in the command line:</p> <pre><code>--fasta_paths\n--output_dir\n--preset\n--max_template_date\n--model_names\n</code></pre> <p>Users can look into the AlphaFold documentation or use the commands:</p> <pre><code>[UserName@dev-amd20-v100 AlphaFold]$ alphafold --help\n</code></pre> <p>to find out how to use all options after you load the AlphaFold module.</p>","tags":["tutorial","AlphaFold"]},{"location":"Alphafold_via_Singularity/","title":"AlphaFold via Singularity","text":"<p>Note</p> <p>These instructions are for AlphaFold Singularitiy container for version 2.2.2 and earlier. If you are using an AlphFold 2.3.0 or later container, please see the update instructions at  https://docs.icer.msu.edu/2023-07-18_LabNotebook_AlphaFold2.3.1_WorkInProgress/</p> <p>AlphaFold can be run via Singularity.</p> <p>AlphaFold database is located in <code>/mnt/research/common-data/alphafold/database</code>.</p> <pre><code>database\n\u251c\u2500\u2500 bfd\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt_a3m.ffdata\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt.tar.gz\n\u251c\u2500\u2500 mgnify\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 mgy_clusters_2018_12.fa\n\u251c\u2500\u2500 params\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_1.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_1_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_2.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_2_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_3_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_4.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_4_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_5.npz\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 params_model_5_ptm.npz\n\u251c\u2500\u2500 pdb70\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 md5sum\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_a3m.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_a3m.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_clu.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_cs219.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_cs219.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_hhm.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_hhm.ffindex\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 pdb_filter.dat\n\u251c\u2500\u2500 pdb_mmcif\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mmcif_files\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 obsolete.dat\n\u251c\u2500\u2500 small_bfd\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 bfd-first_non_consensus_sequences.fasta\n\u251c\u2500\u2500 uniclust30\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 uniclust30_2018_08\n\u2514\u2500\u2500 uniref90\n    \u251c\u2500\u2500 uniref90.fasta\n    \u2514\u2500\u2500 uniref90.fasta.1.gz\n</code></pre> <p>Before running AlphaFold, you need to set</p> <pre><code>export ALPHAFOLD_DATA_PATH=\"/mnt/research/common-data/alphafold/database\"  \nexport ALPHAFOLD_MODELS=\"/mnt/research/common-data/alphafold/database/params\"\n</code></pre> <p>To run alphafold, please use the following template (for more information about options/flags, please refer to the README on Github).</p> <p>In the script, <code>input.fasta</code> is your input data, and\u00a0you need to set up output_dir. Since the command <code>/usr/bin/hhsearch</code> inside the container does not work on intel14 nodes (<code>Illegal instruction</code>), please use the <code>SBATCH</code> option\u00a0<code>--constraint</code> in the job script.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name alphafold-run\n#SBATCH --time=08:00:00\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=20G\n#SBATCH --constraint=\"[intel16|intel18|amd20]\"\n\nexport ALPHAFOLD_DATA_PATH=\"/mnt/research/common-data/alphafold/database\"\nexport ALPHAFOLD_MODELS=\"/mnt/research/common-data/alphafold/database/params\"\n\nsingularity run --nv \\\n-B $ALPHAFOLD_DATA_PATH:/data \\\n-B $ALPHAFOLD_MODELS \\\n-B .:/etc \\\n--pwd  /app/alphafold /opt/software/alphafold/2.0.0/alphafold.sif \\\n--data_dir=/data \\\n--output_dir=/mnt/gs18/scratch/users/my_id/alphafold/output \\\n--fasta_paths=/mnt/gs18/scratch/users/my_id/alphafold/input.fasta  \\\n--uniref90_database_path=/data/uniref90/uniref90.fasta  \\\n--mgnify_database_path=/data/mgnify/mgy_clusters_2018_12.fa   \\\n--bfd_database_path=/data/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\\n--uniclust30_database_path=/data/uniclust30/uniclust30_2018_08/uniclust30_2018_08 \\\n--pdb70_database_path=/data/pdb70/pdb70  \\\n--template_mmcif_dir=/data/pdb_mmcif/mmcif_files  \\\n--obsolete_pdbs_path=/data/pdb_mmcif/obsolete.dat \\\n--max_template_date=2020-05-14   \\\n--model_names=model_1 \\\n--preset=casp14\n</code></pre>","tags":["tutorial","AlphaFold","Singularity"]},{"location":"An_SSH_tunneling_via_multiple_hops/","title":"SSH tunneling to directly access development nodes","text":"<p>In some cases, you want to access dev nodes directly from your local machine instead of manually connecting to the gateway first. There are two ways to accomplish this.</p>","tags":["how-to guide","ssh"]},{"location":"An_SSH_tunneling_via_multiple_hops/#using-proxyjump","title":"Using ProxyJump","text":"<p>We can use a configuration file to set custom names for SSH hosts and simplify the process of connecting. On your local machine, open the <code>~/.ssh</code> directory with the command <code>cd ~/.ssh</code> and create a file called <code>config</code> by running <code>touch config</code> (or edit it, if it already exists).</p> <p>For Windows users using OpenSSH for Windows</p> <p>For most Windows users using OpenSSH for Windows (e.g., to connect to the HPCC through VS Code), the SSH config file should be located at <code>C:\\Users\\&lt;username&gt;\\.ssh\\config</code> where <code>&lt;username&gt;</code> is your username on your Windows computer (or more generally, <code>%userprofile%\\.ssh\\config</code>). Note that the <code>config</code> file does not have a file extension and should not be considered a text (or any other) type of file by Windows. You can open your config file in VS Code by pressing the F1 key, then typing <code>Open SSH Configuration file</code>.</p> <p>In the following example <code>.ssh/config</code> file, we have defined hosts for all seven development nodes. Each entries each contain the line <code>ProxyJump &lt;netid&gt;@hpcc.msu.edu</code> to be able to connect to the development nodes through the gateway nodes.</p> <p>To use the following template <code>.ssh/config</code>,  change all instances of <code>&lt;netid&gt;</code> to your NetID that you use to login to the HPCC.</p> <pre><code>Host intel16\n    HostName dev-intel16\n    User &lt;netid&gt;\n    ProxyJump &lt;netid&gt;@hpcc.msu.edu\n\nHost intel18\n    HostName dev-intel18\n    User &lt;netid&gt;\n    ProxyJump &lt;netid&gt;@hpcc.msu.edu\n\nHost amd20\n    HostName dev-amd20\n    User &lt;netid&gt;\n    ProxyJump &lt;netid&gt;@hpcc.msu.edu\n\nHost k80\n    HostName dev-intel16-k80\n    User &lt;netid&gt;\n    ProxyJump &lt;netid&gt;@hpcc.msu.edu\n\nHost v100\n    HostName dev-amd20-v100\n    User &lt;netid&gt;\n    ProxyJump &lt;netid&gt;@hpcc.msu.edu\n</code></pre> <p>With this file, you can connect to any a development node from your local machine with, e.g., <code>ssh intel18</code> or <code>ssh k80</code>. Your connection will automatically be routed through the HPCC gateway.</p> <p></p> <p>Note</p> <p>With SSH Key-Based Authentication you don't have to type your password when you login.</p>","tags":["how-to guide","ssh"]},{"location":"An_SSH_tunneling_via_multiple_hops/#using-port-forwarding","title":"Using port forwarding","text":"<p>Instead of using ProxyJump, you can use port forwarding. For this method, you need to open two terminals on your local machine.</p> <p>1st terminal (left in the picture): type </p> <pre><code>ssh -L 1234:dev-intel18:22\u00a0&lt;your_net_id&gt;@gateway.hpcc.msu.edu\n</code></pre> <p>You can change 1234 to any number larger than 1024 (1234 here is a port number you are using). You can change dev-intel18 to any dev-node name, but 22 (port number of dev node) should be remained. For example,\u00a0</p> <pre><code>ssh -L 4321:dev-intel16:22 &lt;your_net_id&gt;@gateway.hpcc.msu.edu\n</code></pre> <p>is also working.</p> <p>2nd terminal (right in the picture): type </p> <pre><code>ssh -p 1234 &lt;your_net_id&gt;@localhost\n</code></pre> <p>If it is the first time, it would request connection confirmation. type yes. Then you will arrive at the dev-node on the 2nd terminal.</p> <p></p>","tags":["how-to guide","ssh"]},{"location":"Application_Icons_on_Desktop/","title":"Application Icons on Desktop","text":"<p>It is much easier to execute your favorite apps by clicking icons on the desktop just like using your Windows or Mac PC. We can certainly do this through Open OnDemand. There are some app icons already created in the directory <code>/opt/software/OnDemand/Desktop-Icons</code>, where you can see them by listing the folder:</p> <pre><code>$ ls /opt/software/OnDemand/Desktop-Icons\nANSYS.desktop             Dolphin.desktop  GaussView.desktop  Maestro.desktop   rstudio.desktop  tecplot.desktop           VMD.desktop\nchromium-browser.desktop  firefox.desktop  GSEA.desktop       MATLAB.desktop    sas.desktop      Terminal.desktop\nCOMSOL.desktop            Fluent.desktop   Jupyter.desktop    Nautilus.desktop  Stata.desktop    User's Anaconda3.deskto\n</code></pre> <p>User can easily add the icons to their OnDemand interactive desktop by following the sections below.</p> <p>[ Use Command Lines ] [ Use Interactive Desktop ] [ Create App Icons ]</p> <p>A video instruction is also provided (click to start).</p>"},{"location":"Application_Icons_on_Desktop/#use-command-lines","title":"Use Command Lines","text":"<p>You can simply copy them to your desktop directory <code>~/Desktop</code> . For example, if you would like to have MATLAB icon on your OnDemand desktop, you can run</p> <pre><code>$ mkdir -p ~/Desktop\n$ cp /opt/software/OnDemand/Desktop-Icons/MATLAB.desktop ~/Desktop/\n</code></pre> <p>Once the apps' desktop files are copied, request an  Interactive Desktop session as mentioned in the Open OnDemand page. You should see the apps' icons on your desktop once you launch it:</p> <p></p>"},{"location":"Application_Icons_on_Desktop/#use-interactive-desktop","title":"Use Interactive Desktop","text":"<p>You can also request and start an  Interactive Desktop session to copy the app icons from\u00a0<code>/opt/software/OnDemand/Desktop-Icons</code>. Simply double-click the \"<code>Trash</code>\" or \"<code>Computer</code>\" icon on your desktop. It will pop out the file manager window. In the \"<code>Location:</code>\" place, please enter the directory <code>/opt/software/OnDemand/Desktop-Icons</code>. (If the \"<code>Location:</code>\" place does not allow any input, please click on\u00a0 ). It should show all app icons in the window. Right click on an app icon you would like to copy to your desktop. Choose \"Copy to\" and click \"Desktop\":</p> <p></p> <p>The icon you choose will be copied to your desktop.</p>"},{"location":"Application_Icons_on_Desktop/#create-app-icons","title":"Create App Icons","text":"<p>If your favorite app icons are not in the directory, you can try to use one them as an example</p> <pre><code>$ cat /opt/software/OnDemand/Desktop-Icons/MATLAB.desktop\n\n[Desktop Entry]\nType=Application\nName=MATLAB\nIcon=/opt/software/OnDemand/images/matlab.png\nExec=bash -c \"module load MATLAB/2018a; matlab -desktop\"\nTerminal=false\nGenericName=\n</code></pre> <p>and modify it. Change the following contents</p> <pre><code>Name=&lt;Software Name&gt;\nIcon=&lt;Location and File name of the Software Icon&gt;\nExec=&lt;Commands to Run the Software&gt;\n</code></pre> <p>to your app's. Save the file with your app's file name in <code>~/Desktop</code> directory. Every time you launch an Interactive Desktop session, the icon shows on your desktop. If you have any question, please let us know. We can help you to create one.</p>"},{"location":"Aspera_bulk_file_transfer/","title":"Aspera bulk file transfer","text":"<p>The Aspera Connect application (ascp) is a useful file transfer tool for downloading or uploading large files in bulk between the HPCC and data repository sites such as those operated by NCBI. In order to interact  with a server via aspera, the remote host must be running the Aspera server. </p> <p>This short tutorial will demonstrate how to load and use the command line version of Aspera to download files from the NCBI ftp site.</p> <p>Step 1: Log onto HPCC rsync gateway node:</p> <pre><code>ssh -XY netid@rsync.hpcc.msu.edu\n</code></pre> <p>Step 2: Load Aspera 3.9.8 module:</p> <pre><code>module load Aspera-Connect/3.9.8\n</code></pre> <p>You can only execute Aspera file transfers from a gateway node. Transfers on the dev-nodes will not work.</p> <p>Tip</p> <p>If you need a higer version than 3.9.8, you can try installing it with conda (<code>conda install -c rpetit3 aspera-connect</code>). Conda will handle Glibc issues.</p> <p>Example command for downloading data from NCBI:</p> <pre><code>ascp -T -k 1 -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh anonftp@ftp.ncbi.nlm.nih.gov:/refseq/uniprotkb ~/NCBI_data\n</code></pre> <p>For uploading files from the HPCC, please refer to the NCBI instructions for uploading SRA files.</p>"},{"location":"Assembly_of_PacBio_long_reads_with_Canu/","title":"Assembly of PacBio long reads with Canu","text":""},{"location":"Assembly_of_PacBio_long_reads_with_Canu/#introduction","title":"Introduction","text":"<p>Canu is used for de novo assembly using long reads, as generated from PacBio or Oxford Nanopore technologies. It consists of three steps: read correction, read trimming and contig assembly.</p> <p>As of Sept 2021, we have the latest: version 2.2 installed on the HPCC. You can load it by</p> <pre><code>module purge\nmodule load GCCcore/8.3.0 Java/11 Perl/5.30.0 gnuplot/5.2.8\nexport PATH=/opt/software/canu/canu-2.2/bin:$PATH\n</code></pre> <p>Then, simply running <code>canu</code> will give you a good amount of help information. For example, at the bottom of the help document, we learn that canu supports three types of raw input data:</p> <pre><code>[technology]\n-pacbio      &lt;files&gt;\n-nanopore    &lt;files&gt;\n-pacbio-hifi &lt;files&gt;\n</code></pre> <p>While canu can automate job submission using SLURM, we don't recommend this method. Therefore, please specify <code>useGrid=false</code> in the canu command to disable grid support. Users will write a job script manually, treating canu as an ordinary program.</p>"},{"location":"Assembly_of_PacBio_long_reads_with_Canu/#an-example-using-pacbio-reads","title":"An example using PacBio reads","text":"<p>The PacBio reads we will be assembling are the same as the ones used in the canu tutorial, which can be downloaded using the following command:</p> <pre><code>curl -L -o pacbio.fastq http://gembox.cbcb.umd.edu/mhap/raw/ecoli_p6_25x.filtered.fastq\n</code></pre> <p>By default, the canu pipeline will correct the reads, trim the reads, and then assemble the reads to contigs. Minimally, you can run canu on a dev-node in the following way (we need to first load all necessary modules):</p> <pre><code>module purge\nmodule load GCCcore/8.3.0 Java/11 Perl/5.30.0 gnuplot/5.2.8\nexport PATH=/opt/software/canu/canu-2.2/bin:$PATH\n\n/bin/time -v canu -p ecoli -d ecoli-pacbio genomeSize=4.8m useGrid=false maxThreads=10 -pacbio pacbio.fastq &gt; runCanu_2021-09-14.log 2&gt;&amp;1 &amp;\n</code></pre> <p>Above,</p> <ul> <li><code>pacbio.fastq</code> is the input file, considered as raw and unprocessed     reads. Coupled with <code>-pacbio</code>, canu knows which technology has     generated these reads.</li> <li><code>-p</code>: set the file name prefix of intermediate and output files;     it's mandatory.</li> <li><code>-d</code>: set assembly directory name for canu to run in. If not     supplied, it'll run in the current directory. It is not possible to     run two different assemblies in the same directory.</li> <li><code>genomeSize</code>: in bases, with common prefixes allowed, such as 4.7m     or 2.8g. canu uses it to determine coverage in the input reads.</li> <li><code>useGrid=false</code>: make canu run on the local machine.</li> <li><code>maxThreads</code>: the maximum number of threads that each task can use.</li> <li>Finally, we put <code>time -v</code> in front of the canu command in order to     get resource usage, which will be shown at the end of the log file     <code>runCanu_2021-09-14.log</code>. For example,<ul> <li><code>Maximum resident set size (kbytes): 4113216</code> tells us that the     maximum memory used during the process is about 4G.</li> <li><code>Percent of CPU this job got: 476%</code> tells us we've used on     average 5 CPUs.</li> </ul> </li> </ul> <p>If we want to have canu run in the HPCC cluster, we can write a job script accordingly:</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=canu_ecoli\n#SBATCH --cpus-per-task=10\n#SBATCH --mem=10G\n#SBATCH --time=2:00:00\n#SBATCH --output=%x-%j.SLURMout\n\nmodule purge\nmodule load GCCcore/8.3.0 Java/11 Perl/5.30.0 gnuplot/5.2.8\nexport PATH=/opt/software/canu/canu-2.2/bin:$PATH\n\n/bin/time -v canu -p ecoli -d ecoli-pacbio genomeSize=4.8m useGrid=false maxThreads=10 -pacbio pacbio.fastq &gt; runCanu_2021-09-14.log 2&gt;&amp;1\n</code></pre> <p>The canu command is exactly the same as the one we run on the dev-node, except that the trailing <code>&amp;</code> sign should be removed when it is within a job script.</p> <p>The primary output file for most users is the assembled contigs. In this example, it is <code>ecoli-pacbio/ecoli.contigs.fasta</code> under your current working directory. Refer to this page when you want to learn more about the output, such as the various statistics of the reads analyzed, as reported in the <code>ecoli.report</code> file.</p>"},{"location":"Assembly_of_PacBio_long_reads_with_Canu/#notes","title":"Notes","text":"<ul> <li>To adjust default parameters, you need to consult the     canu parameter reference.</li> <li>The three steps (error correction, trimming and assembly) can be     individually run. See this example.</li> <li>If your data is     PacBio HiFi reads (i.e.     CCS reads with predicted accuracy &gt;= Q20 or 99%), you may want to     use the option <code>-pacbio-hifi</code> rather than <code>-pacbio</code>. Canu will skip     read correction and trimming in this case.</li> </ul>"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/","title":"BLAST/BLAST+ with Multiple Processors","text":""},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#overview","title":"Overview","text":"<p>It is possible to run BLAST or BLAST+ on the HPCC in multi-threaded mode. \u00a0This is advantageous in that is allows users to leverage multiple processors to complete their BLAST searches, thereby decreasing compute time.</p> <p>To load BLAST or BLAST+ on the HPCC:</p> <pre><code># Loading BLAST\nmodule purge\nmodule load BLAST/2.2.26-Linux_x86_64\n\n# Loading BLAST+\nmodule purge\nmodule load icc/2017.4.196-GCC-6.4.0-2.28  impi/2017.3.196 BLAST+/2.8.1-Python-2.7.14\n</code></pre>"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#multi-threading-vs-mpi","title":"Multi-Threading vs. MPI","text":"<p>Multi-threaded BLAST runs enable the user to launch multiple worker threads on a single node. \u00a0However, because standard BLAST and BLAST+ do not use distributed memory, you cannot accomplish multi-threaded runs across multiple nodes. \u00a0Therefore, users executing multi-threaded BLAST or BLAST+ runs should not reserve more than one node, as this will reserve hardware resources that cannot be used.</p>"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#job-submission-guidelines","title":"Job Submission Guidelines","text":"<p>First, we need to differentiate between traditional NCBI BLAST and BLAST+. \u00a0Traditional NCBI BLAST utilizes the \"-a #\" flag to specify the number of processors to use for the job (default is 1). \u00a0BLAST+ uses the \"-num_threads #\" flag to specify the number of worker threads to use. \u00a0Depending upon which type of BLAST you use, you will need to adjust your job submission script parameters accordingly.</p>"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#traditional-blast","title":"Traditional BLAST","text":"<p>Using the \"-a\" flag in BLAST will specify the number of\u00a0processors\u00a0to use. \u00a0To reserve the appropriate quantity of resources in your job submission script, you will need to reserve a number of cores equal to the value specified by the \"-a\" flag \u00a0For example, if you used a command like:</p> <pre><code>blastall -p blastp -d swissprot -i\u00a0prot.fasta\u00a0-o test1.blast -e 0.001 -a 4\n</code></pre> <p>You should specify something like the following in your SLURM job submission script:</p> <pre><code>#SBATCH --cpus-per-task=4\n</code></pre>"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#blast","title":"BLAST+","text":"<p>In contrast, BLAST+ uses the \"-num_threads\" flag to specify the number of worker\u00a0threads\u00a0to create. \u00a0In order to specify the correct number of cores for the job, you will need to\u00a0ADD ONE\u00a0to the number of threads specified. \u00a0This is to account for the number of worker threads,\u00a0PLUS\u00a0the main process thread. \u00a0So if you used an equivalent BLAST+ command like:</p> <pre><code>blastn -task blastn -db swissprot -query prot.fasta -out test1.blast -evalue 0.001 -num_threads 4\n</code></pre> <p>You should use the following in your SLURM\u00a0script:</p> <pre><code>#SBATCH --cpus-per-task=5\n</code></pre>"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#blastdb","title":"BLASTDB","text":"<p>The BLASTDB environmental variable tells BLAST or BLAST+ where to find your databases that can be searched. \u00a0On the HPCC, we offer select BLAST-ready data sets for this purpose in a common read-only area. \u00a0BLAST data sets can be accessed at:</p> <pre><code>/mnt/research/common-data/Bio/blastdb\n</code></pre> <p>If you are using the FASTA sequences instead of nucleotide data sets, you need to augment the path above as follows:</p> <pre><code>/mnt/research/common-data/Bio/blastdb/FASTA\n</code></pre> <p>For cluster jobs, you will need to set the value of BLASTDB in your job submission script, for example:</p> <pre><code>export BLASTDB=/mnt/research/common-data/Bio/blastdb:/mnt/research/common-data/Bio/blastdb/FASTA:$BLASTDB\n</code></pre>"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#a-word-about-memory","title":"A Word About Memory","text":"<p>In either case (BLAST or BLAST+) your requested memory (in the examples above, 4gb) will be divided amongst all of your task threads. \u00a0Plan accordingly.</p>"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#blast-data-preparation","title":"BLAST data preparation","text":"<p>Data downloaded from the NCBI website, or prepared by users can, in most cases, be easily converted for use with BLAST. \u00a0This brief tutorial is designed to illustrate a fairly basic scenario where the user wants to download a set of FASTA sequences from the NCBI website and prepare them for BLAST-ing.</p>"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#download","title":"Download","text":"<p>The simplest way to do this is to note the link of the FASTA file, and use either the \"wget\" or \"curl\" command. \u00a0For example:</p> <pre><code>wget ftp://ftp.ncbi.nih.gov/repository/UniGene/Triticum_aestivum/Ta.seq.all.gz\n</code></pre> <p>or</p> <pre><code>curl -O ftp://ftp.ncbi.nih.gov/repository/UniGene/Triticum_aestivum/Ta.seq.all.gz\n</code></pre> <p>This will download the file \"Ta.seq.all.gz\" into the current directory. Now unzip the file:</p> <pre><code>gunzip Ta.seq.all.gz\n</code></pre> <p>This will leave a file called \"Ta.seq.all\" in your directory.</p>"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#preparing-the-indices","title":"Preparing the Indices","text":"<p>To prepare the BLAST indices for nucleotides:</p> <pre><code>formatdb -i Ta.seq.all -p F\n</code></pre> <p>The command above will produce several files, such as:</p> <pre><code>Ta.seq.all.fa.nhr\nTa.seq.all.fa.nin\nTa.seq.all.fa.nsq\n</code></pre> <p>If you want to produce protein indices instead of, or in addition to nucleotides, run:</p> <pre><code>formatdb -i Ta.seq.all -p T\n</code></pre> <p>In this case, this will produce the files:</p> <pre><code>Ta.seq.all.fa.phr\nTa.seq.all.fa.pin\nTa.seq.all.fa.psq\n</code></pre> <p>You can verify whether your BLAST formatting was successful by looking at the \"formatdb.log\" file which should now be present in your directory.</p>"},{"location":"Backwards_Compatibility_with_CentOS/","title":"Backwards Compatibility with CentOS","text":"<p>Early testing!</p> <p>Please note that the advice and commands on this page are still being tested. Expect the possibility for bugs and errors, and always test for accuracy by comparing against commands you know already work.</p> <p>If you discover a problem with anything on this page, please contact us so we can work to resolve it.</p> <p>As ICER upgrades the operating system on the HPCC to Ubuntu, we are providing limited support for running code based on the current CentOS system. This page outlines a few scripts and tips to help you run your code through a backwards compatibility \"container\".</p> <p>For an overview of containers see our Containers Overview page. Knowledge of containers is helpful, but not required to use the helper scripts provided.</p> <p>MPI and multi-node jobs are not supported</p> <p>Running multiple tasks with MPI is difficult to replicate using containers. As such, the scripts and instructions given here have not been tested for MPI jobs and likely will not work.</p> <p>That being said, running MPI with containers is possible with some extra configuration. For more information, see the Singularity documentation or this tutorial from Pawsey Supercompting.</p>","tags":["how-to guide","OS upgrade"]},{"location":"Backwards_Compatibility_with_CentOS/#automatically-submitting-batch-scripts-with-sbatch_old","title":"Automatically submitting batch scripts with <code>sbatch_old</code>","text":"<p>We have provided a <code>powertool</code> called <code>sbatch_old</code>. This tool will submit all of the commands in your current SLURM batch scripts through the compatibility container. In particular, it is possible to access all modules available on CentOS using their original name. To use it, replace <code>sbatch</code> with <code>sbatch_old</code> when submitting your script to any Ubuntu node.</p>","tags":["how-to guide","OS upgrade"]},{"location":"Backwards_Compatibility_with_CentOS/#example","title":"Example","text":"<p>Consider the following SLURM batch script:</p> my_script.sb<pre><code>#!/bin/bash --login\n#SBATCH --job-name=Rscript\n#SBATCH --ntasks=1\n#SBATCH --mem=20M\n#SBATCH --time=01:00:00\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=yournetid@msu.edu\n#SBATCH --output=%x-%j.SLURMout\n\n# Purge current modules and load those we require\nmodule purge\nmodule load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 powertools\n\n# Run our job\ncd /mnt/home/user123\nsrun Rscript myscript.R\n\n# Print resource information\nscontrol show job $SLURM_JOB_ID\njs -j $SLURM_JOB_ID\n</code></pre> <p>This job can be run on any <code>CentOS</code> node with the command:</p> CentOS<pre><code>sbatch my_script.sb\n</code></pre> <p>This job will fail on Ubuntu because <code>R/4.0.2</code> is not currently available (and even if it were, the name would be different). Instead, use the <code>sbatch_old</code> command:</p> Ubuntu<pre><code>module load powertools\nsbatch_old my_script.sb\n</code></pre> <p>This script should work on jobs that request at most one node and do not use MPI. Jobs using GPUs should still continue to work.</p>","tags":["how-to guide","OS upgrade"]},{"location":"Backwards_Compatibility_with_CentOS/#interactively-using-the-backwards-compatibility-container-with-old_os","title":"Interactively using the backwards compatibility container with <code>old_os</code>","text":"<p>To run commands interactively using the compatibility container, run the command <code>old_os</code> on any Ubuntu node. This will replace your command line with one running inside the compatibility container, and you will have access to the majority of the commands on the CentOS system.</p>","tags":["how-to guide","OS upgrade"]},{"location":"Backwards_Compatibility_with_CentOS/#example_1","title":"Example","text":"<p>Suppose you want to test compiling and running GPU code. On a CentOS node your session might look like</p> CentOS commands<pre><code>getexample helloCUDA\ncd helloCUDA\nmodule load gcccuda/2019\nnvcc Hello.cu -o Hello_CUDA\n./Hello_CUDA\n</code></pre> <p>However, running this on Ubuntu will not work, because <code>gcccuda/2019</code> is not available. You also cannot guarantee that <code>./Hello_CUDA</code> will still work on Ubuntu because the compilers and libraries used to compile it on CentOS are either different or nonexistent on Ubuntu. To get around, this you can start an interactive session in the compatibility container</p> Ubuntu commands<pre><code>module load powertools\nold_os  # All following commands are in CentOS compatibility container\n./Hello_CUDA  # Works\nrm Hello_CUDA\nmodule load gcccuda/2019  # Recompiling with CentOS modules\nnvcc Hello.cu -o Hello_CUDA\n./Hello_CUDA\nexit  # Go back to Ubuntu\n</code></pre>","tags":["how-to guide","OS upgrade"]},{"location":"Backwards_Compatibility_with_CentOS/#advanced-manually-using-the-container","title":"Advanced: Manually using the container","text":"<p>If you are comfortable using Singularity directly and would like a more flexible workflow, e.g., to experiment with MPI jobs, you can also invoke the backwards compatibility container directly.</p> <p>We recommend a invoking <code>singularity</code> with the following options:</p> <pre><code>singularity &lt;command&gt; \\\n    --bind /opt/.software-legacy:/opt/software  # Access module system\n    --bind /opt/.modules-legacy:/opt/modules\n    --bind /cvmfs  # Access software provided by CVMFS endpoints (Icecube, CERN, etc.)\n    --cleanenv  # Ensure module system from Ubuntu doesn't conflict\n    --nv  # If using a GPU\n    /mnt/research/helpdesk/ubuntu-compute/centos79.sif  # Use most recent image\n</code></pre> <p>where <code>&lt;command&gt;</code> is one of <code>shell</code> or <code>exec</code>. See <code>less $(which sbatch_old)</code> for an example.</p>","tags":["how-to guide","OS upgrade"]},{"location":"Buy-In_Accounts_with_SLURM/","title":"Buy-In and Account Management","text":"<p>If you want to have priority access to our clusters, you can purchase buy-in nodes.  Users who run on buy-in nodes receive priority access to their nodes within 4 hours and are exempt from the 1 million CPU hour per year limit.</p> <p>Priority access to buy-in nodes is controlled through the SLURM partitioning system. See SLURM Queueing and Partitions for more information. The sections below describe how to manage accounts in SLURM to ensure access to buy-in partitions.</p>","tags":["how-to guide","buyin","partitions","accounts"]},{"location":"Buy-In_Accounts_with_SLURM/#checking-default-account","title":"Checking Default Account","text":"<p>When submitting a job without specifying an account, your default account is used. You can check your default account using the <code>buyin_status</code> power tool.</p> <pre><code>$ module load powertools\n$ buyin_status -l\n\nUser:     fordste5\nAccounts: test1_classres test1 classres\nDefault:  classres\n</code></pre> <p>User <code>fordste5</code> has a default account of <code>classres</code>. If <code>fordste5</code> submits a job without specifying an account, it will be submitted to the <code>classres</code> partition. If the job requested four hours or less of wall time, it will be submitted to the <code>general-short</code> partition as well. Jobs submitted by users with a default account of <code>general</code> will be queued in the <code>general-long</code> and <code>general-short</code> partitions. See How jobs are assigned to queues for a complete listing of queue assignments.</p>","tags":["how-to guide","buyin","partitions","accounts"]},{"location":"Buy-In_Accounts_with_SLURM/#using-non-default-accounts","title":"Using Non-Default Accounts","text":"<p>You can see what accounts you have access to using the <code>buyin_status</code> powertool:</p> <pre><code>$ module load powertools\n$ buyin_status -l\n\nUser:     fordste5\nAccounts: test1_classres test1 classres\nDefault:  classres\n</code></pre> <p>This output shows that <code>fordste5</code> is a member of the <code>test1_classres</code>, <code>test1</code>, <code>and\u00a0classres</code> buy-in accounts. Both <code>test1</code> and <code>classres</code> correspond to partitions with the same name.</p> <p>Because <code>fordste5</code> has a default account of <code>classres</code>, if they want\u00a0to submit jobs to <code>test1</code>, they will have to explicitly specify the <code>test1</code> account at submission. This is done by using either the <code>-A test1</code> option for <code>srun/sbatch/salloc</code>, or by adding the <code>#SBATCH -A test1</code> directive to their batch script.</p> <p>If you would like to change your default account, you can do so with <code>sacctmgr</code>. In the example above, if <code>fordste5</code> wants to change their default account to  <code>test1</code>, they can do so with the command</p> <pre><code>sacctmgr modify user name=fordste5 set DefaultAccount=test1\n</code></pre>","tags":["how-to guide","buyin","partitions","accounts"]},{"location":"Buy-In_Accounts_with_SLURM/#managing-buy-in-account-membership","title":"Managing Buy-In Account Membership","text":"<p>SLURM allows for the configuration of account coordinators. Account coordinators can add and remove users to accounts that they coordinate. Buy-in account owners are configured as coordinators of their buy-in accounts and can request that other users also be added as coordinators.</p> <p>To check who has coordinator privileges on a buy-in account, run:</p> <pre><code>$ sacctmgr show account &lt;account_name&gt; withcoord\n</code></pre> <p>If you had a buy-in account configured in the old Moab scheduler, account coordinators for your SLURM buy-in account were copied from the managers listed on the Moab account.</p> <p>View what users have access to your buy-in account:</p> <pre><code>$ sacctmgr show association where account=&lt;account&gt;\n</code></pre> <p>Add a user to your buy-in account:</p> <pre><code>$ sacctmgr add user name=&lt;userid&gt; account=&lt;account&gt;\n</code></pre> <p>Remove a user from your buy-in account:</p> <pre><code>$ sacctmgr delete user &lt;userid&gt; where account=&lt;account&gt;\n</code></pre>","tags":["how-to guide","buyin","partitions","accounts"]},{"location":"Buy-In_Accounts_with_SLURM/#checking-the-status-of-buy-ins","title":"Checking the Status of Buy-Ins","text":"<p>The status of buy-ins can be viewed with the <code>buyin_status</code> powertool.</p> <pre><code>$ module load powertools\n$ buyin_status -a classres\n\nUser:     fordste5\nAccounts: test1_classres test1 classres\nDefault:  classres\n\n\nBuyin: classres\n  JOBID      STATE      USER       CPUS PRIORITY  TIME_LIMIT           START_TIME\n  17401      RUNNING    fordste5   10   101          1:00:00  2018-08-09T12:55:17\n  17409      RUNNING    changc81   4    100          1:00:00  2018-08-09T12:57:54\n\n  Partition: classres\n    lac-421 (down*)\n    csn-020 (down*)\n    csp-018 (down*)\n    css-034 (down*)\n    css-035 (down*)\n    css-079 (down*)\n    css-080 (down*)\n    css-033 (allocated)\n      JOBID      ACCOUNT    USER       CPUS       TIME  TIME_LEFT\n      17402      general    fordste5   10         4:36      55:24\n      17401      classres   fordste5   10         4:36      55:24\n</code></pre> <p>This tool will list all jobs that are queued for a buy-in account, all nodes associated with the buy-in\u00a0account\u2013including node\u00a0status\u2013and a list of jobs running on each node. Because jobs with a wall time of four hours or less can run on any buy-in node, it is possible to see jobs running on buy-in nodes from other accounts.</p> <p>Buy-in partitions have the same name as a given buy-in account</p>","tags":["how-to guide","buyin","partitions","accounts"]},{"location":"COMSOL/","title":"COMSOL","text":"<p>To use COMSOL with SLURM you should use the slurm option '-mpibootstrap slurm' and omit using srun.</p> <p>Sample run line:\u00a0</p> <pre><code>comsol batch -mpibootstrap slurm -inputfile batchin.mph -outputfile batchout.mph -batchlog batchlog.out\n</code></pre>"},{"location":"Change_Primary_Group/","title":"Change primary group","text":"<p>Generally, HPCC users need to request\u00a0root privilege through the ticket system to change their primary group permanently. However, a user can also use <code>newgrp</code> command to change their primary group temporarily to any one of their groups.</p>","tags":["how-to guide","groups"]},{"location":"Change_Primary_Group/#primary-group-change-on-command-line","title":"Primary Group Change on Command Line","text":"<p>To change a user's primary group with a command line, simply run</p> <pre><code>[username@dev-intel18 CurrentPath]$ newgrp &lt;Group Name&gt;\n</code></pre> <p>Note that <code>&lt;Group Name&gt;</code> has to be one of the user's group names. To find out\u00a0all group names, please run <code>groups</code> command:</p> <pre><code>[username@dev-intel18 CurrentPath]$ groups\nchemistry VMD g09 BiCEP education-data ParaView\n</code></pre> <p>Here, the primary group is \"<code>chemistry</code>\", the first group name in the results. After the execution of <code>newgrp</code> command, a new shell session is created and\u00a0the current environment, including the current working directory remains the same.</p> <pre><code>[username@dev-intel18 CurrentPath]$ newgrp g09\n[username@dev-intel18 CurrentPath]$ groups\ng09 chemistry VMD BiCEP education-data ParaView\n</code></pre> <p>Note that the primary group is changed to g09. To leave the session, just run \"<code>exit</code>\" command. It will go back to the previous session with the original primary group (<code>chemistry</code>).</p> <p>If a user would like to start the shell session with a new primary group as though the user just logged in, the optional \"<code>-</code>\" flag can be used</p> <pre><code>[username@dev-intel18 CurrentPath]$ newgrp - g09\n[username@dev-intel18 ~]$ groups\ng09 chemistry VMD BiCEP education-data ParaView\n</code></pre> <p>Using the \"<code>-</code>\" flag reinitializes the user's environment and the shell session starts from the home directory.</p> <p>Some users might have the issue of losing the header in front of the prompt after executing the command:</p> <pre><code>[username@dev-intel18 CurrentPath]$ newgrp g09\nbash-4.2$ groups\ng09 chemistry VMD BiCEP education-data ParaView\n</code></pre> <p>Using <code>source</code> on the default bashrc file will get it back.</p> <pre><code>bash-4.2$ source /etc/bashrc\n[changc81@dev-intel18 MyCurrentPath]$ \n</code></pre> <p>Note that, if a job is submitted in the session of a new primary group, the environment variables of the new primary group will be used by the job by default instead of the original environment. For more information, please see <code>--export</code> in the table of List of Job Specifications page.</p>","tags":["how-to guide","groups"]},{"location":"Change_Primary_Group/#primary-group-change-in-job-script","title":"Primary Group Change in Job Script","text":"<p>Many HPCC users have more than one research spaces. In a job running, the user might need their primary group set to be the group of the research space where the output files are. In order to do this, the job script can use <code>newgrp</code> from the beginning of the command lines. For example, an original job script may look like:</p> <pre><code>#!/bin/bash --login\n\n#SBATCH --time=00:10:00             # limit of wall clock time - how long the job will run (same as -t)\n#SBATCH --ntasks=5                  # number of tasks - how many tasks (nodes) that you require (same as -n)\n#SBATCH --mem-per-cpu=2G            # memory required per allocated CPU (or core) - amount of memory (in bytes)\n\nmodule purge\nmodule load GCC/6.4.0-2.28 OpenMPI  ### load necessary modules, e.g. \n\nsrun -n 5 &lt;executable&gt;              ### call your executable (similar to mpirun)\n\nscontrol show job $SLURM_JOB_ID     ### write job information to output file\n</code></pre> <p>If the job script is submitted in a research space, adding a few lines can make the primary group the same as the group of the research space:</p> <pre><code>#!/bin/bash --login\n\n#SBATCH --time=00:10:00             # limit of wall clock time - how long the job will run (same as -t)\n#SBATCH --ntasks=5                  # number of tasks - how many tasks (nodes) that you require (same as -n)\n#SBATCH --mem-per-cpu=2G            # memory required per allocated CPU (or core) - amount of memory (in bytes)\n\nGroupName=$(readlink -f $PWD|awk -F \"/\" '{ if ($2==\"mnt\" &amp;&amp; $3==\"research\" &amp;&amp; $4!=\"\") system(\"ls -ld /mnt/research/\"$4\"|cut -d \\\" \\\" -f 4\") }')\nnewgrp ${GroupName} &lt;&lt; EOS\n\nmodule purge\nmodule load GCC/6.4.0-2.28 OpenMPI  ### load necessary modules, e.g. \n\nsrun -n 5 &lt;executable&gt;              ### call your executable (similar to mpirun)\n\nscontrol show job $SLURM_JOB_ID     ### write job information to output file\nEOS\n</code></pre> <p>The group name of the research space for job running is automatically determined in line 7. You can also directly set the variable \"<code>GroupName</code>\" to be the group name of your research space in line 7 if you are sure about jobs running in the research space:</p> <pre><code>GroupName=&lt;group name of your research space&gt;\n</code></pre> <p>After the group name of the research space is given, the command lines between line 9 and 16 (inside the two \"<code>EOS</code>\") will be run under the environment of the new primary group (<code>${GroupName}</code>).</p>","tags":["how-to guide","groups"]},{"location":"Change_Primary_Group/#automatic-primary-group-change-after-login","title":"Automatic Primary Group Change after Login","text":"<p>To automatically change your primary group after every login, please submit a ticket to ICER's system administrators with the subject \"Membership Changes\".</p> <p>Previously, this page recommended using the <code>PrimaryGroup</code> powertool in your <code>.bashrc</code>. However, this can cause issues when you SSH to other nodes from a development node, and so we no longer recommend using it.</p>","tags":["how-to guide","groups"]},{"location":"Checkpoint_with_DMTCP/","title":"Checkpoint with DMTCP","text":"<p>Note</p> <p>To run DMTCP correctly, the limit of stack size can not be \"unlimited\". To set it to a number n, run <code>ulimit -s n</code>. For example, <code>ulimit -s 8192</code> would set the stack size as 8192. User could also add it into .bashrc file.\u00a0 It should also be set in the job batch script, as seen below.</p> <p>DMTCP (Distributed MultiThreaded Checkpointing) transparently checkpoints both single and multi-node computations. It supports MPI (various implementations), OpenMP, MATLAB, Python, Perl, R, and many programming languages and shell scripting languages. DMTCP allows one to checkpoint running programs to disk, restart calculations from a checkpoint, or even migrate the processes to another host by moving the checkpoint files prior to restarting.</p> <p>Each computation you wish to checkpoint requires one DMTCP coordinator.  First, the DMTCP coordinator process is started on one host. Then application binaries are started with the <code>dmtcp_launch</code> command, causing them to connect to the coordinator upon startup. As threads are spawned, child processes are forked, remote processes are spawned via ssh, libraries are dynamically loaded, etc., DMTCP transparently and automatically tracks them. To checkpoint, use <code>dmtcp_coordinator</code> command to start checkpointing. To restart from a checkpoint, use <code>dmtcp_restart</code>.</p> <p>By default, DMTCP uses <code>gzip</code> to compress the checkpoint images. This can be turned off. This will be faster, and if your memory is dominated by incompressible data, this can be helpful. <code>gzip</code> can add seconds for large checkpoint images. Typically, checkpoint and restart is less than one second without <code>gzip</code>.</p>","tags":["tutorial","checkpointing"]},{"location":"Checkpoint_with_DMTCP/#using-dmtcp","title":"Using DMTCP","text":"<p>Running a program with checkpointing usually involves the following 4 steps (option settings may be needed for special cases) :</p> <ol> <li>Start DMTCP coordinater\u00a0<ul> <li><code>$ dmtcp_coordinator --daemon --exit-on-last $@ 1\\&gt;/dev/null     2\\&gt;&amp;1\u00a0#run coordinator as daemon in background</code></li> </ul> </li> <li>Launch program<ul> <li><code>$ dmtcp_launch ./a.out # launch ./a.out</code></li> </ul> </li> <li>Trigger checkpointing. This will     generate a set of checkpointing image files (file type:\u00a0<code>.dmtcp</code>)\u00a0and     a shell script for restart.<ul> <li><code>$ dmtcp_command\u00a0--bcheckpoint # checkpointing</code></li> </ul> </li> <li>Restart: the DMTCP coordinator will write     a script, <code>dmtcp_restart_script.sh</code>, along with a checkpoint file     (file type: <code>.dmtcp</code>) for each client process. The simplest way to     restart a previously checkpointed computation is:<ul> <li><code>$ ./dmtcp_restart_script.sh\u00a0# restart     using script</code></li> <li>Alternatively, if all processes were on the same processor, and there were no <code>.dmtcp</code> files prior to this checkpoint: <code>$ dmtcp_restart ckpt_*.dmtcp</code></li> </ul> </li> </ol>","tags":["tutorial","checkpointing"]},{"location":"Checkpoint_with_DMTCP/#dctmp-example","title":"DCTMP example","text":"<p>The following is the sample script <code>longjob.sb</code> that uses DMTCP for checkpointing a long job. In this way, the job can be run as a sequence of short walltime jobs. To obtain the complete example, run <code>module load powertools; getexample dmtcp_longjob</code>.</p> <pre><code>#!/bin/bash -login\n\n## resource requests for task:\n\n#SBATCH -J count-longjob                  # Job Name\n\n#SBATCH --time=00:06:00                   # Note that 6 min is not enough to complete the job. It enough for checkpointing and resubmit job\n\n#SBATCH -N 1 -c 1 --mem=20MB              # requested resource\n\n#SBATCH --constraint=lac                  # user could add other requests as usual.\n\n# set a limited stack size so DMTCP could work\nulimit -s 8192\n\n# current working directory shuld have source code dmtcp1.c\ncd ${SLURM_SUBMIT_DIR}\n\n# this script file name. This script may be resubmit multiple times until job completed\nexport SLURM_JOBSCRIPT=\"longjob.sb\"\n\n\n######################## start dmtcp_coordinator #######################\n\nfname=port.$SLURM_JOBID                                                                 # to store port number\n\ndmtcp_coordinator --daemon --exit-on-last -p 0 --port-file $fname $@ 1&gt;/dev/null 2&gt;&amp;1   # start coordinater\n\nh=`hostname`                                                                            # get coordinator's host name\n\np=`cat $fname`                                                                          # get coordinator's port number\n\nexport DMTCP_COORD_HOST=$h                                                  # save coordinators host info in an environment variable\n\nexport DMTCP_COORD_PORT=$p                                                  # save coordinators port info in an environment variable\n\n#rm $fname\n\n\n\n\n# uncommand following lines to print out some information if user wish\n\n#echo \"coordinator is on host $DMTCP_COORD_HOST \"\n\n#echo \"port number is $DMTCP_COORD_PORT \"\n\n#echo \" working directory: ${SLURM_SUBMIT_DIR} \"\n\n#echo \" job script is $SLURM_JOBSCRIPT \"\n\n\n\n\n####################### BODY of the JOB ######################\n\n# prepare work environment of the job\n\nmodule swap GNU/6.4.0-2.28 GCC/4.9.2\n\n\n# build the program if executable file does not exist\n\nif [ ! -f count.exe ] \n\nthen\n\n    cc count.c -o count.exe\n\nfi\n\n\n\n\n# run the program count.exe. \n\n# To run interactively: \n\n#    $ ./count.exe n num.odd 1&gt; num.even \n\n# it will count to number n and generate 2 files: \n\n# num.odd contains all the odd number;\n\n# num.even contains all the even number.\n\n\n\n# To run with DMTCP, use dmtcp commamds.\n\n# if first time launch, use \"dmtcp_launch\"\n\n# otherwise use \"dmtcp_restart\"\n\n\n\n\n# set checkpoint interval. This script would wait after dmtcp_launch\n\n# the job for the interval (in seconds), then do start the checkpoint. \n\nexport CKPT_WAIT_SEC=$(( 3 * 60 ))            # checkpointing when program runs for 3 min\n\n\n\n\n# Launch or restart the execution\n\nif [ ! -f ckpt_*.dmtcp ]                      # if no ckpt file exists, it is first time run, use dmtcp_launch\n\nthen\n\n  # first time run, use dmtcp_launch to start the job and run on background */\n\n  dmtcp_launch -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --rm --ckpt-open-files ./count.exe 800 num.odd 1&gt; num.even 10&gt;&amp;- 11&gt;&amp;- &amp;\n\n\n\n\n  #wait for an inverval of checkpoint seconds to start checkpointing\n  sleep $CKPT_WAIT_SEC\n\n\n\n  # start checkpointing\n  dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --ckpt-open-files --bcheckpoint\n\n\n  # kill the running job after checkpointing\n  dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --quit\n\n\n  # resubmit the job\n  sbatch $SLURM_JOBSCRIPT\n\n\nelse            # it is a restart run\n\n  # restart job with checkpoint files ckpt_*.dmtcp and run in background\n  dmtcp_restart -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT ckpt_*.dmtcp 1&gt; num.even &amp;\n\n\n  # wait for a checkpoint interval to start checkpointing\n  sleep $CKPT_WAIT_SEC\n\n\n  # if program is still running, do the checkpoint and resubmit\n\n  if dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT -s 1&gt;/dev/null 2&gt;&amp;1\n  then   \n    # clean up old ckpt files before start checkpointing\n    rm -r ckpt_*.dmtcp\n\n    # checkpointing the job\n    dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --ckpt-open-files -bc\n\n    # kill the running program and quit\n    dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --quit\n\n    # resubmit this script to slurm\n    sbatch $SLURM_JOBSCRIPT\n\n  else\n\n    echo \"job finished\"\n\n  fi\n\nfi\n\n# show the job status info\nscontrol show job $SLURM_JOB_ID\n</code></pre>","tags":["tutorial","checkpointing"]},{"location":"Cluster_Resources/","title":"Cluster resources","text":"<p>HPCC maintains several clusters. They are named according to the year of installation. Each cluster has very similar hardware with specific processors but has some variety in configuration, such as different coprocessors, memory capacity, or number of CPUs. However, with many different kinds of configurations, HPCC uses a single-queue system managed by SLURM, a resource management software. Jobs submitted to SLURM job queue can run on any possible nodes, unless there are specifications on cluster constraints. Users only have to specify resource requirements and our scheduler can assign your job to an appropriate cluster.</p> <p>All clusters currently run CentOS 7 and use the SLURM resource manager. They are connected to each other with file systems through Infiniband. Your home, research, and scratch is available and identical on all nodes.</p> <p>The following table lists nodes\u00a0that are currently available to run jobs; jobs can be submitted from any of our development nodes. Please note that you may need to scroll the table to see all columns.</p> Cluster Type Node Count Processors (Clock Speed) Cores Memory Disk Size GPUs (Number) Node Name amd22 48 AMD EPYC 7763 Processor (2.445 GHz) 128 493 GB 412 GB acm-[000-047] 18 AMD EPYC 7763 Processor (2.445 GHz) 128 996 GB 412 GB acm-[050-067] 6 AMD EPYC 7763 Processor (2.445 GHz) 128 2005 GB 412 GB acm-[048,049,068-071] amd21 11 AMD EPYC 7713 Processor (2.0 GHz) 128 512 GB 1.92 TB A100 SXM with 81920 MB memory(4) nal-[000-010] intel21 6 Intel Xeon 8358 (2.6 GHz) 64 256 GB 1.92 TB A100 PCIe with 40960 MB memory(4) nif-[000-005] amd20 207 AMD EPYC 7H12 Processor (2.595 GHz) 128 493 GB 412 GB amr-[000-101], amr-[136-238], amr-[252-253] 45 AMD EPYC 7H12 Processor (2.595 GHz) 128 996 GB 412 GB amr-[104-135], amr-[239-251] 2 AMD EPYC 7H12 Processor (2.595 GHz) 128 2005 GB 412 GB amr-[102-103] amd20-v100 20 Intel(R) Xeon(R) Platinum 8260 CPU (2.40 GHz) 48 178 GB 412 GB v100s with 32768 MB memory(4) nvf-[000-016], nvf-[018-020] 1 Intel(R) Xeon(R) Platinum 8260 CPU (2.40 GHz) 48 565 GB 412 GB v100s with 32768 MB memory(4) nvf-017 intel18 113 Intel(R) Xeon(R) Gold 6148 CPU (2.40 GHz) 40 83 GB 413 GB skl-[000-112] 19 Intel(R) Xeon(R) Gold 6148 CPU (2.40 GHz) 40 178 GB 413 GB skl-[113-131] 28 Intel(R) Xeon(R) Gold 6148 CPU (2.40 GHz) 40 367 GB 413 GB skl-[132-139,148-167] 8 Intel(R) Xeon(R) Gold 6148 CPU (2.40 GHz) 40 745 GB 413 GB skl-[140-147] intel18-v100 8 Intel(R) Xeon(R) Gold 6148 CPU (2.40 GHz) 40 367 GB 413 GB v100 with 32768 MB memory (8) nvl-[000-007] intel16 26 Intel(R) Xeon(R) CPU E5-2680 v4 (2.40 GHz) 28 492 GB 190 GB lac-[250-253,256-261,302-317] 39 Intel(R) Xeon(R) CPU E5-2680 v4 (2.40 GHz) 28 240 GB 190 GB lac-[224-225,228-248, 278-285,294-301] 313 Intel(R) Xeon(R) CPU E5-2680 v4 (2.40 GHz) 28 115 GB 190 GB lac-[000-023,032-191,200-223, 254,255,276,277,318-341, 350-369,372,372-445] intel16-k80 48 Intel(R) Xeon(R) CPU E5-2680 v4 (2.40 GHz) 28 240 GB 190 GB k80 with 12206 MB memory (8) lac-[024-031,080-087,136-143, 192-199,286-293,342-349] intel16-xl 2 Intel(R) Xeon(R) CPU E7-8867 v3 (2.50 GHz) 64 2.93 TB 860 GB vim-[000,001] 1 Intel(R) Xeon(R) CPU E7-8867 v4 (2.40 GHz) 144 5.86 TB 3.7 TB vim-002 intel14 18 Intel(R) Xeon(R) CPU E5-2670 v2 (2.50 GHz) 20 240 GB 416 GB csm (11 nodes) &amp; css (10 nodes) 8 Intel(R) Xeon(R) CPU E5-2670 v2 (2.50 GHz) 20 115 GB 416 GB css-[002-003,020,023,032-035] 65 Intel(R) Xeon(R) CPU E5-2670 v2 (2.50 GHz) 20 52 GB 416 GB css nodes intel14-k20 37 Intel(R) Xeon(R) CPU E5-2670 v2 (2.50 GHz) 20 115 GB 416 GB k20 with 4743 MB memory (2) csn-[001-039] intel14-phi 8 Intel(R) Xeon(R) CPU E5-2670 v2 (2.50 GHz) 20 115 GB 416 GB Phi card (2) csp-[006,016-020,025-026] intel14-xl 1 Four Intel Xeon CPU E7-8857 v2 (3.00 GHz) 48 969 GB 1.8 TB qml-003 1 Four Intel Xeon CPU E7-8857 v2 (3.00 GHz) 48 1.45 TB 897 GB qml-[002] 1 Four Intel Xeon CPU E7-8857 v2 (3.00 GHz) 48 2.93TB 1.1 TB qml-000","tags":["reference"]},{"location":"Cluster_amd20_with_AMD_CPUs/","title":"Cluster amd20 with AMD CPUs","text":"<p>In 2020, the HPCC purchased a new cluster amd20 powered by AMD EPYC processors. Each amd20 node has total 128 cores and at least 0.5 TB RAM. Each core has a base clock speed 2.6 GHz, up to 3.3 GHz.\u00a0</p>"},{"location":"Cluster_amd20_with_AMD_CPUs/#basic-architecture-information","title":"Basic architecture information","text":"<p>The AMD CPU contains 2 sockets with CPU packages with 4 NUMA nodes in each socket, and 16 cores in each NUMA node. Each NUMA core contains two \"Core Complex Dies\", which have two four-core \"Core-Complex\" modules. Each four-core Core-Complex shares a 16 MB L3 cache.</p> <p></p> <p>A 7002 series processor (via AMD)</p> <p></p> <p>A Core-Complex (via AMD) logical diagram.</p> <p></p> <p>A 1 TB amd20 node.</p> <p>Generally, cores within the same L3 cache have the lowest latency, followed by in-NUMA cores, other cores on the same socket, and cores on the other socket are slower. The SLURM job scheduler on the HPCC will try to keep within the same NUMA node by default. One option to try during testing is to use OpenMP within the L3 node, and MPI for everything else. When using newer versions of OpenMPI, you can use the following argument with OMP_NUM_THREADS=4 to distribute one 4-thread rank per L3:</p> <pre><code>mpirun -map-by ppr:1:l3cache:pe=4 --bind-to core\n</code></pre> <p>In testing, the Intel Compiler and MKL toolchain works well if you \u201cexport MKL_DEBUG_CPU_TYPE=5\u201d and compile for AVX2 instead of AVX512.</p> <p>If you\u2019re doing single-node scaling work, be aware of memory bandwidth; on these nodes, HPL scales from 1-96 cores linearly but only 3.5 -&gt; 4 TF from 96-&gt;128 when going from 3 cores per L3 to 4.</p> <p>Each node has a 100 gigabit HDR100 connection. There are 52-56 nodes per switch:</p> <p></p> <p>Resources:</p> <p>High Performance Computing: Tuning Guide for AMD EPYC\u2122 7002 Series Processors</p> <p>Compiler Options Quick Ref Guide for AMD EPYC 7xx2 Series Processors</p>"},{"location":"Collaborative_Research/","title":"Collaborative research","text":""},{"location":"Collaborative_Research/#research-space","title":"Research space","text":"<p>To support collaborative research on campus, a PI may request a shared research space where files and softwares can be shared. </p>"},{"location":"Collaborative_Research/#software-installation-request","title":"Software installation request","text":"<p>We suggest the following two steps when you are in need of a particular software package.</p> <ol> <li>Go to <code>/opt/software/</code> on a dev-node. This is where all software packages are installed. Check if your desired software and specific versions are there. Please note there could be some name discrepancy caused by switch between lower and upper case letters. Take BBMap for example, all the versions available can be viewed by \"<code>ls -l /opt/software/BBMap</code>\". Send us a ticket if the software is totally absent in <code>/opt/software</code> or your desired version is missing. If the software and the version are both present in <code>/opt/software</code>, go to step 2.</li> <li>Follow the instructions to try loading software/version. If you can't find the software using <code>module spider</code>, send us a ticket. If you can find the software but not the desired version, send us a ticket too.</li> </ol>"},{"location":"Collaborative_Research/#using-our-academic-research-consulting-services","title":"Using our Academic Research Consulting Services","text":"<p>ICER's Academic Research Consulting service (ARCS) exists to meet the computationally complex needs of the MSU research community. Research groups in need of software and computational workflow development services, dedicated training programs, as well as those that require additional scientific expertise in areas such as bioinformatics, time-series analysis, and computational modeling may request long-term collaborations with ARCS research consultants. ICER ARCS consultants are PhD-level researchers with domain-specific computational expertise and experience at R1 institutions. ARCS collaborations require funding and may be funded internally through startup grants or unit-level support or externally by grant support or industrial partnerships. More information can be found here.</p>"},{"location":"Compilers_and_Libraries/","title":"Compilers and Libraries","text":"<p>The HPCC organizes most compilers and libraries into \"toolchains\". A toolchain is a set of tools bundled together that can be used to build software. This may be as minimal as a single compiler, or as expansive as a compiler with MPI, linear algebra, CUDA, and other helpful libraries.</p> <p>These toolchains are inherited from the program ICER uses to build much of its software, EasyBuild. For more information, please see the EasyBuild documentation pages for common toolchains and all toolchains.</p>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#common-toolchains","title":"Common toolchains","text":"<p>ICER primarily supports two toolchains on the HPCC: <code>foss</code> and <code>intel</code>. These are each numbered by year (with an <code>a</code> or <code>b</code> suffix for midyear and end of year releases respectively) and contain compilers and libraries that are compatible with each other.</p>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#foss-toolchain","title":"<code>foss</code> toolchain","text":"","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#foss-components","title":"<code>foss</code> components","text":"<p>The <code>foss</code> toolchain is currently derived from the following components (and is equivalent to doing a <code>module load</code> on each one individually):</p> <ul> <li><code>GCC</code>: Composed of <ul> <li><code>GCCcore</code>: The core GCC compilers</li> <li><code>binutils</code>: Extra binary tools   for use with GCC</li> </ul> </li> <li><code>OpenMPI</code>: MPI implementation for multi-process   programs</li> <li><code>FlexiBLAS</code>: BLAS and   LAPACK API (using <code>OpenBLAS</code> and reference   <code>LAPACK</code> as backends) for linear   algebra routines</li> <li><code>FFTW</code>: Library for Fast Fourier Transforms</li> <li><code>ScaLAPACK</code>: Parallel/distributed linear   algebra routines</li> </ul>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#foss-versions","title":"<code>foss</code> versions","text":"<p>To see the versions of this toolchain available on the HPCC use</p> <pre><code>module spider foss\n</code></pre> <p>and to see the versions of each component, use</p> <pre><code>module show foss/&lt;version&gt;\n</code></pre>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#foss-usage-examples","title":"<code>foss</code> usage examples","text":"CFortran Using OpenMP and shared libraries<pre><code>$ module load foss/2022a\n$ gcc foo.c -o foo \\\n    -fopenmp \\  # For OpenMP \n    -lflexiblas \\  # For BLAS/LAPACK\n    -lfftw3  # For FFTW\n$ ./foo\n</code></pre> Using OpenMP and shared libraries<pre><code>$ module load foss/2022a\n$ gfortran foo.f90 -o foo \\\n    -fopenmp \\  # For OpenMP\n    -lflexiblas \\  # For BLAS/LAPACK\n    -lfftw3  # For FFTW\n$ ./foo\n</code></pre> CFortran Using MPI<pre><code>$ module load foss/2022a\n$ mpicc foo.c -o foo\n$ srun ./foo  # Use srun for better SLURM integration instead of mpirun\n</code></pre> Using MPI<pre><code>$ module load foss/2022a\n$ mpifort foo.f90 -o foo\n$ srun ./foo  # Use srun for better SLURM integration instead of mpirun\n</code></pre>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#foss-sub-toolchains","title":"<code>foss</code> sub-toolchains","text":"<p>If the entire <code>foss</code> toolchain has too many dependencies for your needs, consider one of the sub-toolchains:</p> <ul> <li><code>GCC</code>: composed of<ul> <li><code>GCCCore</code></li> <li><code>binutils</code></li> </ul> </li> <li><code>gompi</code>: composed of<ul> <li><code>GCC</code></li> <li><code>OpenMPI</code></li> </ul> </li> </ul> <p>Or feel free to load the individual modules that you need by themselves.</p>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#intel-toolchain","title":"<code>intel</code> toolchain","text":"","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#intel-components","title":"<code>intel</code> components","text":"<p>The <code>intel</code> toolchain is currently derived from the following components (and is equivalent to doing a <code>module load</code> on each one individually):</p> <ul> <li><code>intel-compilers</code>:   Intel's set of (classic and oneAPI) C/C++ and Fortran compilers in addition   to:<ul> <li><code>GCCcore</code>: The core GCC compilers (used as       dependencies)</li> <li><code>binutils</code>: Extra binary tools       for use with GCC (used as dependencies)</li> </ul> </li> <li><code>impi</code>:   Intel's MPI implementation for multi-process programs</li> <li><code>imkl</code>:   Intel's BLAS and LAPACK implementations with FFT and other math libraries</li> </ul>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#intel-versions","title":"<code>intel</code> versions","text":"<p>To see the versions of this toolchain currently available on the HPCC use</p> <pre><code>module spider intel\n</code></pre> <p>and to see the versions of each component, use</p> <pre><code>module show intel/&lt;version&gt;\n</code></pre>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#intel-usage-examples","title":"<code>intel</code> usage examples","text":"CFortran Using OpenMP and shared libraries<pre><code>$ module load intel/2022b\n$ icx foo.c -o foo \\\n    -qopenmp \\  # For OpenMP \n    -qmkl # For BLAS/LAPACK/FFT\n$ ./foo\n</code></pre> Using OpenMP and shared libraries<pre><code>$ module load intel/2022b\n$ ifx foo.f90 -o foo \\\n    -qopenmp \\  # For OpenMP \n    -qmkl # For BLAS/LAPACK/FFT\n$ ./foo\n</code></pre> CFortran Using MPI<pre><code>$ module load intel/2022b\n$ mpiicc foo.c -o foo\n$ srun ./foo  # Use srun for better SLURM integration instead of mpirun\n</code></pre> Using MPI<pre><code>$ module load intel/2022b\n$ mpiifort foo.f90 -o foo\n$ srun ./foo  # Use srun for better SLURM integration instead of mpirun\n</code></pre>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#intel-sub-toolchains","title":"<code>intel</code> sub-toolchains","text":"<p>If the entire <code>intel</code> toolchain has too many dependencies for your needs, consider one of the sub-toolchains:</p> <ul> <li><code>intel-compilers</code>: composed of<ul> <li><code>GCCCore</code></li> <li><code>binutils</code></li> <li>The Intel compilers themselves</li> </ul> </li> <li><code>iimpi</code>: composed of<ul> <li><code>intel-compilers</code></li> <li><code>impi</code></li> </ul> </li> </ul> <p>Or feel free to load the individual modules that you need by themselves.</p>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#mix-and-match","title":"Mix and match","text":"<p>Certain components can be combined across toolchains. A notable example is the Intel Math Kernel Library (MKL), <code>imkl</code>, that can be loaded with any other compiler.</p> <p>C example mixing GCC and MKL</p> <pre><code>$ module load GCC/11.3.0\n$ module load imkl/2022.2.1\n$ gcc foo.c -o foo \\\n    -lmkl_rt  # MKL single dynamic library\n$ ./foo\n</code></pre> <p>For more information on linking against MKL, see Intel's documentation.</p>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#alternative-compilers-and-toolchains","title":"Alternative compilers and toolchains","text":"<p>In addition to the <code>foss</code> and <code>intel</code> toolchains, select versions of other compilers are available (with limited support) including:</p>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#aocc","title":"<code>AOCC</code>","text":"<p>AMD optimized compilers for C and Fortran. These use LLVM as a backend and therefore have the same command-line syntax as <code>Clang</code>.</p> <p>See also the documentation for Optimizing for AMD CPUs which gives information on installing AOCL, the AMD Optimizing CPU Libraries for AMD optimizations of math libraries, BLAS, FFTW, and others.</p> CFortran Using OpenMP<pre><code>$ module load AOCC/4.0.0\n$ clang foo.c -o foo.out \\\n    -fopenmp  # For OpenMP\n$ ./foo\n</code></pre> Using OpenMP<pre><code>$ module load AOCC/4.0.0\n$ flang foo.f90 -o foo.out \\\n    -mp  # For OpenMP\n$ ./foo\n</code></pre> CFortran Using AOCL libraries<pre><code># Assuming AOCL libraries are installed into $HOME/amd/aocl/4.0\n$ module load AOCC/4.0.0\n$ export AOCL_ROOT=$HOME/amd/aocl/4.0\n$ clang foo.c -o foo.out \\\n    -I${AOCL_ROOT}/include \\\n    -L${AOCL_ROOT}/lib \\\n    -fopenmp \\  # Required for multithreaded BLAS\n    -lflame \\  # For LAPACK\n    -lblis-mt \\  # For multithreaded BLAS\n    -lfftw3  # For FFTW\n$ ./foo\n</code></pre> Using AOCL libraries<pre><code># Assuming AOCL libraries are installed into $HOME/amd/aocl/4.0\n$ module load AOCC/4.0.0\n$ export AOCL_ROOT=$HOME/amd/aocl/4.0\n$ flang foo.f90 -o foo.out \\\n    -L${AOCL_ROOT}/lib \\\n    -fopenmp \\  # Required for multithreaded BLAS\n    -lflame \\  # For LAPACK\n    -lblis-mt \\  # For multithreaded BLAS\n    -lfftw3  # For FFTW\n$ ./foo\n</code></pre>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#clang","title":"<code>Clang</code>","text":"<p>LLVM-based C compilers. For math libraries like <code>FlexiBLAS</code> and <code>FFTW</code>, you will also need to load a compatible toolchain.</p> CFortran Using OpenMP<pre><code>$ module load GCCcore/11.3.0\n$ module load Clang/14.0.0\n$ clang foo.c -o foo.out \\\n   -fopenmp  # For OpenMP\n$ ./foo\n</code></pre> Using OpenMP<pre><code>$ module load foss/2022a  # Compatible with GCCcore/11.3.0\n$ module load Clang/14.0.0\n$ clang foo.c -o foo.out \\\n    -lflexiblas \\  # For BLAS/LAPACK\n    -lfftw3  # For FFTW\n$ ./foo\n</code></pre>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#nvhpc-formerly-pgi","title":"<code>NVHPC</code> (formerly PGI)","text":"<p>The NVIDIA HPC Software Development Kit. This includes C and Fortran CUDA-compatible compilers, an OpenMPI implementation, and GPU accelerated math libraries. It also includes implementations of BLAS and LAPACK, but for other CPU math libraries like <code>FFTW</code>, you will  need to load a compatible toolchain.</p> <p>Note that these compilers are most effective for building software on GPUs.</p> CFortran Using OpenMP and included shared libraries<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvc foo.c -o foo \\\n    -mp \\  # For OpenMP \n    -lblas \\  # For BLAS\n    -llapack  # For LAPACK\n$ ./foo\n</code></pre> Using OpenMP and included shared libraries<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvfortran foo.f90 -o foo \\\n    -mp \\  # For OpenMP \n    -lblas \\  # For BLAS\n    -llapack  # For LAPACK\n$ ./foo\n</code></pre> CFortran Using outside CPU libraries<pre><code>$ module load foss/2021a  # Compatible with GCC/10.3.0\n$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvc foo.c -o foo \\\n    -lflexiblas \\  # For BLAS/LAPACK\n    -lfftw3  # For FFTW\n$ ./foo\n</code></pre> Using outside CPU libraries<pre><code>$ module load foss/2021a  # Compatible with GCC/10.3.0\n$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvfortran foo.f90 -o foo \\\n    -lflexiblas  # For BLAS/LAPACK\n    -lfftw3  # For FFTW\n$ ./foo\n</code></pre> CFortran Using MPI<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ mpicc foo.c -o foo  # Uses NVHPC's OpenMPI\n$ srun ./foo  # Use srun for better SLURM integration instead of mpirun\n</code></pre> Using MPI<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ mpifort foo.f90 -o foo  # Uses NVHPC's OpenMPI\n$ srun ./foo  # Use srun for better SLURM integration instead of mpirun\n</code></pre>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#alternative-mpi-implementations","title":"Alternative MPI implementations","text":"<p>Though we recommend OpenMPI or Intel MPI, older installations of MPICH and MVAPICH2 are available. These are generally only compatible with a single compiler module as they were built on a one-off basis.</p> <p>You can see the available versions and how to load them with</p> <pre><code>module spider MPICH\n</code></pre> <p>or</p> <pre><code>module spider MVAPICH2\n</code></pre> <p>If OpenMPI, Intel MPI, or the available versions of <code>MPICH</code> and <code>MVAPICH2</code> will not work for your needs, please contact us to discuss alternatives.</p>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#basic-mathematical-library-benchmark","title":"Basic Mathematical Library Benchmark","text":"<p>Possibly outdated</p> <p>This benchmark was run when the AMD EPYC processors were installed (approximately 2020). The results may be outdated due to compiler improvements or hardware changes.</p> <p>This section shows the results of a basic test of the mathematical libraries with different compilers on the AMD EPYC Processors (i.e., those in the <code>amd20</code> cluster). The test runs many calculations of sine, cosine and\u00a0logarithm functions in parallel. Each of the calculations is independent from the others and finally they get summed up. The test is executed by a C program written with OpenMP multi-threading and compiled with different compilers and libraries. Three letters (A, G and I) followed by a digit (1 or 2) are used to specify different tests:</p> Letters First Second Digit A AMD Compiler AMD\u00a0basic mathematical Library 1: all threads running in one socket G GNU Compiler GNU\u00a0basic mathematical Library (<code>-lm</code>) 2: threads evenly spread to two different sockets I Intel Compiler Intel\u00a0basic mathematical Library (included in compiler) <p>where the letter in the first and second position represents which compiler and basic mathematical library is in use respectively. The performance results are presented in the following figure:</p> <p></p> <p>where all timing values were derived by the average of running ten times. As you can see in the figure, the performance of the parallel scaling is almost linear for all compilers and the scaling efficiency is strong (about 61% for AA1 and AA2). From the comparison of the timing results, Intel compiler with its library shows the best performance. However, GCC and AMD compilers with AMD basic mathematical library also perform well. In the results of 128 threads, the elapsed time of AMD compiler with AMD library are very closed to the time of Intel's. In the tests of spreading threads, we also find out all threads running on one socket has no difference from spreading them on two different sockets.</p> <p>The same C program was also compiled and run on an <code>intel18</code> and an <code>intel16</code> node. The timing of <code>amd20</code> node with 128 threads is about 3 times faster than the performance of\u00a0<code>intel18</code> (with 40 threads) and 4.5 times faster than the performance of\u00a0<code>intel16</code> (with 28 threads). The decrease in timing is well prorated with the increase on thread number.\u00a0</p>","tags":["reference","compilers"]},{"location":"Compiling_for_GPUs/","title":"Compiling for GPUs","text":"<p>There are a few different ways for code to access GPUs. This page will focus only on compiling code that uses those techniques rather than explaining the techniques themselves.</p> <p>For more general information on compilers on the HPCC, see our reference page.</p>","tags":["reference","compilers","GPU"]},{"location":"Compiling_for_GPUs/#gpu-compatibility","title":"GPU compatibility","text":"<p>Because GPU hardware can change dramatically from device to device, it's important to keep in mind the GPUs your code will run on when compiling. In practical terms on the HPCC, this means selecting version of CUDA that is best suited for your target GPU.</p> <p>See the table below for the CUDA compute capability and associated CUDA versions of the various GPUs available on the HPCC (additionally, see this CUDA version table and the compute capabilities for GPUs for more information). Using the maximum version or lower will ensure that code compiled with that version of CUDA will be able to run on the associated GPU by default.</p> GPU Compute capability Minimum CUDA version Maximum CUDA version* <code>k20</code> 3.5 4.2 10.2 <code>k80</code> 3.7 4.2 10.2 <code>v100</code> 7.0 9.0 <code>a100</code> 8.0 11.0 <p>*Support beyond the maximum version</p> <p>CUDA versions greater than the maximum CUDA version listed above may still work for certain GPUs (for example, CUDA versions up to 11.4 can work with compute capabilities 3.5 and 3.7), but will not compile for them by default. In these situations, you should explicitly state the minimum compute capability when compiling as discussed below.</p> <p>Compiling for maximum compatibility</p> <p>The minimum version listed in the table above is necessary to support all capabilities of a GPU. However, the drivers for NVIDIA GPUs are generally backwards compatible with earlier versions of PTX code (see below). This means you can use an earlier CUDA version to compile code, and it will be able to run on any newer GPUs. For more information, see Compiling for specific GPUs below.</p> <p>For a good compromise between features and compatibility with all GPUs at ICER, CUDA 9.x or 10.x will work well.</p>","tags":["reference","compilers","GPU"]},{"location":"Compiling_for_GPUs/#cuda-code","title":"CUDA code","text":"<p>CUDA Fortran (that is, Fortran code with CUDA specific extensions) is compiled using the same <code>nvfortran</code> compiler described in the <code>NVHPC</code> section of our compiler reference.</p> <p>CUDA C/C++ code (that is, C/C++ code with CUDA specific extensions to run kernels on a GPU device), is compiled using the <code>nvcc</code> compiler. The recommended way to access this compiler is to load the <code>NVHPC</code> module as described in our compiler reference or a <code>fosscuda</code> or <code>gcccuda</code> toolchain. Alternatively, <code>nvcc</code> is also included in any of the <code>CUDA</code> modules.</p> <p>Behind the scenes, <code>nvcc</code> will use <code>gcc/g++</code> to compile the C/C++ code itself. The version used is whatever is on your path. This means that if you load <code>CUDA</code> by itself, you will be using the system version of <code>gcc/g++</code> which is much older than the versions available in the module system. For this reason, we recommend loading either the <code>NVHPC</code> modules (which will use the version of <code>gcc/g++</code> suffixing the module version), or loading a <code>fosscuda</code> or <code>gcccuda</code> module that will load an appropriate version of <code>CUDA</code> along with the corresponding <code>foss</code> or <code>GCC</code> toolchain.</p> CFortran CUDA with NVHPC<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvcc foo.cu -o foo\n$ ./foo\n</code></pre> CUDA with NVHPC<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvfortran foo.cuf -o foo\n$ ./foo\n</code></pre>","tags":["reference","compilers","GPU"]},{"location":"Compiling_for_GPUs/#compiling-for-specific-gpus","title":"Compiling for specific GPUs","text":"<p>GPU code is compiled in two stages: </p> <ol> <li>Compiling into a virtual instruction set like assembly code (called PTX)</li> <li>Compiling the virtual instructions into binary code (called a cubin) that actually runs on the GPU</li> </ol> <p>These stages are controlled by the compute capability specified to <code>nvcc</code> (in the previous examples, this is set implicitly to 5.2) and <code>nvcc</code> can embed the results of stage 1, stage 2, or both for various compute capabilities in the final executable. If the stage 2 output is missing for the compute capability of the GPU that the code is executed on, the NVIDIA driver will just-in-time (JIT) compile any stage 1 code it finds at runtime into stage 2 code appropriate for that GPU.</p> <p>In general, you should use the lowest compute capability your code supports in step 1 (for the widest compatibility with future JIT compilation) and the compute capability of the target GPU in step 2 (for the best optimization).</p> <p>To specify compute capability x.y for stage 1, use the <code>-arch=compute_xy</code> flag, and for stage 2, use the <code>-code=sm_xy</code> flag. You can also specify <code>-code=compute_xy</code> to embed the output of stage 1 into the final binary for JIT compilation. Multiple <code>compute_xy</code> and <code>sm_xy</code> values can be supplied to <code>-code</code> in a comma separated list.</p> <p>See NVIDIA's documentation on GPU compilation for more information and examples.</p> <p>Compiling for <code>k20</code> and <code>k80</code> GPUs with CUDA 11.4</p> <p>As discussed above, CUDA 11.4 will not compile for <code>k20</code> and <code>k80</code> GPUs by default. However, we can specify the corresponding compute capabilities explicitly:</p> <pre><code>$ module load NVHPC/21.9-GCC-10.3.0-CUDA-11.4\n$ nvcc foo.cu -o foo \\\n    -arch=compute_35 -code=compute_35,sm_35,sm_37,sm_70\n$ ./foo\n</code></pre> <p>The resulting executable will be able to run on all GPUs at ICER:</p> <ul> <li>The <code>k20</code> by compiling <code>compute_35</code> (and higher)-compatible PTX into an <code>sm_35</code>-compatible <code>cubin</code>.</li> <li>The <code>k80</code> by compiling <code>compute_35</code> (and higher)-compatible PTX into an <code>sm_37</code>-compatible <code>cubin</code>.</li> <li>The <code>v100</code> by compiling <code>compute_35</code> (and higher)-compatible PTX into an <code>sm_70</code>-compatible <code>cubin</code>.</li> <li>The <code>a100</code> by JIT compiling the embedded <code>compute_35</code> (and higher)-compatible PTX at runtime.</li> </ul>","tags":["reference","compilers","GPU"]},{"location":"Compiling_for_GPUs/#cuda-libraries","title":"CUDA libraries","text":"<p>The <code>NVHPC</code> and <code>CUDA</code> modules offer many CUDA accelerated math libraries, like cuBLAS, cuSOLVER and cuFFT.</p> <p>For C/C++ code, since using many these libraries do not require writing CUDA code, using the <code>nvcc</code> compiler is optional. We refer to the documentation for the specific libraries for how to link them, but give examples of linking <code>cuBLAS</code> and <code>cuFFT</code> with <code>nvcc</code> and the GNU compilers directly.</p> <p>Take care to link to libraries that are all distributed in the same version of CUDA, to use a version of CUDA compatible with the desired GPUs, and (if using shared libraries) to load the same version of CUDA when running the executable.</p> CFortran Using CUDA shared libraries with NVHPC<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvcc foo.c -o foo \\\n    -lcublas \\  # For BLAS\n    -lcufft  # For cuFFT\n$ ./foo\n</code></pre> Using CUDA shared libraries with NVHPC<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvfortran foo.f90 -o foo \\\n    -cudalib=cublas \\  # For cuBLAS\n    -cudalib=lcufft  # For cuFFT\n$ ./foo\n</code></pre> <p>Fortran support for linking to the CUDA libraries is limited to NVIDIA's compilers. See NVIDIA's Fortran CUDA interfaces for more information. </p> C Using CUDA shared libraries with fosscuda<pre><code>$ module load fosscuda/2020a\n$ gcc foo.c -o foo \\\n    -lcudart  \\ # For CUDA runtime routines like memory management\n    -lcublas \\  # For cuBLAS\n    -lcufft  # For cuFFT\n$ ./foo\n</code></pre>","tags":["reference","compilers","GPU"]},{"location":"Compiling_for_GPUs/#cudnn","title":"cuDNN","text":"<p>A popular set of CUDA libraries not included in the CUDA toolkit is cuDNN. On the HPCC, cuDNN is available as a module. Search for available versions with</p> <pre><code>module spider cuDNN\n</code></pre> <p>and choose one which uses a version of CUDA compatible with any other GPU-based work you may be doing. See NVIDIA's API reference for which libraries to link against.</p>","tags":["reference","compilers","GPU"]},{"location":"Compiling_for_GPUs/#gpu-offloading","title":"GPU offloading","text":"<p>Parts of code can be offloaded onto the GPUs using directive-based APIs like OpenMP and OpenACC. Currently, the recommended approach is to use OpenACC with the NVIDIA HPC SDK compilers.</p>","tags":["reference","compilers","GPU"]},{"location":"Compiling_for_GPUs/#openacc","title":"OpenACC","text":"<p>Offloading with OpenACC is primarily supported by the NVIDIA's compilers in the <code>NVHPC</code> modules. Using the <code>-acc</code> option will activate OpenACC and run kernels by default on the GPU.</p> <p>The specific compute capabilities of the desired target GPUs can also be passed to compile compatible binaries for the respective GPUs. GPUs with other compute capabilities will incur a slight one-time cost when the executable is run (so that embedded PTX code can be JIT compiled to appropriate binary).</p> CFortran GPU offloading with OpenACC with NVHPC<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvc foo.c -o foo \\\n    -acc \\  # To use OpenACC (default is on GPU)\n    -gpu=cc35,cc37,cc70,cc80  # To embed GPU code for various compute capabilities\n$ ./foo\n</code></pre> GPU offloading with OpenACC with NVHPC<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvfortran foo.c -o foo \\\n    -acc \\  # To use OpenACC (default is on GPU)\n    -gpu=cc35,cc37,cc70,cc80  # To embed GPU code for various compute capabilities\n$ ./foo\n</code></pre> <p>The same versions of the <code>GCC</code> modules discussed in the OpenMP section above support OpenACC, however with the same caveats. Experiment with these versions at your own risk.</p> CFortran GPU offloading with OpenMP with GCC<pre><code>$ module load GCC/11.1.0-cuda-9.2.88\n$ gcc foo.c -o foo \\\n    -fopenacc \\  # To activate OpenACC instructions\n    -foffload=nvptx-none=\"-lm\"  # To offload code to NVIDIA GPUs, and\n                                # ensure that offloaded code has access\n                                # to math libraries\n$ ./foo\n</code></pre> GPU offloading with OpenMP with GCC<pre><code>$ module load GCC/11.1.0-cuda-9.2.88\n$ gfortran foo.c -o foo \\\n    -fopenacc \\  # To activate OpenMP instructions\n    -foffload=nvptx-none=\"-lm -lgfortran\"  # To offload code to NVIDIA\n                                           # GPUs, and # ensure that \n                                           # offloaded code has access \n                                           # to math and Fortran libraries\n$ ./foo\n</code></pre>","tags":["reference","compilers","GPU"]},{"location":"Compiling_for_GPUs/#openmp","title":"OpenMP","text":"<p>Offloading with OpenMP is primarily supported by the NVIDIA's compilers in the <code>NVHPC</code> modules. Using the <code>-mp=gpu</code> option will set OpenMP code to use a GPU as a target device.</p> <p>The specific compute capabilities of the desired target GPUs can also be passed to compile compatible binaries for the respective GPUs. GPUs with other compute capabilities will incur a slight one-time cost when the executable is run (so that embedded PTX code can be JIT compiled to appropriate binary).</p> CFortran GPU offloading with OpenMP with NVHPC<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvcc foo.c -o foo \\\n    -mp=gpu \\  # To use OpenMP on GPU\n    -gpu=cc35,cc37,cc70,cc80  # Embed GPU code for compute capabilities\n$ ./foo\n</code></pre> GPU offloading with OpenMP with NVHPC<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvfortran foo.c -o foo \\\n    -mp=gpu \\  # To use OpenMP on GPU\n    -gpu=cc35,cc37,cc70,cc80  # Embed GPU code for compute capabilities\n$ ./foo\n</code></pre> <p>A few versions of the <code>GCC</code> modules available on the HPCC have highly experimental support for offloading OpenMP code to GPUs. These versions include a <code>-cuda</code> or <code>-offload</code> suffix in the version name. Use</p> <pre><code>module spider GCC\n</code></pre> <p>to search for versions including these suffixes.</p> <p>Support for these compilers is very limited, and simple tests indicate that they can suffer from reduced performance in comparison to NVIDIA's compilers or running multi-threaded (or, in extreme cases, single-threaded) on the CPU. Experiment with these versions at your own risk.</p> CFortran GPU offloading with OpenMP with GCC<pre><code>$ module load GCC/11.1.0-cuda-9.2.88\n$ gcc foo.c -o foo \\\n    -fopenmp \\  # To activate OpenMP instructions\n    -foffload=nvptx-none=\"-lm\"  # To offload code to NVIDIA GPUs, and\n                                # ensure that offloaded code has access\n                                # to math libraries\n$ ./foo\n</code></pre> GPU offloading with OpenMP with GCC<pre><code>$ module load GCC/11.1.0-cuda-9.2.88\n$ gfortran foo.c -o foo \\\n    -fopenmp \\  # To activate OpenMP instructions\n    -foffload=nvptx-none=\"-lm -lgfortran\"  # To offload code to NVIDIA\n                                           # GPUs, and # ensure that \n                                           # offloaded code has access \n                                           # to math and Fortran libraries\n$ ./foo\n</code></pre> <p>Other GPU specific compilation options can be passed in the quotes following <code>-foffload=nvptx-none=</code>, e.g., <code>-foffload=nvptx-none=\"-lm -latomic -O3\"</code>.</p>","tags":["reference","compilers","GPU"]},{"location":"Conditional_Statements/","title":"Conditional statements","text":"<p>Conditional statements help you to execute parts of a shell script only if certain conditions hold. Below, we give the syntax and examples for three common variants of <code>if</code> statements as well as the <code>case</code> statement which acts like switch statements in other languages.</p>","tags":["reference","command line"]},{"location":"Conditional_Statements/#if-fi","title":"<code>if ... fi</code>","text":"<p>General syntax:</p> <pre><code>if [ expression ]\nthen\n   Statement(s) to be executed if expression is true\nfi\n</code></pre> <p>Here, <code>expression</code> often takes the form of comparing strings using operators <code>==</code> or <code>!=</code> or comparing numbers using operators <code>-eq</code> (equal), <code>-ne</code> (not equal), <code>-lt</code> (less than), <code>-gt</code> (greater than), or <code>-ge</code> (greater than or equal to).</p> <p>However, there are other conditional expressions, many related to files (for example <code>-e test.txt</code> returns true if the file <code>test.txt</code> exists). For a full listing, see this manual page.</p> <p>Example:</p> test.sh<pre><code>#!/bin/sh\n\nif [ $1 == $2 ]\nthen\n   echo \"$1 is equal to $2\"\nfi\n</code></pre> <p>This is the result of a sample run.</p> <pre><code>sh ./test.sh\n is equal to\n$ sh ./test.sh 1 2\n$ sh ./test.sh 12 12\n12 is equal to 12\n</code></pre>","tags":["reference","command line"]},{"location":"Conditional_Statements/#if-else-fi","title":"<code>if ... else ... fi</code>","text":"<p>General syntax: </p> <pre><code>if [ expression ]\nthen\n   Statement(s) to be executed if expression is true\nelse\n   Statement(s) to be executed if expression is not true\nfi\n</code></pre> <p>Example:</p> test.sh<pre><code>#!/bin/sh\n\nif [ $1 == $2 ]\nthen\n   echo \"$1 is equal to $2\"\nelse\n   echo \"$1 is not equal to $2\"\nfi\n</code></pre> <p>This is the result of a sample run.</p> <pre><code>$ sh ./test.sh\n is equal to\n\n$ sh ./test.sh 1 2\n1 is not equal to 2\n\n$ sh ./test.sh 12 12\n12 is equal to 12\n</code></pre>","tags":["reference","command line"]},{"location":"Conditional_Statements/#if-elif-else-fi","title":"<code>if ... elif ... else ... fi</code>","text":"<p>General syntax: </p> <pre><code>if [ expression 1 ]\nthen\n   Statement(s) to be executed if expression 1 is true\nelif [ expression 2 ]\nthen\n   Statement(s) to be executed if expression 2 is true\nelif [ expression 3 ]\nthen\n   Statement(s) to be executed if expression 3 is true\nelse\n   Statement(s) to be executed if no expression is true\nfi\n</code></pre> <p>Example:</p> test.sh<pre><code>#!/bin/sh\n\nif [ $1 -eq $2 ]\nthen\n   echo \"$1 is equal to $2\"\nelif [ $1 -gt $2 ]\nthen\n   echo \"$1 is greater than $2\"\nelif [ $1 -lt $2 ]\nthen\n   echo \"$1 is less than $2\"\nelse\n   echo \"None of the condition met\"\nfi\n</code></pre> <p>This is the result of a sample run.</p> <pre><code>#!/bin/sh\nsh ./test.sh\n is equal to\n$ sh ./test.sh 1 2\n1 is less than 2\n$ sh ./test.sh 12 2\n12 is greater than 2\n$ sh ./test.sh 12 12\n12 is equal to 12\n</code></pre>","tags":["reference","command line"]},{"location":"Conditional_Statements/#case-esac","title":"<code>case ... esac</code>","text":"<p>General syntax:</p> <pre><code>case word in\n    patterns ) commands ;;\nesac\n</code></pre> <p>Example:</p> test.sh<pre><code>#!/bin/sh\n\nread -p \"Enter a number between 1 and 3 inclusive &gt; \" character\ncase $character in\n    1 ) echo \"You entered one.\"\n        ;;\n    2 ) echo \"You entered two.\"\n        ;;\n    3 ) echo \"You entered three.\"\n        ;;\n    * ) echo \"You did not enter a number between 1 and 3.\"\nesac\n</code></pre> <p>This is the result of a sample run.</p> <pre><code>$ sh test.sh\nEnter a number between 1 and 3 inclusive &gt; 1\nYou entered one.\n\n$ bash test.sh\nEnter a number between 1 and 3 inclusive &gt; 2\nYou entered two.\n\n$sh test.sh\nEnter a number between 1 and 3 inclusive &gt; 3\nYou entered three.\n\n$sh test.sh\nEnter a number between 1 and 3 inclusive &gt; 4\nYou did not enter a number between 1 and 3.\n</code></pre>","tags":["reference","command line"]},{"location":"Connect_over_SSH_with_VS_Code/","title":"Connect over SSH with VS Code","text":"<p>VS Code allows you to work on remote servers from your local machine. </p>","tags":["how-to guide","VS Code","VScode","ssh"]},{"location":"Connect_over_SSH_with_VS_Code/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install VS Code by following the instructions.</li> <li>(Windows only) Install OpenSSH for Windows by following the instructions.</li> <li>Configure SSH so that you can access development nodes using proxy jumps by following these directions.</li> <li>(Optional) Set up SSH keys to connect to the HPCC without needing to enter a password. For Windows users, use the MobaXterm GUI instructions (or ensure that your private SSH key is saved to <code>C:\\Users\\&lt;Account_Name&gt;\\.ssh\\id_rsa</code>).</li> </ol> <p>Warning</p> <p>If you do not add the proxy jumps to the <code>.ssh/config</code> file on your local computer as described in step 3 above, you will not be able to access the HPCC from VS Code.</p>","tags":["how-to guide","VS Code","VScode","ssh"]},{"location":"Connect_over_SSH_with_VS_Code/#connecting-to-the-hpcc-with-vs-code","title":"Connecting to the HPCC with VS Code","text":"<p>First, we need the Remote Development extension for VS Code, which can be obtained here</p> <p></p> <p>When the extension is installed, press F1, and select 'Remote-SSH: Connect to Host...' You may need to start typing this option to get it to appear. </p> <p>Then select intel18, k80, or any other development node. </p> <p>Never connect to a gateway node</p> <p>If you have a <code>Host gateway</code> section in your <code>.ssh/config</code> file, <code>gateway</code> will be an option when connecting to a host using VS Code as shown in the screenshot above. Never choose this option. It will run a VS Code server on gateways node which are not meant for running user processes. This can lead to system slowdown and stop users from being able to login to the HPCC altogether.</p> <p>You may be asked for the type of the remote platform, in which case, choose \"Linux\". Accept any other questions, and if you do not have SSH keys setup, you will be asked for your password.</p> <p>Afterwards new VS Code window will pop up. When you are successfully connected, bottom left of the VS Code will show server information such as 'SSH: intel18'. </p>","tags":["how-to guide","VS Code","VScode","ssh"]},{"location":"Connect_over_SSH_with_VS_Code/#working-on-the-hpcc-with-vs-code","title":"Working on the HPCC with VS Code","text":"<p>Now that you are connected, you can run commands and codes from VS Code. When you are connected to the HPCC, all files created through VS Code will be saved to the HPCC, not on your local machine.</p> <p>To test the ability to run remote code, let's create a Python file 'hello.py' with VS Code. Click 'New file' and create a file with the following content to the file: </p> <pre><code>print('hello!')\n</code></pre> <p>Now, open a terminal from 'Terminal' menu from VS Code (or by pressing the key sequence CTRL+Shift+`) and run the code.</p> <pre><code>$ python hello.py\n</code></pre> <p>Once you have connected the server, click the Remote Explorer to see the list of servers. Click the icon to connect to host in a new window. Right click the name to see the option.  </p> <p></p>","tags":["how-to guide","VS Code","VScode","ssh"]},{"location":"Connect_to_HPCC_System/","title":"Connect to the HPCC","text":"<p>This tutorial requires that you have an SSH client present on your system. It will walk you through connecting to the HPCC's gateway, moving on to a development node, and testing your system's X Windows server.</p>","tags":["how-to guide","ssh"]},{"location":"Connect_to_HPCC_System/#connect-to-the-gateway","title":"Connect to the gateway","text":"<p>Open the SSH client on your local computer and run <code>ssh -XY &lt;username&gt;@hpcc.msu.edu</code>, where <code>&lt;username&gt;</code> should be your HPCC account name. The first time you connect, SSH will ask if you trust the SSH key fingerprint of the HPCC. Type <code>yes</code> and press enter to continue. SSH will prompt  for your password. Please type the password of your MSU NetID (the password will be invisible). Upon a successful login, you will see a welcome message and current usage load for each development node. </p> <p>Our gateway is for entrance to the HPCC system only. Running programs and other tasks should be done on development nodes (see the next section).</p>","tags":["how-to guide","ssh"]},{"location":"Connect_to_HPCC_System/#ssh-to-a-development-node","title":"SSH to a Development Node","text":"<p>To access a development (dev) node from gateway, pick one and type <code>ssh -X node-name</code> (if you don't need to launch a GUI program, omit  <code>-X</code>).   See here for dev-node usage and policy.</p>","tags":["how-to guide","ssh"]},{"location":"Connect_to_HPCC_System/#testing-your-x-windows-system","title":"Testing your X Windows System","text":"<p>An X Windows System allows graphical user interfaces (GUIs) to be shared over SSH. X Windows Systems can be part of any operating system - Windows, Mac, or Linux. You may also hear references to \"X11\" when people are talking about X Windows Systems, as this is the most common protocol used to implement an X Windows System.</p> <p>If your local computer has an X Windows client installed (such as Xquatz or MobaXterm) and you log into the nodes with the <code>-X</code> option, you can test if GUI will work by running <code>xeyes</code> on the command line. If everything is set correctly, you should see a pop-up window containing a pair of eyes.</p> <p>If you see an error about X11 forwarding when logging into the gateway (e.g. \"Warning: untrusted X11 forwarding setup failed: xauth key data not generated\"), please check your local machine's \"config\" file (<code>$HOME/.ssh/config</code>) setting. It should look like</p> $HOME/.ssh/config<pre><code>Host *\nForwardAgent yes\nForwardX11 yes\nForwardX11Trusted yes\nXAuthLocation /opt/X11/bin/xauth\n</code></pre>","tags":["how-to guide","ssh"]},{"location":"Connections_to_compute_nodes/","title":"Connections to compute nodes","text":"","tags":["ssh","how-to guide"]},{"location":"Connections_to_compute_nodes/#ssh","title":"SSH","text":"<p>When a job is running on a compute node, the owner of the job can access the node using <code>ssh</code> from a development node. This will connect your terminal to the  shell on the compute node where the job is running. </p> <p>If your job is running on  the node <code>amr-070</code> the command would be <code>ssh amr-070</code>. Once on the compute node you can inspect individual CPU and memory usage with <code>top</code> and <code>htop</code>.</p>","tags":["ssh","how-to guide"]},{"location":"Connections_to_compute_nodes/#connecting-from-your-local-machine","title":"Connecting from your local machine","text":"<p>If you have SSH Tunneling set up, we can extend the  ProxyJump framework to connect to compute nodes.</p> <p>To do this, add the following lines to your <code>.ssh/config</code>:  <pre><code>    Host lac* vim* skl* nvl* amr* nvf* nif* nal* acm*\n    User here_you_put_your_net_id\n    ProxyJump intel18\n</code></pre></p> <p>The three-letter names correspond to the kinds of nodes available from each cluster. Notice that we are using the development node <code>intel18</code> to connect to the compute nodes, rather than the gateway.</p> <p>We can then run a command such as <code>ssh amr-070</code> from our local terminal (assuming we have a job running on that node). This will connect us directly to that node.</p>","tags":["ssh","how-to guide"]},{"location":"Connections_to_compute_nodes/#ondemand","title":"OnDemand","text":"<p>Using OnDemand you can access the compute node that your job is running on in the same way by opening a development node from  the Development Nodes menu and then using the <code>ssh</code> command as described above.  You can identify the node for interactive sessions from the My Interactive  Sessions tab. It is located after the word \"Host\". Do not include the <code>&gt;_</code>  characters in your <code>ssh</code> command. The image below shows a job running on the  node <code>amr-070</code>.</p> <p></p>","tags":["ssh","how-to guide"]},{"location":"Containers_Overview/","title":"Containers (Singularity and Docker)","text":"<p>Suppose that you develop or use some software on your computer that you would like to use on the HPCC. If that software requires many dependencies or a tricky setup to work properly, it may be difficult to transfer over to the HPCC. This is a prime opportunity to employ containers to simplify your workflow and get you started on the HPCC!</p> <p>A Linux container provides an environment that's different from the host computer you may be running on (e.g., your laptop or the HPCC). For example, you could run a different version of Linux (e.g., running Ubuntu on our CentOS system). One advantage of containers is if your software requires a newer version of system libraries (e.g. glibc) than is available in our operating system, then you can run your software in a container. Another advantage is that many pieces of software provide containers that you can download and start working with immediately, no matter what computer/operating system/software stack you may be using. </p> <p>The following three pages explain container-usage:</p> <ul> <li>Docker: Docker is the most popular tool to work with containers. This tutorial walks you through an example of running containers and building your own.</li> <li>Singularity Introduction: Though Docker is the most popular way to work with containers, it requires permissions that users are not allowed on the HPCC. Instead, HPCC systems use a (Docker compatible) tool called Singularity. This tutorial walks you through the basics of using Singularity to run containers (created with Docker or Singularity) on the HPCC.</li> <li>Singularity Advanced Topics: This page serves as reference for some more advanced topics related to tweak your usage of Singularity on the HPCC.</li> </ul>","tags":["explanation","containers","Docker","Singularity"]},{"location":"Dashboard/","title":"Dashboard","text":"<p>Toggle Fullscreen</p>","tags":["dashboard"]},{"location":"Data_Display_Debugger/","title":"Data Display Debugger","text":"<p>DDD stands for 'Data Display Debugger'. It is a GUI front end of GDB, the GNU debugger. The main advantage of DDD over GDB is that DDD offers a GUI. In this tutorial, we will learn about</p> <ul> <li>setting and removing breakpoints</li> <li>tracing through programs</li> <li>examining data at various points in execution.</li> </ul>","tags":["tutorial","DDD","Debugging"]},{"location":"Data_Display_Debugger/#starting-ddd","title":"Starting DDD","text":"<p>Since DDD is a graphical debugger, you will either need to SSH with X forwarding or use OnDemand.</p>","tags":["tutorial","DDD","Debugging"]},{"location":"Data_Display_Debugger/#ssh-with-x","title":"SSH with X","text":"<p>If you want to debug from a dev node, first make sure that you SSH using the <code>-X</code> option. You can then start DDD using <code>ddd</code> which will open a DDD window on your computer:</p> <pre><code>$ ssh -X user@hpcc.msu.edu\n$ ssh dev-amd20  # only the first ssh needs -X\n$ ddd\n</code></pre> <p>If you would like to run your debugging job interactively on a compute node, please see the instructions for running an interactive graphical application.</p>","tags":["tutorial","DDD","Debugging"]},{"location":"Data_Display_Debugger/#ondemand","title":"OnDemand","text":"<p>You can also use DDD in your browser through OnDemand. Log in using your MSU credentials on the OnDemand site. Then from the Jobs menu, select Interactive Desktop. Once your job starts, you can open your Interactive Desktop and choose \"Data Display Debugger\" from the Applications menu under Programming.</p>","tags":["tutorial","DDD","Debugging"]},{"location":"Data_Display_Debugger/#the-ddd-interface","title":"The DDD interface","text":"<p>When you start DDD, you will see a DDD window like this:</p> <p></p> <p>The DDD window consists of 4 sections:</p> <ul> <li>data window</li> <li>source window</li> <li>machine code window</li> <li>GDB console</li> </ul> <p>You can show/hide each of them in View menu.</p> <p>You can customize the DDD environment in Edit \u2192 Preferences menu.</p> <p>For example, to display line numbers in source window, under Edit \u2192 Preferences \u2192 Source, check 'Display Source Line Numbers'.</p>","tags":["tutorial","DDD","Debugging"]},{"location":"Data_Display_Debugger/#opening-a-program","title":"Opening a program","text":"<p>To use DDD, we need a program to debug. Let's use the following code.</p> debug_ex.c<pre><code>#include &lt;stdio.h&gt;\n\nint main(int argc, char** argv){\n  for(int i = 0; i &lt; 10; i++){\n    int j = i*i;\n    printf(\"%d \", j);\n  }\n  printf(\"\\n\");\n}\n</code></pre> <p>First, you need to compile this code with <code>-g</code> option to include the debug symbols. Run this on the command line:</p> <pre><code>gcc -g debug_ex.c -o debug_ex\n</code></pre> <p>Now, to open this in DDD, you can either select it in the File \u2192 Open Program menu or you can run DDD with this executable from the command line like</p> <pre><code>ddd debug_ex\n</code></pre> <p>Even though you open an executable such as <code>debug_ex</code>, DDD will show the source file name such as <code>debug_ex.c</code>.</p>","tags":["tutorial","DDD","Debugging"]},{"location":"Data_Display_Debugger/#setting-breakpoints","title":"Setting breakpoints","text":"<p>Breakpoints stop your program in the middle of running to examine the current state of variables and data structures.  You can continue from where you set the breakpoint to finish program execution.</p> <p>To set a breakpoint, double click to the left of the source line in the source window. A STOP icon will appear next to it.</p> <p>Click Run at the top of the window to start execution or type <code>run</code> in the GDB console. A green arrow will appear as soon as you hit the breakpoint.</p> <p></p> <p>The breakpoints you set can be deleted or disabled by right-clicking on the line with the breakpoint and choosing either the \"Disable Breakpoint\" or \"Delete Breakpoint\" options.</p> <p>In order to set breakpoints in other files (i.e., not in the <code>main()</code> function), choose the \"Open Source\" option from the File menu of DDD. The file dialog should appear. </p> <p>The example in the figure above has a breakpoint at line 6. The program runs to the line number 5 and waits your input. You can run the code a line by line with 'next' command (by either clicking the button at the top of the screen or typing <code>next</code> into the GDB console). </p> <p>To see a variable's value, type <code>print &lt;variable_name&gt;</code> in the GDB console. For example, <code>print i</code> will show the value of the variable <code>i</code>. You can also right click the variable in the source code choose <code>Print &lt;variable_name&gt;</code>.</p> <p>To go to the next break point, click Cont button or type <code>cont</code> on GDB console.</p> <p>When you find bugs, edit your source code in your editor of choice and recompile the code. Reload the new source code into DDD using the Source menu: Source \u2192 Reload Source.</p>","tags":["tutorial","DDD","Debugging"]},{"location":"Data_Display_Debugger/#common-commands","title":"Common commands","text":"<p>DDD offers command buttons, but you can also type commands directly on GDB console.  </p> command Description <code>help</code> help documentation for topics and commands <code>help breakpoint</code> Lists help information about breakpoints <code>break</code> sets breakpoint <code>break line_number</code> Sets breakpoint at a line number <code>break function_name</code> Sets breakpoint at the begining of function name <code>enable</code>, <code>disable</code>, <code>delete</code>/<code>clear</code> Enable, disable, or delete one or more breakpoints. <code>disable 3</code> Disables breakpoint number 3 <code>clear line_number</code> Clears breakpoint at line_number <code>delete 3</code> Deletes breakpoint number 3 <code>delete</code> Deletes all beakpoints <code>run</code> Starts program running from the begining. <code>continue</code> (or <code>cont</code>) Continues execution from the current line to the next breakpoint <code>step</code> (or <code>s</code>) Execute next line(s) of program <code>step</code> Executes one line of a program <code>step number</code> Executes next number of lines of program next (or n) Like step, but treats a function as a single line. <code>next</code> Execute the next line <code>next number</code> Executes next number of lines of program <code>until line_number</code> Executes program until line number <code>quit</code> quit DDD <code>list</code> Lists program source code <code>condition</code> Conditional breakpoints <code>print</code> Display program values, results of expressions <code>whatis</code> List type of an expression <code>whatis j</code> Shows data type of expression <code>j</code> <code>info</code> Get information <code>info locals</code> Shows local variables in current stack frame <code>info args</code> Shows the argument variable of current stack frame <code>info break</code> Show breakpoints <code>set</code> Change values of variables, memory, registers <code>set x = 123*y</code> Set variable <code>x</code>'s value to <code>123*y</code>","tags":["tutorial","DDD","Debugging"]},{"location":"Data_Display_Debugger/#examining-data","title":"Examining data","text":"<p>While the program is running, you may want to examine the contents of variables. You can do this by right-clicking on a variable name in the DDD window.</p> <p>Upon right-clicking, select \"Display\". If you want to display the value of a pointer, use the \"Display*\" menu item.</p> <p></p> <p>Right-clicking on a variable name offers other capabilities such as Print, Lookup, What Is (showing the data type), Break, and Clear.</p> <p>Instead of right-clicking, you can peek at memory contents also. To do that, click Data \u2192 Memory.</p>","tags":["tutorial","DDD","Debugging"]},{"location":"Data_Display_Debugger/#some-useful-resources","title":"Some useful resources","text":"<ul> <li>The official DDD Manual</li> <li>A good debugging tutorial using DDD</li> </ul>","tags":["tutorial","DDD","Debugging"]},{"location":"Data_Machine/","title":"The Data Machine","text":"<p>ICER is excited to offer a new computational resource called the \u201cData Machine\u201d. Many research areas are now faced with large amounts of data thanks in large part to the growth of available datasets. Manipulating, analyzing, and visualizing  large amounts of data requires specialized computational resources that are not typically offered by more traditional high performance computing systems. Additionally, this \u201cdata explosion\u201d is occurring primarily in fields where research computing has historically not been widely used.</p> <p>To meet these hardware and educational needs, ICER is developing the Data Machine and associated outreach and training programs. Though the machine is not yet available to the broader research community (both at MSU and beyond), ICER is looking to connect with researchers who are interested in this new initiative and willing to experiment with the machine and offer feedback. ICER would also like to develop relationships with instructors hoping to integrate data-intensive computation (including demonstrations, assignments, and projects) into their classrooms.</p> <p>Researchers and instructors looking for more information are encouraged to read below and submit short proposals using this form. For the time being, access to the Data Machine is only provided upon request.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#would-you-benefit-from-the-data-machine","title":"Would you benefit from the Data Machine?","text":"<p>The Data Machine is structured to benefit users whose research and/or instruction is described by many or all of the following characteristics:</p> <ul> <li>Datasets that are many GB in size or larger</li> <li>A combination of datasets of varying types and provenances</li> <li>Datasets that require many read &amp; write (I/O) operations, including those composed of many small files</li> <li>Desire for interactive data analysis or workflow development</li> <li>Desire to incorporate data from publicly available repositories or Cloud-based networks</li> <li>Desire to share data products with researchers at other institutions</li> <li>Use of machine learning and artificial intelligence techniques</li> </ul> <p>In particular, the Data Machine is intended to be accessible to researchers (particularly students) with little background in programming or high performance computing.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#example-use-cases","title":"Example Use Cases","text":"<p>Below are some example use cases for the new Data Machine drawn from early users. We hope these examples will illuminate how the Data Machine might benefit your research.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#genomics-of-microbial-communities","title":"Genomics of Microbial Communities","text":"<p>A DNA sequencing machine can produce millions of short raw sequences that need to be combined into larger assemblies and genomes in order to be useful. This requires accessing many small files and holding lots of data in memory. These assemblies are then compared to data available through public repositories and inform ecological and evolutionary modeling.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#agent-based-modeling-of-social-systems","title":"Agent-Based Modeling of Social Systems","text":"<p>Agent-based models (ABMs) simulate thousands of individual actors to uncover large scale behavioral patterns. Each of these agents require information from a large combination of datasets from archival maps to near real-time GPS and social media data. Aggregating these datasets for use in ABMs benefits from interactive development and testing, requiring the data to be available in memory. Production runs of ABMs produce many terabytes of data that similarly benefit from interactive workflow development as well as high-end GPUs to accelerate machine learning and visualization.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#spatial-and-community-ecology","title":"Spatial and Community Ecology","text":"<p>Data from a breadth of biological, ecological, and earth science disciplines can be combined to provide insights on large-scale ecological patterns and their drivers. This requires large-scale statistical analysis of combined datasets. These datasets and analysis tools can then be made available to external collaborators. Additionally, these data can be used to parameterize mathematical models that simulate ecological interactions over long periods of time.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#data-driven-turbulence-modeling","title":"Data-Driven Turbulence Modeling","text":"<p>Simulations capturing the behavior of turbulent flows at both small and large scales are computationally expensive, taking months to run and producing many terabytes of data. Machine learning algorithms can be trained to emulate small scale behavior of turbulence for more efficient modeling of large scale systems. These models must be trained on large quantities of high-fidelity simulation data, requiring high-end GPUs.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#design-of-the-data-machine","title":"Design of the Data Machine","text":"<p>The characteristics outlined above can be translated into hardware constraints, particularly large amounts of memory per core, ample low latency data storage, GPUs, and networks capable of transferring large volumes of data. The Data Machine will have a total of 8 nodes, 4 focused on CPU-intensive jobs and 4 for jobs requiring GPUS.</p> <p>The Data Machine will be connected to the existing HPCC file systems and compute resources. Though the CPU and GPU hardware is similar to what is offered by the current amd21 and amd22 clusters, what sets the Data Machine apart is the amount of memory available to CPU cores, the amount of local data storage available to the node, and the way users will interface with the machine.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#cpu-nodes","title":"CPU Nodes","text":"<p>Each of the 4 CPU-focused nodes will have the following:</p> <ul> <li>128 CPU cores</li> <li>2 TB of memory</li> <li>32 TB of local high speed SSD storage</li> </ul>","tags":["explanation","data machine"]},{"location":"Data_Machine/#gpu-nodes","title":"GPU Nodes","text":"<p>Each of the 4 GPU nodes will have the following:</p> <ul> <li>128 CPU cores</li> <li>512 GB of memory</li> <li>32 TB of local high speed SSD storage</li> <li>4 NVIDIA A100 GPUs with 80 GB of memory per GPU</li> </ul> <p>The A100 GPUs have a combination of tensor processing units and mixed-precision arithmetic units, making them ideal for machine learning and artificial intelligence applications. A job may utilize multiple GPUs per node, or the GPUs may be partitioned for use in interactive data exploration or for course work.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#the-advantage-of-solid-state-drives","title":"The Advantage of Solid State Drives","text":"<p>A unique feature of the Data Machine is that each node (CPU or GPU) is directly connected to 32 TB of solid state drive (SSD) storage. This local storage allows for more efficient read and write (I/O) access than the existing disk-based file systems used by the HPCC.</p> <p>These SSDs are accessed following the NVMe specification, which allows for many possible data access options. Some of these options include direct filesystem access from the GPUs (bypassing the CPU) or using a portion of the SSD as virtual memory to allow access to datasets larger than the memory limits of the node. Users interested in these alternative configurations are encouraged to contact ICER.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#running-on-the-data-machine","title":"Running on the Data Machine","text":"<p>Though the Data Machine is not yet ready for general access, users can anticipate the following workflow features. Please note that the Data Machine will be separate from ICER\u2019s buy-in program and is currently only accessible upon request.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#interactive-jobs","title":"Interactive Jobs","text":"<p>The structure of ICER\u2019s HPCC is oriented towards submitting batch jobs to a system queue, which has been the traditional access pattern for high performance computing (HPC) resources. The Data Machine will instead prioritize interactive usage through OnDemand. Via OnDemand, users will have access to tools such as RStudio, Jupyter notebooks, Matlab, and Stata. Other tools can be added to OnDemand upon request; if this is desired, please fill out a ticket.  When the Data Machine is experiencing high demand, interactive jobs will be able to preempt lower-priority workloads.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#containerization","title":"Containerization","text":"<p>Research groups who can make efficient use of the Data Machine for their research may have complex and specialized software needs. This includes software with large sets of dependencies or which expect a particular runtime environment. The difficulties associated with building and deploying such software can be greatly alleviated by containers. Containers bundle together the user\u2019s software and its minimum set of dependencies into a single executable that will behave consistently on any system.</p> <p>ICER will develop containers for the most common anticipated use cases (e.g. ML/AI with Python, bioinformatics) and make these container images available to users. ICER will also work with users looking to design images for their own research groups, courses, etc. Users interested in developing containers to match their needs should contact ICER.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#data-sharing-and-cloud-integration","title":"Data Sharing and Cloud Integration","text":"<p>The Data Machine will be connected to ICER\u2019s existing infrastructure, including the HPCC file systems and MSU's High-Speed Research Network. The latter will allow researchers to share their datasets and other products with collaborators outside of MSU via Globus.</p> <p>Researchers will also be able to move data into and out of the Data Machine via cloud systems. High speed access to the main cloud providers (AWS, Google Cloud, and Microsoft Azure) is a priority for the Data Machine so that researchers can leverage datasets regardless of where they are stored.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#educational-support","title":"Educational Support","text":"<p>ICER will be offering dedicated training for the Data Machine in addition to its traditional offerings. These trainings will be available both synchronously and asynchronously via workshops, web-based tutorials, and self-paced training modules. All new users will be required to take an orientation workshop prior to being granted access to the Data Machine. This will ensure users have the knowledge and skills to make efficient use of the machine.</p> <p>Users of the Data Machine will be able to access ICER support staff through the existing ticket system and weekly office hours on Microsoft Teams. Researchers requiring additional assistance with, for example, particularly complex research needs or workflow development are encouraged to leverage the ARCS program.</p> <p>Support will also be offered to instructors looking to integrate computational exercises into their courses. Instructors interested in initiating such projects should submit an abstract to this form.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#data-machine-user-advisory-board","title":"Data Machine User Advisory Board","text":"<p>Once the Data Machine becomes widely available, ICER will be looking for graduate students, postdocs, and faculty who use the Data Machine to join the User Advisory Board (UAB). This UAB will meet regularly to discuss the current state of the machine and user experiences, and to make recommendations about software and policy changes. The UAB will also make recommendations as to user training and support for the Data Machine.</p> <p>A typical UAB term is two years, and researchers may serve consecutive terms. Researchers interested in serving on the UAB should contact ICER.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#request-access-to-the-data-machine","title":"Request Access to the Data Machine","text":"<p>Currently, the Data Machine is only available upon request. Users interested in using the Data Machine will need to submit short proposals for research and instructional support using this form. Instructional use of the Data Machine should be requested by the course's lead instructor. Research use should be requested by the Principal Investigator (PI).</p> <p>For both research and instructional use, the requestor must supply the following information: * The NetID of the user(s) they are making the request for * A short abstract describing the research or course work that will be done on the data machine * A brief justification for why the resources of the Data Machine will benefit this work</p>","tags":["explanation","data machine"]},{"location":"Display_Compute_Nodes_and_Job_Partitions_by_sinfo_command/","title":"Display Compute Nodes and Job Partitions by sinfo command","text":"","tags":["reference"]},{"location":"Display_Compute_Nodes_and_Job_Partitions_by_sinfo_command/#information-of-compute-nodes","title":"Information of Compute Nodes","text":"<p>If you would like to run a job with a lot of resources, it is a good idea to check available resources, such as which nodes are available as well as how many cores and how much memory is available on those nodes,  so the job will not wait for too much time. Users can use SLURM command sinfo to get a list of nodes controlled by the job scheduler. Such as, running the command <code>sinfo -N -r -l</code>, where the specifications <code>-N</code> for showing nodes, <code>-r</code> for showing nodes only responsive to SLURM and <code>-l</code> for long description are used.</p> <p>However, for each node, <code>sinfo</code> displays all possible partitions and causes repetitive information. Here, the powertools command <code>node_status</code> can be used to display much better results:</p> <pre><code>$ node_status                       # powertools command\n\nWed Apr 22 11:14:40 EDT 2020\n\nNodeName       Account         State     CPU(Load:Aloc Idl:Tot)    Mem(Aval:Tot)Mb   GPU(I:T)   Reason\n----------------------------------------------------------------------------------------------------------\ncsm-001        general       ALLOCATED      13.61: 20    0: 20       45186: 246640      N/A\ncsm-002       albrecht         MIXED        10.14: 15    5: 20        1072: 246640      N/A\ncsm-003         colej        ALLOCATED       7.45: 20    0: 20       50032: 246640      N/A\n......\ncsn-005        general         MIXED         9.92: 12    8: 20       16160: 118012    k20(0:2)\n......\ncs*      =&gt;   33.3%(buyin)   91.4%(162)     43.6%: 59.5%( 3240)      69.9%(17.0Tb)    97%( 78)   Usage%(Total)\n......\n......\nlac-078        general         MIXED        11.38:  8   20: 28       69884: 118012      N/A\nlac-079          ptg         ALLOCATED      22.37: 28    0: 28       15612: 118012      N/A\nlac-080       merzjrke         MIXED         2.48: 16   12: 28       50032: 246640    k80(0:8)\n......\n......\nvim-002          ccg           MIXED        66.14: 63   81:144     5427008:6145856      N/A\n\nintel14  =&gt;   34.5%(buyin)   91.7%(168)     47.8%: 62.7%( 3576)      60.1%(31.1Tb)    97%( 78)   Usage%(Total)\nintel16  =&gt;   69.0%(buyin)   98.8%(429)     55.2%: 65.1%(12200)      76.6%(79.9Tb)    70%(384)   Usage%(Total)\nintel18  =&gt;   63.6%(buyin)   99.4%(176)     45.8%: 55.8%( 7040)      77.1%(31.3Tb)    55%( 64)   Usage%(Total)\n\nSummary  =&gt;   60.3%(buyin)   97.4%(773)     51.2%: 61.9%(22816)      73.1%( 142Tb)    72%(526)   Usage%(Total\n</code></pre> <p>The result of <code>node_status</code> is a good reference to find out how many nodes available for your  jobs as it displays important information including node names, buyin accounts, node states,  CPU cores, memory, GPU, and the reason the node is unavailable.</p> <p>If you need more complete details of a particular node, you can use <code>scontrol show node -a &lt;node_name&gt;</code> command:</p> <pre><code>$ scontrol show node -a skl-166\nNodeName=skl-166 Arch=x86_64 CoresPerSocket=20\n   CPUAlloc=0 CPUTot=40 CPULoad=0.01\n   AvailableFeatures=skl,gbe,intel18,ib,edr18\n   ActiveFeatures=skl,gbe,intel18,ib,edr18\n   Gres=(null)\n   NodeAddr=skl-166 NodeHostName=skl-166 Version=18.08\n   OS=Linux 3.10.0-693.21.1.el7.x86_64 #1 SMP Wed Mar 7 19:03:37 UTC 2018\n   RealMemory=376162 AllocMem=0 FreeMem=382562 Sockets=2 Boards=1\n   State=DOWN ThreadsPerCore=1 TmpDisk=174080 Weight=103 Owner=N/A MCS_label=N/A\n   Partitions=general-short,general-short-18,general-long,general-long-18,qian-18,nvl-benchmark-18,piermaro-18,vmante-18,liulab-18,devolab-18,tsangm-18,plzbuyin-18,chenlab-18,shadeash-colej-18,allenmc-18,cmse-18,seiswei-18,niederhu-18,daylab-18,junlin-18,mitchmcg-18,pollyhsu-18,davidroy-18,yueqibuyin-18,eisenlohr-18\n   BootTime=2019-02-11T15:07:38 SlurmdStartTime=2019-02-11T15:08:44\n   CfgTRES=cpu=40,mem=376162M,billing=57176\n   AllocTRES=\n   CapWatts=n/a\n   CurrentWatts=0 LowestJoules=0 ConsumedJoules=0\n   ExtSensorsJoules=n/s ExtSensorsWatts=0 ExtSensorsTemp=n/s\n   Reason=Currently being imaged [fordste5@2019-02-11T09:49:30]\n</code></pre>","tags":["reference"]},{"location":"Display_Compute_Nodes_and_Job_Partitions_by_sinfo_command/#slurm-partitions-for-jobs","title":"SLURM Partitions for Jobs","text":"<p>One of the important details about a node is what kind of jobs can run on it. For example, if a node is a buy-in node, only jobs with walltime equal to or less than 4 hours can run for a non-buyin users. We can check the summary of all partitions using <code>sinfo</code> with  the <code>-s</code> specification:</p> <pre><code>$ sinfo -s\nPARTITION           AVAIL  TIMELIMIT   NODES(A/I/O/T)  NODELIST\ngeneral-short          up    4:00:00    729/26/16/771  csm-[001-005,007-010,017-022],csn-[001-039],csp-[006-007,016-020,025-026],css-[001-003,007-012,014,016-020,023,032-036,038-045,047-050,052-067,071-072,074-076,079-085,087-095,097-103,106-109,111-127],lac-[000-225,228-247,250-261,276-369,372,374-445],nvl-[000-007],qml-[000-005],skl-[000-167],vim-[000-002]\ngeneral-long           up 7-00:00:00      269/0/8/277  csm-001,csn-020,csp-[006-007,016-018,020,025],css-[008-012,014,016-019,023,032,034-036,038-045,047-050,052-066,071,075-076,079-080,083,087-089,092-095,097-099,107,118,121,124,126],lac-[038-044,078,123,209,217,225,228,230-235,246-247,276-284,300-301,336-339,353-360,363-364,372,374-399,401-420,422-445],skl-[023,026-112]\ngeneral-long-bigmem    up 7-00:00:00        17/0/0/17  lac-[252-253,306],qml-[000,005],skl-[143-147,162-167],vim-001\ngeneral-long-gpu       up 7-00:00:00       46/12/0/58  csn-[001-019,021-036],lac-[030,087,137,143,192-199,287-290,292-293,342,348],nvl-[005-007]\n</code></pre> <p>where the list of job partitions and their setup for walltime limit and nodes are shown. More detailed information for each job partition can also be found by <code>-p</code> specification:</p> <pre><code>$ sinfo -p general-long -r -l\nMon Jul 13 12:22:16 2020\nPARTITION    AVAIL  TIMELIMIT   JOB_SIZE ROOT OVERSUBS     GROUPS  NODES       STATE NODELIST\ngeneral-long    up 7-00:00:00 1-infinite   no       NO        all      2    draining lac-[231,247]\ngeneral-long    up 7-00:00:00 1-infinite   no       NO        all      1     drained css-053\ngeneral-long    up 7-00:00:00 1-infinite   no       NO        all    217       mixed csm-001,csp-[006,017-018,020,025],css-[010,018-019,023,032,034-035,038,044,047-049,052,055-056,061-066,075,088-089,098-099,107,118,126],lac-[038-044,078,123,209,217,225,228,230,232,234-235,276-280,282-284,300-301,336-337,339,353-360,363,372,374-382,384-399,401-420,423,427-445],skl-[023,026,028-029,031,033-034,036-042,044-046,048,050-067,069-079,081-094,096-106,108-112]\ngeneral-long    up 7-00:00:00 1-infinite   no       NO        all     50   allocated csn-020,csp-016,css-[008-009,011,016-017,036,039-043,045,050,054,057-060,083,087,092-095,097,121,124],lac-[233,246,281,338,364,383,422,424-426],skl-[027,030,032,035,043,047,049,068,080,095,107]\n</code></pre> <p>Users can also show nodes only allowed for specific job partitions by using <code>-N</code> and <code>-p</code>:</p> <pre><code>$ sinfo -N -l -r -p general-short,general-long\nMon Jul 13 12:25:58 2020\nNODELIST   NODES     PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE REASON\ncsm-001        1 general-short       mixed   20   2:10:1 246640   174080    101 gbe,ib,i none\ncsm-001        1  general-long       mixed   20   2:10:1 246640   174080    101 gbe,ib,i none\ncsm-002        1 general-short       mixed   20   2:10:1 246640   174080    101 gbe,ib,i none\ncsm-003        1 general-short       mixed   20   2:10:1 246640   174080    101 gbe,ib,i none\ncsm-004        1 general-short       mixed   20   2:10:1 246640   174080    101 gbe,ib,i none\ncsm-005        1 general-short       mixed   20   2:10:1 246640   174080    101 gbe,ib,i none\n...\n...\nskl-166        1 general-short       mixed   40   2:20:1 376162   174080    103 skl,gbe, none\nskl-167        1 general-short       mixed   40   2:20:1 376162   174080    103 skl,gbe, none\nvim-000        1 general-short       mixed   64   4:16:1 306780   174080    102 gbe,inte none\nvim-001        1 general-short       mixed   64   4:16:1 306780   174080    102 gbe,inte none\nvim-002        1 general-short   allocated  144   8:18:1 614585   174080    102 gbe,inte none\n</code></pre> <p>For a complete instruction of <code>sinfo</code>, please refer to the SLURM web page.</p>","tags":["reference"]},{"location":"Docker/","title":"Docker","text":"<p>What is Docker? Docker is a tool to make it easier to create, deploy and run applications by using containers. Containers allow developers to package up an application with all of the dependencies such as libraries and tools, and deploy it as one package. The application will run on most operating systems (Mac/Windows/Linux) regardless of any customized settings. This page covers how you can run a development environments using Docker containers and package up your own code into a portable container.</p> <p>Warning</p> <p>This tutorial is meant to be run on your personal computer, not the HPCC. Docker does not work on the HPCC since it requires super user (<code>sudo</code>) permissions that users do not have access to. To run containers on the HPCC, you will need to use Singularity.</p> <p>Nevertheless, many of the skills taught in this tutorial transfer over to using Singularity, and most Docker containers can be used without modification on the HPCC through Singularity. However, if you just want to get started running containers on the HPCC, start with the Singularity Introduction.</p>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#docker-installation","title":"Docker installation","text":"<p>Docker can be installed on all major operating systems. However, note that installation on Windows requires the Windows Subsystem For Linux (WSL).</p> <p>For detailed installation instructions depending on operating system, click here:\u00a0Mac/Windows/Linux.</p>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#testing-docker-installation","title":"Testing Docker installation","text":"<p>When you have installed Docker, test your Docker installation by opening a terminal (if you are running Windows, this should be a WSL terminal) and running the following command:</p> <pre><code>$ docker --version\nDocker version 19.03.8, build afacb8b\n</code></pre> <p>When you run the <code>docker</code> command without <code>--version</code>, you will see the options available with docker. Alternatively, you can test your installation by running the following (you have to log into Docker to use this test):</p> <pre><code>$ docker run hello-world\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\n0e03bdcc26d7: Pull complete\nDigest: sha256:6a65f928fb91fcfbc963f7aa6d57c8eeb426ad9a20c7ee045538ef34847f44f1\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n1. The Docker client contacted the Docker daemon.\n2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n(amd64)\n3. The Docker daemon created a new container from that image which runs the\nexecutable that produces the output you are currently reading.\n4. The Docker daemon streamed that output to the Docker client, which sent it\nto your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n$ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\nhttps://hub.docker.com/\n\nFor more examples and ideas, visit:\nhttps://docs.docker.com/get-started/\n</code></pre>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#running-docker-containers-from-prebuilt-images","title":"Running Docker containers from prebuilt images","text":"<p>Now, you have setup everything, and it is time to use Docker seriously. You will run a container from the Alpine Linux image on your system and will learn the\u00a0<code>docker run</code>\u00a0command. However, you should first know what containers and images are, and the difference between containers and images.</p> <p>Images: The file system and configuration of applications which are created and distributed by developers. Of course, you can create and distribute images.</p> <p>Containers: Running instances of Docker images. You can have many containers for the same image.</p> <p>Now that you know what containers and images are, let's get some practice, by running the <code>docker run alpine ls -l</code> command in your terminal.</p> <pre><code>$ docker run alpine ls -l\nUnable to find image 'alpine:latest' locally\nlatest: Pulling from library/alpine\ncbdbe7a5bc2a: Pull complete\nDigest: sha256:9a839e63dad54c3a6d1834e29692c8492d93f90c59c978c1ed79109ea4fb9a54\nStatus: Downloaded newer image for alpine:latest\ntotal 56\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 bin\ndrwxr-xr-x    5 root     root           340 May 26 17:11 dev\ndrwxr-xr-x    1 root     root          4096 May 26 17:11 etc\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 home\ndrwxr-xr-x    5 root     root          4096 Apr 23 06:25 lib\ndrwxr-xr-x    5 root     root          4096 Apr 23 06:25 media\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 mnt\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 opt\ndr-xr-xr-x  187 root     root             0 May 26 17:11 proc\ndrwx------    2 root     root          4096 Apr 23 06:25 root\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 run\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 sbin\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 srv\ndr-xr-xr-x   12 root     root             0 May 26 17:11 sys\ndrwxrwxrwt    2 root     root          4096 Apr 23 06:25 tmp\ndrwxr-xr-x    7 root     root          4096 Apr 23 06:25 usr\ndrwxr-xr-x   12 root     root          4096 Apr 23 06:25 var\n</code></pre> <p>When you run the <code>docker run alpine ls -l</code> command, it searches for the <code>alpine:latest</code> image from your system first. If your system has it (i.e. if you downloaded it previously), Docker uses that image.</p> <p>If your system does not have that image, then Docker\u00a0fetches the <code>alpine:latest</code> image from Docker Hub first, saves it in\u00a0your system, then runs a container from the saved image.\u00a0Docker Hub is a huge repository of images people have uploaded so that others can download and run their code in containers. Though Docker Hub is the most popular place to find Docker images, there are other sources that work just as well (for example, Quay.io).</p> <p><code>docker run alpine</code> starts a container, and then <code>ls -l</code> will be a command which is fed to the container, so Docker starts the given command and results show up.</p> <p>To see a list of all images on your system, you can use the <code>docker images</code> command.</p> <pre><code>$ docker images\nalpine                     latest              f70734b6a266        4 weeks ago         5.61MB\nhello-world                latest              bf756fb1ae65        4 months ago        13.3kB\n</code></pre> <p>Next, let's try another command.</p> <pre><code>$ docker run alpine echo \"Hello world\"\nHello world\n</code></pre> <p>In this case, Docker ran the <code>echo</code> command in your <code>alpine</code> container, and then exited it. Exit means the container is terminated after running the command.</p> <p>Let's try another command.</p> <pre><code>$ docker run alpine sh\n</code></pre> <p>It seems nothing happened. In fact, docker ran the <code>sh</code> command in your alpine container, and exited it. If you want to be inside the container shell, you need to use <code>docker run -it alpine sh</code>. The <code>-i</code> flag tells Docker you want to run the container interactively and the <code>-t</code> flag tells it you want to start a terminal in that image to run your command. You can find more help on the <code>run</code> command\u00a0with <code>docker run --help</code>.</p> <p>Let's run a few commands inside the <code>docker run -it alpine sh</code> container.</p> <pre><code>$ docker run -it alpine sh\n/ # ls\nbin    etc    lib    mnt    proc   run    srv    tmp    var\ndev    home   media  opt    root   sbin   sys    usr\n/ # uname -a\nLinux c1552c9b6cf0 4.19.76-linuxkit #1 SMP Fri Apr 3 15:53:26 UTC 2020 x86_64 Linux\n/ # exit\n</code></pre> <p>You are inside of the container shell and you can try out a few commands like <code>ls</code> and <code>uname -a</code> and others. To quit the container, type <code>exit</code> on the terminal. If you use the <code>exit</code> command, the container is terminated. If you want to keep the container active, then you can use keys <code>Ctrl-p</code> followed by <code>Ctrl-q</code> (you don't have to press these key combinations simultaneously). If you want to go back into the container, you can type <code>docker attach &lt;container_id&gt;</code>, such as <code>docker attach\u00a0c1552c9b6cf0</code>. You can find container id with <code>docker ps -all</code>. This command will be explained next.</p> <p>Now, let's learn about the <code>docker ps</code> command which shows you all containers that are currently running.</p> <pre><code>$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS\n</code></pre> <p>In this case, you don't see any container because no containers are running. To see a list of all containers that you ran, use <code>docker ps --all</code>. You can see that STATUS says that all containers exited. \u00a0</p> <pre><code>$ docker ps --all\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS               NAMES\nc1552c9b6cf0        alpine              \"sh\"                     6 minutes ago       Exited (0) 2 minutes ago                        wonderful_cori\n5de22ab86f2a        alpine              \"echo 'Hello world'\"     18 minutes ago      Exited (0) 18 minutes ago                       goofy_visvesvaraya\ndf35ee7df7e3        alpine              \"ls -l\"                  31 minutes ago      Exited (0) 31 minutes ago                       fervent_gould\n6dbe999044b4        hello-world         \"/hello\"                 3 hours ago         Exited (0) 3 hours ago\n</code></pre> <p>When Docker containers are created, the Docker system automatically assign a universally unique identifier (UUID) number to each container to avoid any naming conflicts. CONTAINER ID is a shortform of the UUID. Each container also has a randomly generated name. You can usually use this name in place of the CONTAINER ID to make typing a bit easier.</p> <p>You can also assign names to your Docker containers when you run them, using the <code>--name</code> flags. In addition, you can rename your Docker container's name with <code>rename</code> command. For example, let's rename \"wonderful_cori\" to \"my_container\" with\u00a0<code>docker rename</code> command.</p> <pre><code>$ docker rename wonderful_cori my_container\n$ docker ps --all\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS               NAMES\nc1552c9b6cf0        alpine              \"sh\"                     10 minutes ago      Exited (0) 6 minutes ago                        my_container\n5de22ab86f2a        alpine              \"echo 'Hello world'\"     22 minutes ago      Exited (0) 22 minutes ago                       goofy_visvesvaraya\ndf35ee7df7e3        alpine              \"ls -l\"                  35 minutes ago      Exited (0) 35 minutes ago                       fervent_gould\n</code></pre>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#build-docker-images-which-contain-your-own-code","title":"Build Docker images which contain your own code","text":"<p>Now you are ready to use Docker to create your own applications! First, you will learn more about Docker images. Then you will build your own image and use that image to run an application on your local machine.</p>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#docker-images","title":"Docker images","text":"<p>Docker images are basis of containers. In the above example, you pulled the alpine image from Docker Hub and ran a container based on that image. To see the list of images that are available on your local machine, run the <code>docker images</code> command.</p> <pre><code>$ docker images\nREPOSITORY    TAG       IMAGE ID       CREATED         SIZE\nalpine        latest    9ed4aefc74f6   4 weeks ago     7.05MB\nhello-world   latest    feb5d9fea6a5   19 months ago   13.3kB\n</code></pre> <p>The TAG refers to a particular snapshot of the image and the ID is the corresponding UUID of the image. Images can have multiple versions. When you do not assign a specific version number, the client defaults to latest. If you want a specific version of the image, you can use docker pull command as follows:</p> <pre><code>$ docker pull centos:7\n7: Pulling from library/centos\n2d473b07cdd5: Pull complete \nDigest: sha256:be65f488b7764ad3638f236b7b515b3678369a5124c47b8d32916d6487418ea4\nStatus: Downloaded newer image for centos:7\ndocker.io/library/centos:7\n</code></pre> <p>Notice that here we pulled the image without running it. When we run a container with the <code>centos:7</code> image in the future, it will use this downloaded copy.</p> <p>You can search for images from a repository's website (for example, searching Docker hub for CentOS) or directly from the command line using <code>docker search</code>.</p> <pre><code>$ docker search centos\nNAME                               DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED\ncentos                             The official build of CentOS.                   6014                [OK]\nansible/centos7-ansible            Ansible on Centos7                              129                                     [OK]\nconsol/centos-xfce-vnc             Centos container with \"headless\" VNC session\u2026   115                                     [OK]\njdeathe/centos-ssh                 OpenSSH / Supervisor / EPEL/IUS/SCL Repos - \u2026   114                                     [OK]\ncentos/mysql-57-centos7            MySQL 5.7 SQL database server                   76\nimagine10255/centos6-lnmp-php56    centos6-lnmp-php56                              58                                      [OK]\ntutum/centos                       Simple CentOS docker image with SSH access      46\ncentos/postgresql-96-centos7       PostgreSQL is an advanced Object-Relational \u2026   44\nkinogmt/centos-ssh                 CentOS with SSH                                 29                                      [OK]\npivotaldata/centos-gpdb-dev        CentOS image for GPDB development. Tag names\u2026   12\nguyton/centos6                     From official centos6 container with full up\u2026   10                                      [OK]\ncentos/tools                       Docker image that has systems administration\u2026   6                                       [OK]\ndrecom/centos-ruby                 centos ruby                                     6                                       [OK]\npivotaldata/centos                 Base centos, freshened up a little with a Do\u2026   4\npivotaldata/centos-mingw           Using the mingw toolchain to cross-compile t\u2026   3\ndarksheer/centos                   Base Centos Image -- Updated hourly             3                                       [OK]\nmamohr/centos-java                 Oracle Java 8 Docker image based on Centos 7    3                                       [OK]\npivotaldata/centos-gcc-toolchain   CentOS with a toolchain, but unaffiliated wi\u2026   3\nmiko2u/centos6                     CentOS6 \u65e5\u672c\u8a9e\u74b0\u5883                                   2                                       [OK]\nblacklabelops/centos               CentOS Base Image! Built and Updates Daily!     1                                       [OK]\nindigo/centos-maven                Vanilla CentOS 7 with Oracle Java Developmen\u2026   1                                       [OK]\nmcnaughton/centos-base             centos base image                               1                                       [OK]\npivotaldata/centos7-dev            CentosOS 7 image for GPDB development           0\nsmartentry/centos                  centos with smartentry                          0                                       [OK]\npivotaldata/centos6.8-dev          CentosOS 6.8 image for GPDB development         0\n</code></pre>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#building-your-first-docker-image","title":"Building your first Docker image","text":"<p>In this section, you will build a simple Docker image with writing a Dockerfile, and run it. For this purpose, we will create a Python script and a Dockerfile.</p>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#creating-working-directory","title":"Creating working directory","text":"<p>Let's create a working directory where you will make the following files: <code>hello.py</code>, <code>Dockerfile</code>.</p> <pre><code>$ cd ~\n$ mkdir my_first_Docker_image\n$ cd my_first_Docker_image\n</code></pre>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#python-script","title":"Python script","text":"<p>Create the <code>hello.py</code> file with the following content.</p> hello.py<pre><code>print(\"Hello world!\")\nprint(\"This is my 1st Docker image!\")\n</code></pre>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#dockerfile","title":"Dockerfile","text":"<p>A Dockerfile is a text file which has a list of commands that Docker calls while creating an image. The Dockerfile is similar to a job batch file, and contains all information that Docker needs to know to to run the application package.</p> <p>In the <code>my_first_Docker_image</code> directory, create a file, called Dockerfile, which has the content below.</p> Dockerfile<pre><code># our base image. The latest version will be pulled.\nFROM alpine\n\n# install python and pip\nRUN apk add --update py3-pip\n\n# copy files required to run\nCOPY hello.py /usr/src/my_app/\n\n# run the application\nCMD python3 /usr/src/my_app/hello.py\n</code></pre> <p>Now, let's learn the meaning of each line.</p> <p>The first line means that we will use Alpine Linux as a base image. No version is specified, so the latest version will be pulled. Use the <code>FROM</code> keyword.</p> <pre><code>FROM alpine\n</code></pre> <p>Next, the Python <code>pip</code> package is installed using the Alpine Package Keeper (<code>apk</code>). Use the <code>RUN</code> keyword.</p> <pre><code>RUN apk add --update py3-pip\n</code></pre> <p>Next, copy the file to the image. <code>/usr/src/my_app</code> will be created while the file is copied. Use the <code>COPY</code> keyword.</p> <pre><code>COPY hello.py /usr/src/my_app/\n</code></pre> <p>The last step is run the application with the <code>CMD</code> keyword.\u00a0<code>CMD</code>\u00a0tells the container what the container should do by default when it is started.</p> <pre><code>CMD python3 /usr/src/my_app/hello.py\n</code></pre>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#build-the-image","title":"Build the image","text":"<p>Now you are ready to build your first Docker image. The <code>docker build</code> command will do most of the work.</p> <p>To build the image, use the following command.</p> <pre><code>$ docker build -t my_first_image .\n</code></pre> <p>The client will pull all necessary images and create your image. If everything goes well, your image is ready to be used! Run the <code>docker images</code> command to see if your image <code>my_first_image</code> is shown.</p>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#run-your-image","title":"Run your image","text":"<p>When you successfully create your Docker image, test it by starting a new container from the image.</p> <pre><code>$ docker run my_first_image\n</code></pre> <p>If everything went well, you will see this message.</p> <pre><code>Hello world!\nThis is my 1st Docker image!\n</code></pre>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#connecting-docker-and-your-computer","title":"Connecting Docker and your computer","text":"<p>Containers are great for keeping all the parts of a piece of software isolated together. But this means that there are a few extra steps necessary to share information from that container with the computer you're running it on.</p>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#sharing-data","title":"Sharing data","text":"<p>Let's pretend that you have a container that runs a long analysis and outputs the results in some data file. We'll mimic this by just creating an empty file with the <code>touch</code> command. Let's make a new directory for our output and create our \"important data\" in our alpine image:</p> <pre><code>$ mkdir results\n$ ls results\n\n$ docker run alpine touch data.dat \n$ ls results\n</code></pre> <p>Nothing happened! You can look in your current directory with <code>ls</code>, and you won't see anything either. We can even run an interactive alpine container and look around:</p> <pre><code>docker run -it alpine sh\n/ # ls\nbin    dev    etc    home   lib    media  mnt    opt    proc   root   run    sbin   srv    sys    tmp    usr    var\n/ # exit\n</code></pre> <p>No results directory and no <code>data.dat</code> file...</p> <p>What happened is that the file was created and locked away in the previous container. When we start a new container, we start fresh from whatever the image specified. Nothing sticks around! So we need a way to get that data out of the container we're working in.</p> <p>The way Docker does this is through \"bind mounts\". It's like we are \"binding\" a directory on our computer to a directory that's \"mounted\" in the container. Let's try it interactively first:</p> <pre><code>$ docker run -it -v ./results:outside_world alpine sh\n/ # ls\nbin            etc            lib            mnt            outside_world  root\nsbin           sys            usr            dev            home           media\nopt            proc           run            srv            tmp            var\n/ # exit\n</code></pre> <p>The <code>-v</code> command tells Docker that I want to connect the <code>./results</code> directory on my computer to a directory called <code>/outside_world</code> inside the container.</p> <p>Now we can put it all together:</p> <pre><code>$ docker run -v ./results:/outside_world alpine touch /outside_world/data.dat\n$ ls results\ndata.dat\n</code></pre> <p>Notice that we had to write to the container version of our directory with the <code>touch</code> command, but it's now visible on the computer in the <code>results</code> directory.</p> <p>This is a contrived example, but in real life, you could replace <code>touch ...</code> with any command that can run in your container, including heavy duty data analysis. You just need to make sure that there is a bind mount between wherever that data is being written inside the container and wherever you want it outside the container (and/or vice versa if you want to input data into your container).</p>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#exposing-ports-and-jupyter-example","title":"Exposing ports (and Jupyter example)","text":"<p>Software that connects to your web browser uses network ports to share information. These ports are just a number that tell your web browser where to access the content shared by the software, and are usually setup by the software (though there are often options a user can set to change the port number).</p> <p>In the context of research computing, one of the most popular examples of this setup is Jupyter Notebook. When a Jupyter Notebook is running, it usually is available on port 8888, meaning you can access it from your web browser with the URL http://127.0.0.1:8888. Here, 127.0.0.1 will always be the IP address of your own computer which in this case is running the Jupyter Notebook on port 8888.</p> <p>Since containers are meant to be an isolated computing environment, network ports are not accessible by default from outside the container. We will now go through an example of exposing ports from a Docker container using Jupyter Hub (a more full-featured version of Jupyter Notebooks). This example will also show an alternate way to include files in a Docker container, albeit, in more of a read-only way.</p> <p>First, let's check the Jupyter images available on Docker Hub. We will use minimal-notebook.</p> <pre><code>$ docker search jupyter\nNAME                                    DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED\njupyter/datascience-notebook            Jupyter Notebook Data Science Stack from htt\u2026   666\njupyter/all-spark-notebook              Jupyter Notebook Python, Scala, R, Spark, Me\u2026   301\njupyterhub/jupyterhub                   JupyterHub: multi-user Jupyter notebook serv\u2026   248                                     [OK]\njupyter/scipy-notebook                  Jupyter Notebook Scientific Python Stack fro\u2026   241\njupyter/tensorflow-notebook             Jupyter Notebook Scientific Python Stack w/ \u2026   218\njupyter/pyspark-notebook                Jupyter Notebook Python, Spark, Mesos Stack \u2026   157\njupyter/base-notebook                   Small base image for Jupyter Notebook stacks\u2026   106\njupyter/minimal-notebook                Minimal Jupyter Notebook Stack from https://\u2026   105\n...\n</code></pre> <p>Let's start by creating a directory <code>my_notebook</code>. Copy <code>hello.py</code>, which we used for the Python image to the <code>my_notebook</code> directory. Then\u00a0create a Dockerfile in the <code>my_notebook</code> directory with the following content:</p> Dockerfile<pre><code># base image\nFROM jupyter/base-notebook\n\n# copy files\nCOPY hello.py /home/jovyan/work\n\n# the port number the container should expose\nEXPOSE 8888\n</code></pre> <p>The <code>COPY</code> line moves our <code>hello.py</code> script into the container directory <code>/home/jovyan/work</code> which is where the Jupyer instance inside the container can access files.</p> <p>The <code>EXPOSE</code> line specifies the port number which needs to be exposed. The default port for Jupyter is 8888, and therefore, we will expose that port.</p> <p>Now, build the image using the following command:</p> <pre><code>$ docker build -t mynotebook .\nSending build context to Docker daemon  3.072kB\nStep 1/3 : FROM jupyter/base-notebook\n ---&gt; 6494235c84ec\nStep 2/3 : COPY hello.py /home/jovyan/work\n ---&gt; 6e22bc10eee0\nStep 3/3 : EXPOSE 8888\n ---&gt; Running in 2d754a40aa2b\nRemoving intermediate container 2d754a40aa2b\n ---&gt; 464731f2e3a7\nSuccessfully built 464731f2e3a7\nSuccessfully tagged mynotebook:latest\n</code></pre> <p>Now, everything is ready. You can run the image using the <code>docker run</code> command. We use the <code>-p 8888:8888</code> option to tell Docker that we'd like to bind the 8888 port in the container to the 8888 port on your host computer.</p> <pre><code>$ docker run -p 8888:8888 mynotebook\nEntered start.sh with args: jupyter lab\nExecuting the command: jupyter lab\n[I 2023-05-02 17:45:26.694 ServerApp] Package jupyterlab took 0.0000s to import\n...\n[I 2023-05-02 17:45:26.941 ServerApp] Serving notebooks from local directory: /home/jovyan\n[I 2023-05-02 17:45:26.941 ServerApp] Jupyter Server 2.5.0 is running at:\n[I 2023-05-02 17:45:26.941 ServerApp] http://ad7f42d45370:8888/lab?token=b76dcd0514e9fe7f60b145e936852ab7836df45e6e4b0879\n[I 2023-05-02 17:45:26.941 ServerApp]     http://127.0.0.1:8888/lab?token=b76dcd0514e9fe7f60b145e936852ab7836df45e6e4b0879\n[I 2023-05-02 17:45:26.941 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 2023-05-02 17:45:26.943 ServerApp] \n\n    To access the server, open this file in a browser:\n        file:///home/jovyan/.local/share/jupyter/runtime/jpserver-7-open.html\n    Or copy and paste one of these URLs:\n        http://ad7f42d45370:8888/lab?token=b76dcd0514e9fe7f60b145e936852ab7836df45e6e4b0879\n        http://127.0.0.1:8888/lab?token=b76dcd0514e9fe7f60b145e936852ab7836df45e6e4b0879\n</code></pre> <p>If you navigate to one of the URLs that Jupyter outputs, you will see your containerized Jupyter Hub ready to go. If you look inside the <code>work</code> directory, you'll even see our <code>hello.py</code> script!</p>","tags":["tutorial","containers","Docker"]},{"location":"EasyBuild_Reference/","title":"EasyBuild Reference","text":"<p>EasyBuild can install software on the HPCC relying on a huge library of on user-contributed recipes called EasyConfigs. This page collects some of the most useful commands and topics for using EasyBuild at ICER. See our tutorial to learn how to use EasyBuild.</p>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#loading-easybuild","title":"Loading EasyBuild","text":"<p>EasyBuild is a module available on the HPCC. Make sure to remove all other modules before using it.</p> <pre><code>module purge\nmodule load EasyBuild\n</code></pre>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#configuration-and-options","title":"Configuration and options","text":"<p>Use the <code>--show-config</code> option to see all of the configuration options and their values.</p> <pre><code>$ ebS --show-config\n</code></pre> <p>Any option can be changed by passing it as a command line argument when using <code>ebS</code>. Key examples are given below.</p>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#software-installation-directory-installpath-software","title":"Software installation directory: <code>--installpath-software</code>","text":"<p>Set to the directory where the final software is installed (default: <code>$HOME/software</code>).</p>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#module-directory-installpath-module","title":"Module directory: <code>--installpath-module</code>","text":"<p>Set to the directory where the software's module files are stored (default: <code>$HOME/module</code>).</p>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#temporary-build-directory-buildpath","title":"Temporary build directory: <code>--buildpath</code>","text":"<p>Set to the directory the software is downloaded and compiled (default: <code>/tmp/$USER/EASYBUILD</code>).</p>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#number-of-cores-allowed-in-parallel-installation-parallel","title":"Number of cores allowed in parallel installation: <code>--parallel</code>","text":"<p>Set to the number of cores you wish to use (default: all cores on node)</p> <p>Warning</p> <p>When installing software on a development node, please use the <code>--parallel</code> option to restrict your usage and ensure that the node stays usable for other users. When in doubt, set <code>--parallel=8</code>.</p>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#easyconfigs","title":"EasyConfigs","text":"<p>EasyConfigs are the recipes used to install software. There is a large collection (focusing on research software) already available on the HPCC contributed by many users and software developers across the world.</p>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#searching-with-search-or-m","title":"Searching with <code>--search</code> or <code>-M</code>","text":"<p>Use the <code>--search</code> or <code>-S</code> option to <code>ebS</code> to find an EasyConfig available on the HPCC.</p> <p>Example:</p> <pre><code>$ ebS -S zfp\n</code></pre>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#viewing-with-show-ec","title":"Viewing with <code>--show-ec</code>","text":"<p>Use the <code>--show-ec</code> option to view an EasyConfig based on its name (not its full path).</p> <p>Example:</p> <pre><code>$ ebS --show-ec zfp-1.0.0-GCCcore-10.3.0.eb\n</code></pre>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#installing-software","title":"Installing software","text":"<p>Run <code>ebS</code> with any options you wish to set followed by the name of the EasyConfig you wish to install.</p> <p>Example:</p> <pre><code>$ ebS --parallel=8 --installpath-software=$SCRATCH/software zfp-1.0.0-GCCcore-10.3.0.eb\n</code></pre>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#accessing-installed-software-with-the-module-system","title":"Accessing installed software with the <code>module</code> system","text":"<p>Add the location where your module files are installed (i.e., the value of <code>--installpath-modules</code> in your EasyBuild configuration to your <code>$MODULEPATH</code> with <code>module use</code> so modules can be found by further <code>module</code> commands.</p> <p>Example:</p> <pre><code>$ module use $HOME/modules\n$ module spider zfp\n----------------------------------------------------------------------------\n  zfp: zfp/1.0.0-GCCcore-10.3.0\n----------------------------------------------------------------------------\n...\n    This module can be loaded directly: module load zfp/1.0.0-GCCcore-10.3.0\n...\n$ module load zfp/1.0.0-GCCcore-10.3.0\n</code></pre>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#the-ebs-alias-and-default-options","title":"The <code>ebS</code> alias and default options","text":"<p>When you load the EasyBuild module, you have access to the <code>eb</code> command which is the traditional way to interface with EasyBuild. The ICER staff has also supplied the <code>ebS</code> alias to take the place of <code>eb</code>. It sets some reasonable defaults for you including</p> <ul> <li>setting your EasyBuild <code>installpath</code> to <code>$HOME</code><ul> <li>software is installed in <code>$HOME/software</code></li> <li>modules are installed in <code>$HOME/modules</code></li> </ul> </li> <li>automatically purging modules before installing (to ensure new software works as desired)</li> <li>automatically installing dependencies using the <code>--robot</code> option</li> <li>rebuilding the desired software if it's already installed</li> </ul> <p>If you read the EasyBuild documentation or any other sources, you are free to use <code>ebS</code> in place of <code>eb</code>.</p>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Tutorial/","title":"EasyBuild Tutorial","text":"<p>One of the most complex parts of using an HPCC can often be installing the software you want to use. EasyBuild is a piece of software that helps simplify the process. It compiles well-tested recipes contributed by people installing this software (often on HPCCs just like MSU's) all over the world. It's also what ICER uses to install all of the modules you use on the HPCC!</p> <p>A warning: while \"easy\" is in the name, don't ever expect installing software on the HPCC to be easy... But EasyBuild is probably the closest you'll get.</p> <p>What kind of software do you need?</p> <p>EasyBuild can help you install all kinds of software, but there are other options that we recommend for things like Python and R. For Python, we recommend Anaconda, and for R we recommend using the built-in <code>install.packages</code> command. EasyBuild will be most helpful if you need to compile a piece of software from scratch that somebody else has created a recipe for.</p> <p>In this tutorial, we are going to try to install a piece of software that's not already on the HPCC called <code>zfp</code>.</p>","tags":["tutorial","EasyBuild"]},{"location":"EasyBuild_Tutorial/#loading-easybuild","title":"Loading EasyBuild","text":"<p>To get started, we load the EasyBuild module:</p> <pre><code>module purge\nmodule load EasyBuild\n</code></pre> <p>We now have access to the <code>eb</code> command that does everything you need in EasyBuild as the alias <code>ebS</code> which takes the place of the <code>eb</code> command (with some nice defaults setup).</p>","tags":["tutorial","EasyBuild"]},{"location":"EasyBuild_Tutorial/#configuring-easybuild","title":"Configuring EasyBuild","text":"<p>We can first check our global EasyBuild configuration using</p> <p>input<pre><code>ebS --show-config\n</code></pre> output<pre><code>#\n#\n# Current EasyBuild configuration\n# (C: command line argument, D: default value, E: environment variable, F: configuration file)\n#\nbuildpath                 (E) = /tmp/k0068027/EASYBUILD\ncontainerpath             (D) = /mnt/home/k0068027/.local/easybuild/containers\ncuda-compute-capabilities (C) = 3.5, 3.7, 7.0\ndetect-loaded-modules     (C) = purge\ngroup-writable-installdir (C) = True\ninstallpath               (E) = /mnt/home/k0068027\ninstallpath-modules       (E) = /mnt/home/k0068027/modules\ninstallpath-software      (E) = /mnt/home/k0068027/software\njob-backend               (C) = Slurm\nmodule-depends-on         (C) = True\noptarch                   (E) = GENERIC\nrebuild                   (C) = True\nrepositorypath            (D) = /mnt/home/k0068027/.local/easybuild/ebfiles_repo\nrobot                     (C) = /mnt/research/helpdesk/EB_Files_4, /opt/software/EasyBuild/4.7.1/easybuild/easyconfigs, /mnt/research/helpdesk/ebfiles\nrobot-paths               (E) = /mnt/research/helpdesk/EB_Files_4, /opt/software/EasyBuild/4.7.1/easybuild/easyconfigs, /mnt/research/helpdesk/ebfiles\nsourcepath                (D) = /mnt/home/k0068027/.local/easybuild/sources\nsuffix-modules-path       (C) = ''\n</code></pre></p> <p>Whoa! There are a lot of options here! Some of them have been setup by HPCC staff to make your life easier (compare with the output of <code>eb --show-config</code>), and you shouldn't worry about most of them. However, let's highlight the important ones:</p> <code>installpath</code> <p>This is the root directory for your software installation. In this case, it's my (<code>k0068027</code>) home directory. Usually the software and modules both fall under this directory, but can be set separately (see the next two options).</p> <code>installpath-software</code> <p>This is where all of your software is actually installed. When your installation is finished, you should be able to find it under its name in this directory.</p> <code>installpath-modules</code> <p>This is where the module files are stored for your software installations. What's a module file? It's how <code>module load mysoftware</code> works! So after you install something with EasyBuild, you'll have built your own personal module that you can load like anything else on the HPCC!</p> <code>buildpath</code> <p>This is where the software is compiled. Usually, keeping it as a <code>/tmp</code> directory is good since it's fast storage on the node for lots of small reading and writing. Once it's built, it gets moved to your <code>installpath-software</code> directory anyways, so it really is temporary.</p> <p>These are usually good defaults, but you might want to change them. For example, what if you need to install a piece of software in a research space so everyone in your group can access it? Or what if your home directory is filling up, you only need the software temporarily, and are okay installing it into your scratch space?</p> <p>For the sake of this tutorial, we'll practice by installing the software into our scratch space, but leaving the module files in our home directory.</p> <p>To change the configuration, we can do it by passing the new value as a command line argument to any <code>ebS</code> command:</p> <p>input<pre><code>ebS --installpath-software=$SCRATCH/software --show-config\n</code></pre> output<pre><code>#\n#\n# Current EasyBuild configuration\n# (C: command line argument, D: default value, E: environment variable, F: configuration file)\n#\nbuildpath                 (E) = /tmp/k0068027/EASYBUILD\n...\ninstallpath               (E) = /mnt/home/k0068027\ninstallpath-modules       (E) = /mnt/home/k0068027/modules\ninstallpath-software      (C) = /mnt/gs21/scratch/k0068027/software\n...\n</code></pre></p> <p>Great! It also tells us that this option was set using a command line argument by the <code>(C)</code>. We'll have to make sure to include this option when we actually try to install the software: it's not \"set and forget\"!</p> <p>As you can see from the output, there are multiple ways to configure EasyBuild. If you want to make more lasting changes that are \"set and forget\", try setting up a configuration file.</p>","tags":["tutorial","EasyBuild"]},{"location":"EasyBuild_Tutorial/#finding-our-easyconfig","title":"Finding our EasyConfig","text":"<p>So now that we're happy with and (mostly) understand our <code>ebS --show-config</code> results, we can try finding the recipe for the software we'd like to install. These recipes are called EasyConfigs and there's a good chance that someone has already created one for the software you're trying to install.</p> <p>The list of EasyConfigs is stored on the HPCC and we can search through it using the <code>-S</code> option of <code>ebS</code>:</p> <p>input<pre><code>ebS -S zfp\n</code></pre> output<pre><code>== found valid index for /opt/software/EasyBuild/4.7.1/easybuild/easyconfigs, so using it...\nCFGS1=/opt/software/EasyBuild/4.7.1/easybuild/easyconfigs/z/zfp\n * $CFGS1/zfp-0.5.5-GCCcore-10.2.0.eb\n * $CFGS1/zfp-1.0.0-GCCcore-9.3.0.eb\n * $CFGS1/zfp-1.0.0-GCCcore-10.3.0.eb\n</code></pre></p> <p>This tells us that there a few different EasyConfigs available to help us install different versions of <code>zfp</code> under different toolchains.</p> What is a toolchain? <p>A toolchain is a set of software dependencies used to install new software. Most often, this is a compiler like GCC or a compiler/MPI pair like GCC and OpenMPI. The most basic toolchains are just single compilers and are labeled using their software version (like <code>GCCcore-10.2.0</code>).</p> <p>EasyBuild organizes installed modules by toolchain. For example, if you look for the R/4.2.2 module file, it's under <code>/opt/modules/MPI/GCC/11.2.0/OpenMPI/4.1.1/R/4.2.2.lua</code> because it was built using a GCC/OpenMPI toolchain.</p> <p>Some of these are so commonly used that EasyBuild groups dependency software into larger toolchains like foss and intel that contain a compiler/MPI pair and a number of other common dependencies. These are labeled by their year and an <code>a</code> or <code>b</code> for the first or second half of the year. You can check what's in them by searching for their EasyConfig and showing it with <code>eb --show-ec</code>:</p> <p>input<pre><code>ebF foss\n</code></pre> output<pre><code>...\n====== $ebF_PATH/f/foss/\nfoss-2016.04.eb  foss-2016b.eb    foss-2018b.eb  foss-2021a.eb    foss-2022b.eb\nfoss-2016.06.eb  foss-2017a.eb    foss-2019a.eb  foss-2021b.eb\nfoss-2016.07.eb  foss-2017b.eb    foss-2019b.eb  foss-2022.05.eb\nfoss-2016.09.eb  foss-2018.08.eb  foss-2020a.eb  foss-2022.10.eb\nfoss-2016a.eb    foss-2018a.eb    foss-2020b.eb  foss-2022a.eb\n...\n</code></pre> input<pre><code>eb --show-ec foss-2022a.eb\n</code></pre> output<pre><code>easyblock = 'Toolchain'\n\nname = 'foss'\nversion = '2022a'\n\nhomepage = 'https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain'\ndescription = \"\"\"GNU Compiler Collection (GCC) based compiler toolchain, including\n OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK.\"\"\"\n\ntoolchain = SYSTEM\n\nlocal_gccver = '11.3.0'\n\n# toolchain used to build foss dependencies\nlocal_comp_mpi_tc = ('gompi', version)\n\n# we need GCC and OpenMPI as explicit dependencies instead of gompi toolchain\n# because of toolchain preparation functions\ndependencies = [\n    ('GCC', local_gccver),\n    ('OpenMPI', '4.1.4', '', ('GCC', local_gccver)),\n    ('FlexiBLAS', '3.2.0', '', ('GCC', local_gccver)),\n    ('FFTW', '3.3.10', '', ('GCC', local_gccver)),\n    ('FFTW.MPI', '3.3.10', '', local_comp_mpi_tc),\n    ('ScaLAPACK', '2.2.0', '-fb', local_comp_mpi_tc),\n]\n\nmoduleclass = 'toolchain'\n</code></pre></p> <p>We can see that <code>foss</code> includes <code>GCC</code>, <code>OpenMPI</code>, <code>FlexiBLAS</code>, <code>FFTW</code>, <code>FFTW.MPI</code>, and <code>ScaLAPACK</code>.</p> <p>We'll try to install the newest version of <code>zfp</code> with the newest compiler there's a corresponding EasyConfig for: <code>zfp-1.0.0-GCCcore-10.3.0.eb</code>.</p>","tags":["tutorial","EasyBuild"]},{"location":"EasyBuild_Tutorial/#checking-dependencies","title":"Checking dependencies","text":"<p>One of the great part of EasyBuild is that it will handle the dependencies of the software you're using for you. If they're not already installed, it will use other EasyConfigs to install them.</p> <p>We can check to see if we're missing any of <code>zfp</code>'s dependencies on the system using <code>ebS -M</code>:</p> <p>input<pre><code>ebS -M zfp-1.0.0-GCCcore-10.3.0.eb\n</code></pre> output<pre><code>...\n7 out of 16 required modules missing:\n\n* help2man/1.48.3-GCCcore-10.3.0 (help2man-1.48.3-GCCcore-10.3.0.eb)\n* M4/1.4.18-GCCcore-10.3.0 (M4-1.4.18-GCCcore-10.3.0.eb)\n* zlib/1.2.11-GCCcore-10.3.0 (zlib-1.2.11-GCCcore-10.3.0.eb)\n* Bison/3.7.6-GCCcore-10.3.0 (Bison-3.7.6-GCCcore-10.3.0.eb)\n* flex/2.6.4-GCCcore-10.3.0 (flex-2.6.4-GCCcore-10.3.0.eb)\n* binutils/2.36.1-GCCcore-10.3.0 (binutils-2.36.1-GCCcore-10.3.0.eb)\n* zfp/1.0.0-GCCcore-10.3.0 (zfp-1.0.0-GCCcore-10.3.0.eb)\n...\n</code></pre></p> <p>So we're missing seven dependencies; the rest are already available on the HPCC. EasyBuild will install those for us, so long as we use the <code>--robot</code> option when installing <code>zfp</code>. This is option included in the <code>ebS</code> alias by default, so you won't need to worry about it.</p> <p>\"It's already on the HPCC, why is it missing!\"</p> <p>Unfortunately, since some HPCC modules are behind other \"gateway\" modules (e.g., to load <code>R</code>, you first have to load <code>GCC</code> and <code>OpenMPI</code>), they are unavailable as dependencies of user installs. If the software you try to install needs something behind one of these gateways as a dependency, EasyBuild will install another copy for you.</p>","tags":["tutorial","EasyBuild"]},{"location":"EasyBuild_Tutorial/#installing","title":"Installing","text":"<p>Now we're ready to install. We just use the <code>ebS</code> alias with our EasyConfig, and hope things go well!</p> <p>We can run shorter installs like this on a development node. By default, EasyBuild will try to parallelize compilation using all of the cores on the machine. To be a good dev node neighbor, we can use the <code>--parallel</code> option to only use a few of the cores, and leave the rest of the machine useable for everyone. For longer builds, you might consider running EasyBuild through a batch or interactive job.</p> <p>And don't forget to change your software install path!</p> <p>input<pre><code>ebS --parallel=8 --installpath-software=$SCRATCH/software zfp-1.0.0-GCCcore-10.3.0.eb\n</code></pre> output<pre><code>...\n== processing EasyBuild easyconfig /mnt/ufs18/home-237/k0068027/zfp-1.0.0-GCCcore-10.3.0.eb\n== building and installing zfp/1.0.0-GCCcore-10.3.0...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== ... (took 1 secs)\n== configuring...\n== building...\n== ... (took 5 secs)\n== testing...\n== installing...\n== taking care of extensions...\n== restore after iterating...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== permissions...\n== packaging...\n== COMPLETED: Installation ended successfully (took 8 secs)\n== Results of the build can be found in the log file(s) /mnt/gs21/scratch/k0068027/software/zfp/1.0.0-GCCcore-10.3.0/easybuild/easybuild-zfp-1.0.0-20230713.150451.log\n...\n</code></pre></p> <p>This process should take about five minutes in total.</p> <p>There's a lot of output here, but we can see that it completed steps like configuring, building, testing, installing, and checking that important files are where they're supposed to be. And not only did it do this for <code>zfp</code>, but for all of the dependencies too. You would usually have to do this all manually!</p> <p>We can check that the software is where it's supposed to be.</p> <p>input<pre><code>ls $SCRATCH/software/zfp/1.0.0-GCCcore-10.3.0\n</code></pre> output<pre><code>bin  easybuild  include  lib  lib64\n</code></pre> input<pre><code>ls $SCRATCH/software/zfp/1.0.0-GCCcore-10.3.0/bin\n</code></pre> output<pre><code>testzfp  zfp\n</code></pre></p> <p>Note that the installation is stored under the <code>zfp</code> directory in a directory labeled with the software and toolchain versions. If, in the future, we wanted a new version or one built with a different compiler, these two versions can coexist in different directories.</p>","tags":["tutorial","EasyBuild"]},{"location":"EasyBuild_Tutorial/#using-the-software","title":"Using the software","text":"<p>Now that it's installed, we can try to test it with the <code>testzfp</code> executable. We know exactly where it is, so let's try and run it from it's installation directory:</p> <p>input<pre><code>$SCRATCH/software/zfp/1.0.0-GCCcore-10.3.0/bin/testzfp\n</code></pre> output<pre><code>/mnt/gs21/scratch/k0068027/software/zfp/1.0.0-GCCcore-10.3.0/bin/testzfp: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.26' not found (required by /mnt/gs21/scratch/k0068027/software/zfp/1.0.0-GCCcore-10.3.0/bin/testzfp)\n/mnt/gs21/scratch/k0068027/software/zfp/1.0.0-GCCcore-10.3.0/bin/testzfp: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /mnt/gs21/scratch/k0068027/software/zfp/1.0.0-GCCcore-10.3.0/bin/testzfp)\n</code></pre></p> <p>Well that doesn't look right... It looks there are some missing libraries. The real issue is that we didn't load the module for the new software we built! That would have also loaded all the dependencies for us!</p> <p>Remember that modules are installed into <code>$HOME/modules</code>. We can add these to our \"module path\" so that they show up when we try to do <code>module load</code>:</p> <p>input<pre><code>module use $HOME/modules\necho $MODULEPATH\n</code></pre> output<pre><code>/mnt/home/k0068027/modules:/opt/software/hpcc/modules:/opt/modules/Core\n</code></pre></p> <p>Let's try searching for it:</p> <p>input<pre><code>module spider zfp\n</code></pre> output<pre><code>----------------------------------------------------------------------------\n  zfp: zfp/1.0.0-GCCcore-10.3.0\n----------------------------------------------------------------------------\n    Description:\n      zfp is a compressed format for representing multidimensional\n      floating-point and integer arrays. zfp provides compressed-array\n      classes that support high throughput read and write random access to\n      individual array elements. zfp also supports serial and parallel\n      (OpenMP and CUDA) compression of whole arrays, e.g., for applications\n      that read and write large data sets to and from disk.\n\n\n     Other possible modules matches:\n        lib/zfp\n\n    This module can be loaded directly: module load zfp/1.0.0-GCCcore-10.3.0\n...\n</code></pre></p> <p>Notice that the version is followed by the toolchain it depends on. Now we can load it using the command <code>module spider</code> gave.</p> <p>input<pre><code>module load zfp/1.0.0-GCCcore-10.3.0\nmodule list\n</code></pre> output<pre><code>Currently Loaded Modules:\n  1) EasyBuild/4.6.2   2) GCCcore/10.3.0   3) zfp/1.0.0-GCCcore-10.3.0\n</code></pre></p> <p>This does a few things including </p> <ul> <li>adding <code>zfp</code>'s <code>bin</code> directory to our path,</li> <li>adding <code>zfp</code>'s <code>lib</code> directory to our <code>LD_LIBRARY_PATH</code> (so we can compile new programs using the libraries it provides in the future),</li> <li>and loading the modules for its runtime dependencies.</li> </ul> <p>This means we can run <code>testzfp</code> without using it's absolute path:</p> <p>input<pre><code>testzfp\n</code></pre> output<pre><code>zfp version 1.0.0 (August 1, 2022)\nlibrary version 4096\nCODEC version 5\ndata model LP64\n\ntesting 1D array of floats\n  compress:   rate= 2                                                 OK \n  decompress: rate= 2 1.626e+01 &lt;= 1.627e+01                          OK \n...\nall tests passed\n</code></pre></p> <p>Though this was a small example, this workflow should get you through most EasyBuild installations. Checkout our EasyBuild reference in the future if you need a quick refresher of the most important commands.</p> <p>(Oh, and you can delete the <code>$HOME/modules</code> and <code>$SCRATCH/software</code> directories to start fresh for your real installations.)</p>","tags":["tutorial","EasyBuild"]},{"location":"Editing_Text_with_Nano/","title":"Editing Text with <code>nano</code>","text":"<p>Life on the HPCC is filled with text files: batch scripts, configuration files, source code... Sooner or later you will have to create or edit one!</p> <p>While there are options like VS Code that are closer to the types of text editors used on a normal desktop computer, they can be difficult to setup on the HPCC, and if you're just moving around using the command line, you might want to quickly drop in and edit a file without having to switch programs.</p> <p>We recommend that new users try <code>nano</code> to edit files on the command line. While there are other text editors you can use directly from the shell (e.g., <code>vim</code> or <code>emacs</code>), <code>nano</code> is much more user-friendly, and easier to start with.</p>","tags":["how-to guide","nano","text editors"]},{"location":"Editing_Text_with_Nano/#opening-a-file","title":"Opening a file","text":"<p>To open a file using <code>nano</code>, type</p> <pre><code>nano &lt;filename&gt;\n</code></pre> <p>into the shell, replacing <code>&lt;filename&gt;</code> with the file you'd like to edit. You can either give the full path to the file like</p> <pre><code>nano ~/my/important/text/file.txt\n</code></pre> <p>or if you're already in the directory where your file is, you can just use the filename, like</p> <pre><code>cd ~/my/important/text\nnano file.txt\n</code></pre> <p>If the file doesn't exist, <code>nano</code> will create it for you.</p>","tags":["how-to guide","nano","text editors"]},{"location":"Editing_Text_with_Nano/#editing-a-file","title":"Editing a file","text":"<p>Once the file is open, you can use the arrow keys, page up/down, and home/end to move around. You can type anywhere to add text.</p>","tags":["how-to guide","nano","text editors"]},{"location":"Editing_Text_with_Nano/#copying-and-pasting","title":"Copying and pasting","text":"<p>To copy, first use the mouse to highlight text. Right click, and choose Copy from the menu.</p> <p>To paste, move the cursor to the position you want to paste using the arrow keys. Right click, and choose Paste from the menu.</p> <p>The keyboard shortcuts to copy and paste depend on your operating system and terminal program:</p> WindowsMacLinux <p>In MobaXterm:</p> Command Shortcut Copy Text selected with the mouse will be copied by default Paste <code>shift + insert</code> Command Shortcut Copy <code>cmd + c</code> Paste <code>cmd + v</code> <p>This depends on your terminal, but usually the following work:</p> Command Shortcut Copy <code>ctrl + shift + c</code> Paste <code>ctrl + shift + v</code> <p>If you need to scroll through a file to select all of the text you need to copy, the shortcuts in <code>nano</code> become more complicated. A simpler way to do this is to exit <code>nano</code>, and enter</p> <pre><code>cat &lt;filename&gt;\n</code></pre> <p>on the command line.</p> <p>This will output all contents of the filename you give to your command line, and you can select the part you want to copy with the mouse. The copy and paste shortcuts above will work in the same way.</p>","tags":["how-to guide","nano","text editors"]},{"location":"Editing_Text_with_Nano/#shortcuts","title":"Shortcuts","text":"<p>At the bottom of the screen, you will see some helpful key combinations:</p> <pre><code>^G Get Help  ^O WriteOut  ^R Read File ^Y Prev Page ^K Cut Text  ^C Cur Pos\n^X Exit      ^J Justify   ^W Where Is  ^V Next Page ^U UnCut Text^T To Spell\n</code></pre> <p>Each of these combinations starts with <code>^</code> symbolizing the <code>ctrl</code> key on your keyboard. For example, to exit, hold <code>ctrl</code> on your keyboard and press <code>x</code>. Even though <code>nano</code> shows an upper case <code>X</code>, you don't need to use the <code>shift</code> key.</p> <p>The shortcut descriptions are meant to be mnemonic and match the key used, which can lead to some confusion. Some helpful shortcuts in plain language include:</p> Task Shortcut Get help <code>^G</code> Exit <code>nano</code> <code>^X</code> Save a file <code>^O</code> Search for text <code>^W</code> Delete a line <code>^K</code> Paste line that was deleted <code>^T</code> Spell check <code>^T</code>","tags":["how-to guide","nano","text editors"]},{"location":"Editing_Text_with_Nano/#saving-a-file","title":"Saving a file","text":"<p>To save a file, use <code>^O</code> (that is, hold the <code>ctrl</code> key and press <code>o</code>). You will be asked what filename to save your file to, with the default being the already existing filename. Press the <code>enter</code> or <code>return</code> key without making any changes to the filename to save.</p> <p>If you'd like to make a copy or save your edits in a new place, change the filename before accepting.</p>","tags":["how-to guide","nano","text editors"]},{"location":"Editing_Text_with_Nano/#exiting-nano","title":"Exiting <code>nano</code>","text":"<p>To exit, use <code>^X</code> (that is, hold the <code>ctrl</code> key and press <code>x</code>). If you exit before having saved the file, <code>nano</code> will ask if you would like to save before exiting. Press the <code>y</code> key to save changes, the <code>n</code> to discard your changes, or <code>^C</code> to cancel.</p>","tags":["how-to guide","nano","text editors"]},{"location":"Example_SLURM_scripts/","title":"Example SLURM scripts","text":"<p>This page contains example job scripts for several kinds of jobs that you can modify for your own needs:</p> <ul> <li>Single core on a single node</li> <li>Single node, single core job with a GPU</li> <li>Multiple threads and cores on a single node</li> <li>Multiple nodes (e.g., an MPI program)</li> <li>Multiple threads and cores across multiple nodes (e.g., a hybrid MPI-OpenMP program)</li> </ul> <p>See the tutorial on writing and submitting job scripts for more guidance on using job scripts. </p> <p>More information on SLURM job specifications is also available.</p> <p>Users looking for example SLURM scripts for Singularity jobs should see this page.</p> <p>Different kinds of executables (compiled C/C++ code, Python and R scripts) are demonstrated across these examples for variety. How an executable should be run and the resources required depends on the details of that program.</p>","tags":["reference","slurm","job script"]},{"location":"Example_SLURM_scripts/#single-node-single-core","title":"Single node, single core","text":"<p>This example demonstrates a simple R script job but is suited to any serial job. The <code>Rscript</code> command does not necessarily have to be wrapped with the <code>srun</code> command because this job uses no parallelism.</p> basic_job.sb<pre><code>#!/bin/bash --login\n# Job name:\n#SBATCH --job-name=Rscript\n\n# Number of tasks (processes)\n# SLURM defaults to 1 but we specify anyway\n#SBATCH --ntasks=1\n\n# Memory per node\n# Specify \"M\" or \"G\" for MB and GB respectively\n#SBATCH --mem=20M\n\n# Wall time\n# Format: \"minutes\", \"hours:minutes:seconds\", \n# \"days-hours\", or \"days-hours:minutes\"\n#SBATCH --time=01:00:00\n\n# Mail type\n# e.g., which events trigger email notifications\n#SBATCH --mail-type=ALL\n\n# Mail address\n#SBATCH --mail-user=yournetid@msu.edu\n\n# Standard output and error to file\n# %x: job name, %j: job ID\n#SBATCH --output=%x-%j.SLURMout\n\n# Purge current modules and load those we require\nmodule purge\nmodule load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 powertools\n\n# Run our job\ncd /mnt/home/user123\nsrun Rscript myscript.R\n\n# Print resource information\nscontrol show job $SLURM_JOB_ID\njs -j $SLURM_JOB_ID\n</code></pre>","tags":["reference","slurm","job script"]},{"location":"Example_SLURM_scripts/#single-node-single-core-with-gpu","title":"Single node, single core with GPU","text":"<p>Jobs that use GPUs must request these resources within their SLURM script. SLURM will automatically allocate the job to a node with the appropriate GPU.  Multiple GPUs may be available per node.</p> <p>Note</p> <p>This example requests only a single GPU node.  Users looking to use multiple nodes, each with their own GPU(s), should replace the <code>--gpus</code> option with <code>--gpus-per-node</code>. See the list of SLURM specifications for more.</p> <p>The fake Python script in this example would use PyTorch to define and train a neural network. The script loads our conda environment following the recommended setup in the conda usage guide.</p> gpu_job.sb<pre><code>#!/bin/bash --login\n# Job name:\n#SBATCH --job-name=pytorch\n\n# Number of processes.\n# Unless programmed using MPI,\n# most programs using GPU-offloading only need\n# a single CPU-based process to manage the device(s)\n#SBATCH --ntasks=1\n\n# Type and number of GPUs\n# The type is optional.\n#SBATCH --gpus=v100:4\n\n# Total CPU memory\n# All available memory per GPU is allocated by default.\n# Specify \"M\" or \"G\" for MB and GB respectively\n#SBATCH --mem=2G\n\n# Wall time\n# Format: \"minutes\", \"hours:minutes:seconds\", \n# \"days-hours\", or \"days-hours:minutes\"\n#SBATCH --time=01:00:00\n\n# Mail type\n# e.g., which events trigger email notifications\n#SBATCH --mail-type=ALL\n\n# Mail address\n#SBATCH --mail-user=yournetid@msu.edu\n\n# Standard output and error to file\n# %x: job name, %j: job ID\n#SBATCH --output=%x-%j.SLURMout\n\nmodule load Conda/3\nconda activate myenv\n\n# Run our job\ncd /mnt/home/user123\nsrun python train_neural_network.py\n\n# Print resource information\nscontrol show job $SLURM_JOB_ID\njs -j $SLURM_JOB_ID\n</code></pre>","tags":["reference","slurm","job script"]},{"location":"Example_SLURM_scripts/#single-node-multiple-cores","title":"Single node, multiple cores","text":"<p>There are two ways a job could use multiple CPU cores (also known as processors) on a single node: each CPU could work independently, or they could work collaboratively on tasks (also called processes). The latter style is appropriate for jobs written with OpenMP. Examples of each are shown below.</p>","tags":["reference","slurm","job script"]},{"location":"Example_SLURM_scripts/#independent-cpus","title":"Independent CPUs","text":"<p>The fake Python script in this example would use a pool of processes independently completing tasks. The script loads our conda environment following the recommended setup in the conda usage guide.</p> pleasantly_parallel.sb<pre><code>#!/bin/bash --login\n# Job name:\n#SBATCH --job-name=python-pool\n\n# Number of nodes\n#SBATCH --nodes=1\n\n# Number of tasks to run on each node\n#SBATCH --ntasks-per-node=16\n\n# Memory per CPU\n# Specify \"M\" or \"G\" for MB and GB respectively\n#SBATCH --mem-per-cpu=20M\n\n# Wall time\n# Format: \"minutes\", \"hours:minutes:seconds\", \n# \"days-hours\", or \"days-hours:minutes\"\n#SBATCH --time=01:00:00\n\n# Mail type\n# e.g., which events trigger email notifications\n#SBATCH --mail-type=ALL\n\n# Mail address\n#SBATCH --mail-user=yournetid@msu.edu\n\n# Standard output and error to file\n# %x: job name, %j: job ID\n#SBATCH --output=%x-%j.SLURMout\n\nmodule load Conda/3\nconda activate myenv\n\n# Run our job\ncd /mnt/home/user123\nsrun python my_processor_pool.py\n\n# Print resource information\nscontrol show job $SLURM_JOB_ID\njs -j $SLURM_JOB_ID\n</code></pre>","tags":["reference","slurm","job script"]},{"location":"Example_SLURM_scripts/#collaborative-cpus","title":"Collaborative CPUs","text":"<p>This example is suited to software written with OpenMP, where multiple CPU cores are needed per task (also called a process).</p> <p>Warning</p> <p>For this kind of job, users must specify <code>--cpus-per-task</code> at the top of their job file and when calling <code>srun</code>. See the Lab Notebook on changes to <code>srun</code> for more.</p> <p>Warning</p> <p>When using <code>srun -c</code> around your job's executable as in this example, it is usually not necessary to specify <code>OMP_NUM_THREADS</code>; however, setting <code>OMP_NUM_THREADS</code> will override any options passed to <code>srun</code>.</p> threaded_job.sb<pre><code>#!/bin/bash --login\n# Job name:\n#SBATCH --job-name=openmp-threaded\n\n# Number of nodes\n#SBATCH --nodes=1\n\n# Number of tasks to run on each node\n#SBATCH --ntasks-per-node=1\n\n# Number of CPUs per task\n#SBATCH --cpus-per-task=32\n\n# Memory per Node\n# Specify \"M\" or \"G\" for MB and GB respectively\n#SBATCH --mem=2G\n\n# Wall time\n# Format: \"minutes\", \"hours:minutes:seconds\", \n# \"days-hours\", or \"days-hours:minutes\"\n#SBATCH --time=01:00:00\n\n# Mail type\n# e.g., which events trigger email notifications\n#SBATCH --mail-type=ALL\n\n# Mail address\n#SBATCH --mail-user=yournetid@msu.edu\n\n# Standard output and error to file\n# %x: job name, %j: job ID\n#SBATCH --output=%x-%j.SLURMout\n\n# Purge current modules and load those we require\nmodule purge\nmodule load GCC/10.3.0\n\n# Run our job\ncd $SCRATCH\n# You MUST specify the number of CPUs per task again.\n# Alternatively, you can set OMP_NUM_THREADS\nsrun -c $SLURM_CPUS_PER_TASK my_openmp\n\n# Print resource information\nscontrol show job $SLURM_JOB_ID\njs -j $SLURM_JOB_ID\n</code></pre>","tags":["reference","slurm","job script"]},{"location":"Example_SLURM_scripts/#multiple-nodes","title":"Multiple nodes","text":"<p>When your required resources exceed those available on a single node,  you can request multiple nodes while specifying per-node resources.</p> <p>Note</p> <p>In most cases, the <code>srun</code> command takes the place of <code>mpirun</code>. The <code>srun</code> command does not require an argument specifying the number of processes to be used for the job.</p> mpi_job.sb<pre><code>#!/bin/bash --login\n# Job name:\n#SBATCH --job-name=mpi-parallel\n\n# Number of nodes\n#SBATCH --nodes=4\n\n# Number of tasks to run on each node\n#SBATCH --ntasks-per-node=32\n\n# Memory per Node\n# Specify \"M\" or \"G\" for MB and GB respectively\n#SBATCH --mem=2G\n\n# Wall time\n# Format: \"minutes\", \"hours:minutes:seconds\", \n# \"days-hours\", or \"days-hours:minutes\"\n#SBATCH --time=01:00:00\n\n# Mail type\n# e.g., which events trigger email notifications\n#SBATCH --mail-type=ALL\n\n# Mail address\n#SBATCH --mail-user=yournetid@msu.edu\n\n# Standard output and error to file\n# %x: job name, %j: job ID\n#SBATCH --output=%x-%j.SLURMout\n\n# Purge current modules and load those we require\nmodule purge\nmodule load GCC/10.3.0 OpenMPI/4.1.1\n\n# Run our job\ncd $SCRATCH\nsrun my_mpi_job\n\n# Print resource information\nscontrol show job $SLURM_JOB_ID\njs -j $SLURM_JOB_ID\n</code></pre>","tags":["reference","slurm","job script"]},{"location":"Example_SLURM_scripts/#multiple-nodes-with-multiple-cores-per-task-hybrid-jobs","title":"Multiple nodes with multiple cores per task (hybrid jobs)","text":"<p>Some programs can use, for example, both MPI and OpenMP to execute what  is called \"hybrid\" parallelism where each node runs one or more threaded processes.</p> <p>Warning</p> <p>For this kind of job, users must specify <code>--cpus-per-task</code> at the top of their job file and when calling <code>srun</code>. See the Lab Notebook on changes to <code>srun</code> for more.</p> <p>Warning</p> <p>When using <code>srun -c</code> around your job's executable as in this example, it is usually not necessary to specify <code>OMP_NUM_THREADS</code>; however, setting <code>OMP_NUM_THREADS</code> will override any options passed to <code>srun</code>.</p> hybrid_job.sb<pre><code>#!/bin/bash --login\n# Job name:\n#SBATCH --job-name=mpi-hybrid\n\n# Number of nodes\n#SBATCH --nodes=8\n\n# Number of tasks to run on each node\n#SBATCH --ntasks-per-node=6\n\n# Number of CPUs per task\n#SBATCH --cpus-per-task=4\n\n# Memory per Node\n# Specify \"M\" or \"G\" for MB and GB respectively\n#SBATCH --mem=2G\n\n# Wall time\n# Format: \"minutes\", \"hours:minutes:seconds\", \n# \"days-hours\", or \"days-hours:minutes\"\n#SBATCH --time=01:00:00\n\n# Mail type\n# e.g., which events trigger email notifications\n#SBATCH --mail-type=ALL\n\n# Mail address\n#SBATCH --mail-user=yournetid@msu.edu\n\n# Standard output and error to file\n# %x: job name, %j: job ID\n#SBATCH --output=%x-%j.SLURMout\n\n# Purge current modules and load those we require\nmodule purge\nmodule load GCC/10.3.0 OpenMPI/4.1.1\n\n# Run our job\ncd $SCRATCH\n# You MUST specify the number of CPUs per task again.\n# Alternatively, you can set OMP_NUM_THREADS\nsrun -c $SLURM_CPUS_PER_TASK my_hybrid_program\n\n# Print resource information\nscontrol show job $SLURM_JOB_ID\njs -j $SLURM_JOB_ID\n</code></pre>","tags":["reference","slurm","job script"]},{"location":"Expansion/","title":"Expansion","text":"<p>When we type a command with arguments/inputs and press the enter key, the shell does several things to the arguments/input text before it actually carries out the command. This action is called expansion. With expansion, the arguments expands into something else before the shell acts on it with the command. Let's see an example with <code>echo</code> command which prints out the text arguments given on standard output.</p> <pre><code>$ echo hello world!\nhello world!\n</code></pre> <p>Let's use <code>echo</code> with <code>*</code>:</p> <pre><code>$ echo *\nhello.c hello.qsub hello.sb README\n</code></pre> <p>Instead of printing <code>*</code>, it prints out all file names in the directory because shell expands <code>*</code> into something else before the <code>echo</code> command acts on the argument (in this case <code>*</code>).</p> <pre><code>$ ls\nhello.c hello.qsub hello.sb README\n</code></pre> <p>The <code>*</code> character is called a \"wildcard\" is replaced by everything in the current directory. We can also modify it so that it's only replaced by certain things. For example, <code>h*</code> is replaced everything that starts with <code>h</code> in the current directory.</p> <pre><code>$ echo h*\nhello.c hello.qsub hello.sb\n</code></pre> <p><code>~</code> is another special character with a special meaning. It expands into the name of the home directory of the user:</p> <pre><code>$ echo ~\n/mnt/home/temp_user_01\n</code></pre> <p>The shell will also expand arithmetic. Arithmetic expansion uses the form <code>$((expression))</code>. Look at the example.</p> <pre><code>$ echo $((1 + 1))\n2\n</code></pre> <p>Please keep in mind that arithmetic expansions allows only integers. Arithmetic expression can be nested, and spaces are allowed.</p> <pre><code>$ echo $(( 7*( 2 + 2 ) ))\n28\n$ echo $((7*(2+2)))\n28\n</code></pre> <p>Brace expansion is useful when you write a shell script or batch script. The brace expression can contain a comma separated list of characters, strings, or integers that will be expanded into multiple expressions. Here are a few examples.</p> <pre><code>$ echo srt-{a,b,c}-end\nsrt-a-end srt-b-end srt-c-end\n\n$ echo number_{1..5}\nnumber_1 number_2 number_3 number_4 number_5\n\n$ echo {A..Z}\nA B C D E F G H I J K L M N O P Q R S T U V W X Y Z\n\n$ echo a{A{1,5},B{6..10}}b\naA1b aA5b aB6b aB7b aB8b aB9b aB10b\n</code></pre> <p>The next example creates multiple directories with a brace expansion.</p> <pre><code>$ mkdir Photos\n$ cd Photos/\n$ mkdir {2020..2021}-{01..12}\n$ ls\n2020-01/  2020-03/  2020-05/  2020-07/  2020-09/  2020-11/  2021-01/  2021-03/  2021-05/  2021-07/  2021-09/  2021-11/\n2020-02/  2020-04/  2020-06/  2020-08/  2020-10/  2020-12/  2021-02/  2021-04/  2021-06/  2021-08/  2021-10/  2021-12/\n</code></pre> <p>Expansion also allows us to use the output of a command in different places. The syntax is the same as arithmetic expansion, except the command is placed inside <code>$(...)</code></p> <pre><code>$ echo $(ls | grep 2020)\n2020-01/ 2020-02/ 2020-03/ 2020-04/ 2020-05/ 2020-06/ 2020-07/ 2020-08/ 2020-09/ 2020-10/ 2020-11/ 2020-12/\n</code></pre> <p>Next, let's learn how to control expansion.\u00a0Consider the following two examples where automatic expansion doesn't do what we want.</p> <pre><code>$ echo This is a      test\nThis is a test\n\n$ echo The total is $100.00\nThe total is 00.00\n</code></pre> <p>In the first example, the shell removes extra space from the <code>echo</code> command's argument. In the second example, <code>$1</code> is interpreted as the first input parameter which is not defined here, and therefore, it is replaced as empty string. With quoting, we can suppress unwanted expansions.</p> <p>First, let's learn about double quotes. If we place text inside double quotes, parameters are not split by white space and all special characters lose their special meaning, and are treated as ordinary characters. However <code>$</code>, <code>\\</code> (backslash), and <code>`</code> (back quote) are exceptions.</p> <pre><code>$ echo \"This is a     test\"\nThis is a     test\n\n$ echo \"The total is $100.00\"\nThe total is 00.00\n</code></pre> <p>Here is another example where the shell splitting the parameters by white space would cause a problem. Suppose that our working directory has the file <code>two words.txt</code> in it, where the filename has a space.</p> <pre><code>$ ls two words.txt\nls: cannot access two: No such file or directory\nls: cannot access words.txt: No such file or directory\n</code></pre> <p>The shell splits the two words and looks for them each separately with <code>ls</code>. Quoting fixes this.</p> <pre><code>$ ls \"two words.txt\"\ntwo words.txt\n</code></pre> <p>If you want to suppress all expansions, you need to use single quotes.</p> <pre><code>$ echo 'This is a     test'\nThis is a     test\n\n$ echo 'The total is $100.00'\nThe total is $100.00\n</code></pre> <p>The next three examples show how quoting gives different results.</p> <pre><code>$ echo text ~/*.txt {1..5} $(echo foo) $((2+2)) $(date)\ntext /mnt/home/user_name/hostfile.txt /mnt/home/user_name/powertools.txt 1 2 3 4 5 foo 4 Tue Jan 19 15:10:00 EST 2021\n\n$ echo \"text ~/*.txt {1..5} $(echo foo) $((2+2)) $(date)\"\ntext ~/*.txt {1..5} foo 4 Tue Jan 19 15:10:09 EST 2021\n$ echo 'text ~/*.txt {1..5} $(echo foo) $((2+2)) $(date)'\ntext ~/*.txt {1..5} $(echo foo) $((2+2)) $(date)\n</code></pre> <p>A backslash is useful when we want to quote a single character. A backslash is called the escape character. Next example shows how quoting and an escape character work.</p> <pre><code>$ echo The balance of $(date) is $100\nThe balance of Tue Jan 19 15:15:36 EST 2021 is 00\n\n$ echo \"The balance of $(date) is $100\"\nThe balance of Tue Jan 19 15:15:48 EST 2021 is 00\n\n$ echo 'The balance of $(date) is $100'\nThe balance of $(date) is $100\n\n$ echo \"The balance of $(date) is \\$100\"\nThe balance of Tue Jan 19 15:16:09 EST 2021 is $100\n\n$ echo 'The balance of $(date) is \\$100'\nThe balance of $(date) is \\$100\n</code></pre> <p>The table show the most frequently used escape characters.</p> Escape Character Name usage \\n newline Adding blank lines to text \\t tab Inserting horizontal tabs to text \\a alert Making the user terminal beep \\\\ backslash Inserting a backslash","tags":["tutorial","command line"]},{"location":"External_Resources/","title":"External resources","text":"<p>In addition to ICER's workshops and D2L training materials, there are many trainings, tutorials, and resources online created by other high-performance computing centers and education groups. The links below include just a few of these materials that are mostly oriented towards general high-performance computing tasks.</p> <p>If you have found reference materials online that have helped you use ICER's HPCC more effectively, please share them by filling out the documentation feedback form.</p>","tags":["reference","training"]},{"location":"External_Resources/#introductory-materials","title":"Introductory materials","text":"<ul> <li>Introductory Supercomputing slides from Pawsey Supercomputing Research   Center</li> <li>HPC Carpentry lessons for basic skills in high-performance   computing</li> </ul>","tags":["reference","training"]},{"location":"External_Resources/#intermediate-materials","title":"Intermediate materials","text":"<ul> <li>Intermediate Supercomputing slides from Pawsey Supercomputing Research   Center</li> </ul>","tags":["reference","training"]},{"location":"External_Resources/#advanced-materials","title":"Advanced materials","text":"<ul> <li>Best Practices for HPC Software Developers webinar series from   IDEAS</li> </ul>","tags":["reference","training"]},{"location":"File-Permission-in-Research-Space_34963746.html/","title":"File Permission in Research Space","text":""},{"location":"File-Permission-in-Research-Space_34963746.html/#file-permission-in-research-space","title":"File Permission in Research Space","text":"<p>A user with account name <code>User1</code> is not able to access a directory <code>Dirct</code> in his research space <code>Group1</code>. The following is the result of the <code>ls</code> command:</p> <pre><code>[User1@dev-intel18 ~]$ ls -la /mnt/research/Group1\ntotal 98\ndrwxrwS---   3 ProjInvs Group1 8192 Aug  6 08:53 .\ndrwxr-xr-x 391 root     root      0 Sep  9 07:34 ..\n-rwx------   1 User2    Group2 4299 Jul  2  2018 file1\n-rwx------   1 User3    Group3 2452 Jul  2  2018 file2\ndrwxrwS---   2 User2    Group2 8192 May 22 11:31 Dirct\n-rw-rw-r--   1 User1    Group1  263 Aug  6 08:54 file3\n</code></pre> <p>Q: How to make <code>User1</code> able to access the directory <code>Dirct</code>?</p> <p>A: Since <code>User2</code> is the owner of the directory, <code>User2</code> can run a\u00a0 command to change the group ownership:</p> <pre><code>[User2@dev-node ~]$ chgrp -R Group1 /mnt/research/Group1/Dirct\n</code></pre> <p>Q: In order for all group users able to access files and directories in the research space, what should they do?</p> <p>A: They should run the following commands:</p> <ol> <li> <p>Change the group ownership of all files and directories to the research group <code>Group1</code>:</p> <pre><code>[UserID@dev-node ~]$ chgrp -R Group1 /mnt/research/Group1/ 2&gt;/dev/null\n</code></pre> </li> <li> <p>Open the permissions of all files and directories to be readable (<code>r</code>) and writable (<code>w</code>) to group users:</p> <pre><code>[UserID@dev-node ~]$ chmod -R g+rw /mnt/research/Group1/ 2&gt;/dev/null\n</code></pre> </li> <li> <p>Make all directories sticky to the group ownership for any file generated inside (turn on group sticky bits):</p> <pre><code>[UserID@dev-node ~]$ chmod g+s $(find /mnt/research/Group1/ -type d -user $USER 2&gt;/dev/null)\n</code></pre> </li> <li> <p>Group users should not copy files to their research space with preserving the ownership, such as using command \"<code>cp -p ...</code> \".</p> </li> </ol>"},{"location":"File_Count/","title":"<code>file-count</code>","text":"<p><code>file-count</code> is a powertools program that shows you the number of files in your home directory or any directory that you pass as an argument. This can be useful for comparing against the number of files from the <code>quota</code> command or determining where you may have a large number of files (especially if they are hidden).</p>","tags":["reference","files","quota"]},{"location":"File_Count/#examples","title":"Examples","text":"<p>Below are some examples of it's usage. Since <code>file-count</code> is a powertool, the <code>powertools</code> module must be loaded:</p> <pre><code>module load powertools\n</code></pre>","tags":["reference","files","quota"]},{"location":"File_Count/#list-file-count-of-home-directory","title":"List file count of home directory","text":"input<pre><code>file-count\n</code></pre> output<pre><code>385162  /mnt/home/user/\n</code></pre>","tags":["reference","files","quota"]},{"location":"File_Count/#list-file-count-of-all-subdirectories-in-home-directory","title":"List file count of all subdirectories in home directory","text":"<p>This includes hidden directories that may contain large numbers of files, like <code>.anaconda3</code>.</p> input<pre><code>file-count --detail \n</code></pre> output<pre><code>4   /mnt/home/user/.thumbnails\n12  /mnt/home/user/.jupyter\n22950   /mnt/home/user/R\n5   /mnt/home/user/.ssh\n446 /mnt/home/user/.config\n39  /mnt/home/user/.vnc\n...\n53  /mnt/home/user/scripts\n54356   /mnt/home/user/.cache\n6209    /mnt/home/user/.vscode-server\n385162  /mnt/home/user/\n</code></pre> <p>Note that a summary of all files is given at the bottom</p>","tags":["reference","files","quota"]},{"location":"File_Count/#list-file-count-of-a-research-directory","title":"List file count of a research directory","text":"input<pre><code>file-count /mnt/research/my_group\n</code></pre> output<pre><code>400124 /mnt/research/my_group/\n</code></pre>","tags":["reference","files","quota"]},{"location":"File_Permissions_on_HPCC/","title":"File Permissions on HPCC","text":"<p>The HPCC offers several different types of storage for users. All of these filesystems make use of standard UNIX file permissions. Understanding how standard UNIX permissions and ownership works is an important way to control access to your files.</p>","tags":["explanation","files","groups"]},{"location":"File_Permissions_on_HPCC/#unix-users-and-groups","title":"UNIX users and groups","text":"<p>Every user has a unique username on HPCC systems. This is typically your MSU NetID. Every user is also a member of at least one group. This group is typically the department the user is in (such as <code>cse</code> or <code>plb</code>). An user can be a member of additional groups. To see what groups you are a member of, run the <code>groups</code> command. If you feel you are in the wrong group, please contact HPCC staff.</p>","tags":["explanation","files","groups"]},{"location":"File_Permissions_on_HPCC/#unix-file-ownership","title":"UNIX file ownership","text":"<p>Every file and directory has two sets of ownership, the user and the group. The user owner is normally set to the user that created the file. Normally, the user owner of a file or directory is the only user that is able to change permissions or group ownership.</p> <p>The group owner of a file or directory allows a user owner to grant permissions to a group of users for a particular file or directory. The user owner of a file can change the group ownership of a file to any group that they are a member of. Any file created by a user normally defaults to group owner being set to the user's primary group, unless the user or directory owner has changed the behavior (using procedures described here.)</p>","tags":["explanation","files","groups"]},{"location":"File_Permissions_on_HPCC/#the-three-types-of-basic-unix-permissions","title":"The three types of basic UNIX permissions","text":"","tags":["explanation","files","groups"]},{"location":"File_Permissions_on_HPCC/#read","title":"Read","text":"<p>Read permission on a file allows the contents of a file to be read. The read permission, when applied to a directory, allows the contents of a directory to be listed. Referred to as \"r\" in the output of the <code>ls -l</code> command.</p>","tags":["explanation","files","groups"]},{"location":"File_Permissions_on_HPCC/#write","title":"Write","text":"<p>Write permission on a file allows the file to be modified or deleted. Write permissions in a directory allow the creation of additional files in that directory. Referred to as \"w\" in the output of the <code>ls -l</code> command.</p>","tags":["explanation","files","groups"]},{"location":"File_Permissions_on_HPCC/#execute","title":"Execute","text":"<p>The execute permission allows a file to be run as an executable. When applied to a directory it allows traversal of that directory: the ablility to access files or subdirectories in that directory. Referred to as \"x\" in the output of the <code>ls -l</code> command.</p>","tags":["explanation","files","groups"]},{"location":"File_Permissions_on_HPCC/#other-resources","title":"Other resources","text":"<p>This just covers the basic ideas of UNIX file permissions. In order to work with permissions on the HPCC most effectively, see the page on managing file permissions. Here are some other resources for more in-depth information: </p> <ul> <li>Software Carpentry - Permissions</li> <li>Linux Cookbook, 2nd ed., Chapter 9</li> <li>Computer Hope - Linux <code>umask</code> command</li> </ul>","tags":["explanation","files","groups"]},{"location":"File_transfer/","title":"File transfer","text":"<p>This document highlights several simple methods to transfer files to the HPCC home and research directories. There are two main gateway systems for copying files.\u00a0</p> <ol> <li> <p><code>hpcc.msu.edu</code>: This is our login gateway. While it can be used for file transfer, it's not intended for high volumes of files. More importantly, the scratch space is not mounted there and so you can't access your files on scratch.</p> </li> <li> <p><code>rsync.hpcc.msu.edu</code>: It has access to scratch, and is dedicated to file transfer. Although this gateway is named by the popular Linux \"rsync\" command, it can be used for \"sftp\" or \"scp\" as well. Starting in October 2022, login to the rsync gateway will accept SSH keys as the ONLY authentication method. Username/password won't work. Please refer to the SSH key tutorial for setting up your keypair.</p> </li> </ol>","tags":["how-to guide","rsync","scp","OnDemand"]},{"location":"File_transfer/#all-operating-systems","title":"All operating systems","text":"<p>Note</p> <p>The OnDemand portal is best for transferring files less than ~1 GB in size. For transferring larger files to and from the HPCC, see Large file transfer (Globus)</p> <p>The most straightforward way to transfer files to and from the HPCC is via our OnDemand web portal. Log in with your NetID at https://ondemand.hpcc.msu.edu and click \"Files\" to access your different user spaces.</p> <p></p> <p>On this page, you can upload, download, rename, and modify most files in storage locations you have access to.</p>","tags":["how-to guide","rsync","scp","OnDemand"]},{"location":"File_transfer/#specific-operating-systems","title":"Specific operating systems","text":"<p>Use the tabs below to view the relevant options for your system.</p> Linux and Mac commandsWindows <p>A number of different command-line utilities are available to OS X and Linux users. Each of them has its own advantages.</p> <p>Warning</p> <p>Using the commandline to connect to the <code>rsync.hpcc.msu.edu</code> gateway requires SSH key setup. Please refer to the SSH key tutorial for setting up your keypair.</p> <ol> <li> <p>Basic file copy (<code>scp</code>)</p> <p>A simple command for transferring files between the cluster and another host is <code>scp</code>. To copy a file from a local directory to file space on the cluster, run a command such as</p> <pre><code>scp example.txt username@rsync.hpcc.msu.edu:example_copy.txt\n</code></pre> <p>This will copy the file named <code>example.txt</code> in the local host's current directory to the user's home directory on the cluster, with the copy having the name <code>example_copy.txt</code>. Leaving the space after the colon blank gives the new file the same name as the original.\u00a0 Note: To transfer a file name with spaces you must put a backslash before each space in your file name, i.e. <code>scp \"My File Name\" username@hpcc.msu.edu:\"My\\ File\\ Name\"</code>.</p> <p>To copy a file from the cluster to your local directory,</p> <pre><code>scp username@rsync.hpcc.msu.edu:example.txt ./example_copy.txt\n</code></pre> <p>will copy the file named <code>example.txt</code> from the user's home directory on the cluster to the home directory of the local host, naming the new file <code>example_copy.txt</code>. Leaving the space after the slash blank gives the new file the same name as the original. The <code>-r</code> option can be used to copy entire directories recursively.\u00a0</p> </li> <li> <p>Synchronize directories (<code>rsync</code>)</p> <p>If you are an advanced LINUX/Mac user, there is a useful utility that makes mirroring directories simple. The syntax looks very similar to <code>scp</code>.</p> <ul> <li> <p>To mirror <code>&lt;local_dir&gt;</code> on my local computer to <code>&lt;hpcc_dir&gt;</code> on hpcc, the following command can be run:</p> <pre><code>rsync -ave ssh &lt;local_dir&gt; username@rsync.hpcc.msu.edu:&lt;hpcc_dir&gt;\n</code></pre> <p>In the above command, rsync will scan through both directories. If any files in the <code>&lt;local_dir&gt;</code> are newer, they will be uploaded to <code>&lt;hpcc_dir&gt;</code>. (It is also possible to get rsync to upload ALL different files, regardless of which is newer).</p> </li> <li> <p>To mirror the HPCC directory to your local system, call</p> <pre><code>rsync -ave ssh username@rsync.hpcc.msu.edu:&lt;hpcc_dir&gt; &lt;local_dir&gt;\n</code></pre> </li> <li> <p>Please use the <code>rsync</code> command with the option <code>--chmod=Dg+s</code> to transfer files from a local computer to your research space.     See the following example:</p> <pre><code>rsync -ave ssh TestDir --chmod=Dg+s &lt;username&gt;@rsync.hpcc.msu.edu:/mnt/research/&lt;GroupName&gt;/\n</code></pre> </li> </ul> <p>Note</p> <p>the first time you use <code>rsync</code>, you might want to add the <code>-n</code> flag to do a dry run before any files are copied.</p> </li> <li> <p>Interactive file copy (<code>sftp</code>)</p> <p>When performing several data transfers between hosts, the <code>sftp</code> command may be preferable, as it allows the user to work interactively. Running</p> <pre><code>sftp username@rsync.hpcc.msu.edu\n</code></pre> <p>from a local host establishes a connection between that host and the cluster. Both hosts can be navigated. For the local file system, <code>lcd</code> changes to the specified directory, <code>lpwd</code> prints the working directory, and <code>lls</code> prints a list of files in the current directory. For the remote file system, the same three commands are available, minus the leading <code>l</code>. Also available are commands to change permissions, rename files, and manipulate directories on the remote host. The two key commands are <code>get &lt;file&gt;</code>, which copies the file in the remote working directory to the local working directory, and <code>put &lt;file&gt;</code>, which copies the file in the local working directory to the remote working directory. The <code>quit</code> command closes the connection between hosts.</p> </li> <li> <p>Copy files from Internet (<code>wget</code>)</p> <p><code>wget</code> is a simple command useful for copying files from the Internet to a user's file space on the cluster.\u00a0 Running the line</p> <pre><code>wget http://www.examplesite.com/examplefile.txt\n</code></pre> <p>downloads <code>examplefile.txt</code> to the user's working directory.</p> </li> </ol> <p>An alternate method for transferring files on Windows is MobaXTerm. Installation and setup instructions are available here. Once you are connected to the HPCC, you can use the MobaXterm SCP interface tab to upload and download files, available on the left side of the window.</p> <p></p> <p>You can type a path at the top of the SCP interface tab to access anywhere you have permissions on the HPCC e.g. your scratch and research spaces.</p> <p>MobaXTerm can also be set up for use as a SFTP client to transfer data with the <code>rsync.hpcc.msu.edu</code> gateway. As with other uses of the <code>rsync.hpcc.msu.edu</code> gateway, you must have an SSH key pair. Please refer to the SSH key tutorial for setting up your keypair.</p> <p>The configuration for the SFTP session should look like this:</p> <p></p> <p>The settings should be:</p> <ul> <li>Remote host: <code>rsync.hpcc.msu.edu</code></li> <li>Username: your HPCC username.</li> <li>Select the Advanced Sftp settings tab.</li> <li>Use private key: check the box. Click the small file icon to open a file browser and select your private key. Alternatively, type in the path to your private key.</li> <li>All other settings can remain default.</li> </ul>","tags":["how-to guide","rsync","scp","OnDemand"]},{"location":"Frequently_Asked_Questions_FAQ_/","title":"Frequently Asked Questions (FAQ)","text":"<p>This page lists many of our frequently asked questions. Please search for keywords related to an issue by using Ctrl+F (on Windows/Linux) or Cmd+F (on Mac), or scroll through the list of questions in the table of contents to the right.</p> <p>If you don't see an answer to your question, please contact us.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#table-of-contents","title":"Table of contents","text":"<ul> <li>Logging in and accessing the HPCC</li> <li>Limits and usage</li> <li>Storage and files</li> <li>Submitting jobs and running code</li> <li>Software and modules</li> <li>Python and Conda</li> <li>Getting help</li> </ul>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#logging-in-and-accessing-the-hpcc","title":"Logging in and accessing the HPCC","text":"","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#what-is-my-hpcc-user-namepassword","title":"What is my HPCC user name/password?","text":"<p>If you are affiliated with MSU, then your MSU NetID is your user name, and your NetID password is your HPCC password. This is the same as those for all the MSU online services. An HPCC account must be requested by an MSU faculty member at\u00a0https://contact.icer.msu.edu/account</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#can-i-reset-my-password-on-the-hpcc-because-my-login-got-denied-after-multiple-failed-attempts","title":"Can I reset my password on the HPCC because my login got denied after multiple failed attempts?","text":"<p>There are two ways you can be blocked by entering an incorrect password too many times. The authentication on the HPCC is directly tied to MSU. If you attempt an incorrect password too many times, you may need to request a password reset at https://netid.msu.edu/netid/password/index.html. The HPCC also maintains blocks from hosts with too many failed attempted SSH connections. Users that can log into other MSU resources (Spartan365, D2L, EBS) but are unable to connect to the HPCC should submit a ticket on our contact forms. Be sure to include your external IP address; you can check it with Google.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-used-to-be-able-to-connect-to-the-hpcc-server-but-now-i-cant-why","title":"I used to be able to connect to the HPCC server, but now I can't. Why?","text":"<p>There can be multiple reasons for this, such as system downtime (so please check the ICER blog first). Another common reason is account expiry. The HPCC periodically disables users who are no longer affiliated with the university or registered with a class for which the instructor has created temporary student accounts. To re-activate your HPCC account, please have your PI submit a sponsoring form at https://contact.icer.msu.edu/sponsoredrenewal</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-get-a-permission-denied-error-but-i-put-in-the-right-password-whats-wrong","title":"I get a \"Permission denied\" error, but I put in the right password. What's wrong?","text":"<p>If you are attempting to connect to the <code>rsync.hpcc.msu.edu</code> server, this requires a SSH key pair. See our documentation for how to generate a key pair here. Otherwise, see the question above.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#can-i-use-hpc-through-web-browsers","title":"Can I use HPC through web browsers?","text":"<p>Yes, we provide Open OnDemand, a web portal for easy web access to the HPCC. Check out this tutorial.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#limits-and-usage","title":"Limits and usage","text":"","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#are-there-any-limits-per-user-on-using-the-hpcc-resources","title":"Are there any limits per user on using the HPCC resources?","text":"","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#dev-node-limits","title":"Dev node limits","text":"<p>Each process on a dev-node is limited to 2 CPU hours. If you are running a multi-threaded program, the wall time limit would be (roughly) 2 hours divided by the number of threads.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#limits-on-storage","title":"Limits on storage","text":"<ol> <li>Each user has up to 1 TB of storage for free and 1 million files, for each    of the home and research directories. Beyond 1 TB, the cost is $89 per TB    per year for MSU users.</li> <li>For scratch space (i.e. <code>/mnt/scratch/&lt;your_user_name&gt;</code>), 50 TB is the    maximum; more may be requested via contact    forms with center director approval.).</li> </ol>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#limits-on-cluster-usage","title":"Limits on cluster usage","text":"<ol> <li>the longest wall time you can request is 7 days; </li> <li>the maximum number of CPU cores you can use is 1040\u00a0at any one time (see    SLURM variable <code>QOSMaxCpuPerUserLimit</code>), unless you have a larger buy-in and    your PI has requested that your buy-in account only run on the buy-in nodes;</li> <li>the maximum number of jobs that can be queued is 1000 and 520 running at any    one time (except in the scavenger queue); </li> <li>non-buyin users have a maximum of 500,000 CPU hours per year.</li> </ol>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-would-like-to-know-more-about-the-dev-node-limit","title":"I would like to know more about the dev-node limit?","text":"<p>When you connect to any\u00a0of the HPCC's dev-nodes, you will see the following\u00a0message:</p> <p><code>processes on development nodes are limited to two hours of CPU time.</code></p> <p>The two hour\u00a0CPU time limit is for each process you run on that dev-node.\u00a0If one process uses CPU time greater\u00a0than 2 hours, then only that\u00a0process will be killed. You can, however, still connect to that dev-node, and run another process.\u00a0Additionally, if your process uses 100% CPU (1 core), it will be terminated in two hours. If your process uses 200% CPU (2 cores), it will be terminated in one hour, and so on.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#how-do-i-check-my-cpu-or-gpu-time-usage","title":"How do I check my CPU or GPU time usage?","text":"<p>Run the command <code>SLURMUsage</code> for both CPU and GPU. </p> <p>NOTE: This time usage does not include time that was submitted to a buyin node.</p> <ul> <li>If you would like to get full usage data, including all buyin usage, you can   run <code>sreport</code> to get the information for specific date ranges: <code>sreport job   SizesByAccount Users=$USER start=2023-01-01 end=now -t hour</code>. This report is   broken down by job size and all columns should be summed for the total usage   in hours. </li> <li>A detailed accounting report can be generated with <code>sacct -X --duplicates -u   $USER -S 2023-01-01 -E 2023-04-03 -o jobid,ncpus,elapsedraw,CPUTimeRaw</code>.   This output should be saved to a file and the CPUTimeRaw column summed for   total hours.  CPUTimeRaw is equal to ncpus * elapsedraw.</li> </ul>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#how-to-check-the-hpcc-node-usage","title":"How to check the HPCC node usage?","text":"<p>Users can see this information by simply running the <code>node_status</code> command on any dev node. We also offer a web-based dashboard at https://icer.msu.edu/dashboard.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#storage-and-files","title":"Storage and files","text":"","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#quota","title":"Quota","text":"","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#quota-issues-writing-to-research-spaces","title":"Quota issues writing to research spaces","text":"<p>Many users have reported problems copying or transferring files to their research space. Although their research space still has plenty of space, they still get the following error message:</p> <p>failed to ... Disk quota exceeded</p> <p>This problem may occur because you do not have your primary group set to match the research space or the folders which you copy or transfer files to have\u00a0incorrect group ownership or no set-group-ID. Please read the instructions for using a research space, in particular, point 5.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#quotafile-limit-exceeded-or-general-issues-related-to-writing-files-especially-in-home-directories","title":"Quota/file limit exceeded or general issues related to writing files (especially in home directories)","text":"<p>Begin by checking your storage usage with the <code>quota</code> command. Then compare with the results from running the <code>file-count</code> powertool:</p> <pre><code>module load powertools\nfile-count\n</code></pre> <p>For more detailed information including a count of files in each subdirectory, use</p> <pre><code>file-count --detail\n</code></pre> <p>If you find that you are over quota, please delete files, move them to another location (like a research space or if they are temporary, scratch space), or move them off of the HPCC. If this resolves the issue, then you may consider keeping your files in a different location or asking for more space in your home directory.</p> <p>If you don't see a change in your quota, the number reported by <code>quota</code> and <code>file-count</code> are extremely different, or your quota is showing unrealistic numbers like negative or extremely large file counts, please contact ICER as this is likely the result of an unresolved issue with one of the HPCC's storage systems.</p> <p>Sometimes exceeding your quota can stop you from being able to login or access systems like OnDemand because they require writing to a small file. If this is the case, please contact ICER</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#my-files-in-the-scratch-space-are-gone","title":"My files in the scratch space are gone?","text":"<p>Files in scratch are automatically purged if the last changed time is older than 45 days. Note that the scratch spaces are not intended for long-term storage. Files saved in scratch have no back-up.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#how-do-i-copy-files-fromto-my-ms-one-drivegoogle-drive","title":"How do I copy files from/to my MS One Drive/Google Drive?","text":"<p>Rclone is currently installed on the HPCC. This software supports research in the cloud and helps HPCC users to sync files and directories between MSU\u2019s HPCC and their cloud storage, including OneDrive and Google Drive. Please refer to Rclone</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#what-is-hpccs-data-protection-policy","title":"What is HPCC's data protection policy?","text":"<p>All of the HPCC's shared storage systems are protected against individual drive and storage node failure (using RAID and highly availabile, active-active servers.) </p> <p>We maintain an offsite disaster recovery system for users' home and research directories. We do not archive users' scratch spaces nor the persistent 'nodr' space. </p> <p>Our goal is to maintain hourly snapshots for the last 24 hours and 60 days of file history on the disaster recovery servers. However, when there is a significant amount of data written, there may be a delay in copying updated data to the disaster recovery servers. Users that have hard requirements for should consider using MSU's Data Storage Finder.</p> <p>Users may request older versions of their files via the contact forms.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#does-hpcc-offer-a-cheaper-long-term-archiving-plan","title":"Does HPCC offer a cheaper long-term archiving plan?","text":"<p>We do not. However, MSU offers the Data Storage Finder (https://data-storage-finder.tech.msu.edu, on-campus only). There are several possible options for data archiving.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#submitting-jobs-and-running-code","title":"Submitting jobs and running code","text":"","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-have-a-buyin-account-do-i-need-to-specify-it-when-i-submit-jobs","title":"I have a buyin account, do I need to specify it when I submit jobs?","text":"<p>No, unless your PI has requested that it be opt-in instead of the default. When submitting a job without specifying an account, your default account is used. You can check your default account using the \"buyin_status -l\" command; buyin user's default is their buyin account. We recommend you read this if you have purchased buyin nodes.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#do-you-support-running-gpu-jobs","title":"Do you support running GPU jobs?","text":"<p>Yes. There are three GPU dev-nodes and a series of compute nodes in the cluster; see Cluster resources.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#what-does-the-message-nodes-required-for-job-are-down-drained-or-reserved-for-jobs-in-higher-priority-partitions-mean-after-my-job-is-submitted","title":"What does the message \"Nodes required for job are DOWN, DRAINED or reserved for jobs in higher priority partitions\" mean after my job is submitted?","text":"<p>Once a job is submitted the scheduler adds it to the calculations and continues to update the status of the job as the system works. The status for a job will reflect the current state of the scheduler, so you will see this message update once the scheduler has found a place to put the job. There are always some nodes which are down or drained in the cluster due to normal maintenance, but the \"reserved for jobs in higher priority partitions\" is the important part, and simply indicates that the scheduler has not yet found a time to schedule the job. This will update as the scheduler continues to function.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#why-did-i-get-an-illegal-instruction-error","title":"Why did I get an \"Illegal Instruction\" error?","text":"<p>This is usually because a program was compiled on a newer CPU architecture (e.g., intel18) but then run on an older one (e.g., intel14). Our system has a range of CPUs, and the newest versions support new instructions not available on the older CPUs. One short-term fix is to run programs on the same CPU that they were compiled on. Based on our experience, this error has occurred only on intel14 nodes and therefore you need to avoid them. That is, for dev-node testing, pick one from dev-intel16, dev-intel16-k80 and dev-intel18. For job submission, add <code>#SBATCH--constraint=\"[intel16|intel18]\"</code>in your SLURM script.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#software-and-modules","title":"Software and modules","text":"","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-want-to-install-software-packages-what-should-i-do","title":"I want to install software packages, what should I do?","text":"<p>The HPCC has a lot of software installed already. Search for the software you  want to install using <code>module spider &lt;software name&gt;</code>, then follow the instructions provided by the output to use <code>module load</code> and use the software.  See our documentation on this subject here. We have additional documentation on the module system  here.</p> <p>If the software is not present, you can submit a ticket. However, we encourage  users to install software on their own, if possible. The HPCC has provided  numerous versions of compilers and libraries which should accommodate the vast  majority of software across different fields.</p> <p>If you are thinking of requesting the system-wide installation of a piece of software, we strongly recommend you check the following factors when submitting a request for software installation:\u00a0</p> <ol> <li> <p>How popular is the software? If it is not a popular software, are there    other users on HPCC who would also be using it? If you are the only one    using it, we would recommend it be installed in your home directory.\u00a0</p> </li> <li> <p>What type of license agreement does the software have? Some software    licenses may restrict use even when they are free. Examples include software    with export control, specific end-user license agreement, etc. When software    licenses restrict use, we typically recommend the user directly make an    agreement with the software provider to obtain and install it in their home    directory. If it will be used by a group of people, HPCC system    administrators can help with setting up the group access in compliance with    the license agreement.\u00a0</p> </li> <li> <p>Is the software well maintained and up-to-date? If the software you wish    to install is legacy software or is not being well maintained, chances are    its installation will require an older version of its dependencies as well.    The effort to install this software may then be greater than the effort    required to find an up-to-date software with the same, similar, or even    better functionality. It may be time to consider transitioning to using a    newer software.\u00a0\u00a0</p> </li> </ol>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#why-did-my-module-load-command-output-errors","title":"Why did my \"module load\" command output errors?","text":"<p>There are many reasons that errors occur when you try loading a module. However, the most common cause is that you have forgotten to run <code>module purge</code>. Sometimes, <code>module spider</code> can also fail to find the module. Most likely it's because your personal module cache is out of date. To clear it, run <code>rm -r ~/.lmod.d/.cache</code>.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#what-should-i-do-when-i-cannot-load-modules","title":"What should I do when I cannot load modules?","text":"<p>See How to find and load software modules.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#what-is-powertools","title":"What is powertools?","text":"<p>The powertools module is a collection of software tools and examples that allows researchers to better utilize HPC systems. Powertools was created to help advanced users use the HPCC more effectively. To learn more about powertools, run the command <code>powertools</code>.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#python-and-conda","title":"Python and Conda","text":"","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#how-do-i-use-python-on-the-hpcc","title":"How do I use Python on the HPCC?","text":"<p>There are two methods: users can install their own version of Python with Anaconda or use the versions of Python installed on the HPCC system. See here.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-have-a-python-conflict-what-should-i-do-to-resolve-it","title":"I have a Python conflict. What should I do to resolve it?","text":"<p>Upon login to a dev-node, a default module list will load automatically. Since Python/3.6.4 is included in the list, it can interfere with a user's conda environment. As a consequence, your program may not be able to find packages installed in your conda environment even if it has been activated. In other words, the program still picks up Python/3.6.4 in the module system. The solution is to run <code>module unload Python</code> before activating the conda environment.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#how-do-i-deactivate-conda-base-environment","title":"How do I deactivate Conda base environment?","text":"<p>Many users have reported that after a local installation of Anaconda on the HPCC, their login prompt changes\u00a0to something starting with <code>(base) -bash-4.2$</code>. This is because conda activates the default environment, <code>base</code>, upon startup. To disable this behavior, which often results in conflicts with system defaults, users can run the following command:</p> <pre><code>conda config --set auto_activate_base False\n</code></pre>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-tried-to-start-a-jupyter-notebook-through-ondemand-but-my-job-will-not-start-or-will-not-recognize-my-conda-environment","title":"I tried to start a Jupyter Notebook through OnDemand, but my job will not start or will not recognize my Conda environment","text":"<p>All Conda environments used with the Jupyter Notebook OnDemand app must have Jupyter installed. Without this, the OnDemand job status will stay stuck on </p> <p>Your session is currently starting... Please be patient as this process can take a few minutes.</p> <p>before moving to</p> <p>For debugging purposes, this card will be retained for 6 more days</p> <p>without giving the chance to start the notebook. Depending on the setup, the job may start, but the environment will not be properly recongized and the app will fall back to the default version installed on the HPCC.</p> <p>To install Jupyter in your Conda environment on the command line, activate it first by running</p> <pre><code>module load Conda/3\nconda activate &lt;environment_name&gt;\n</code></pre> <p>and then run</p> <pre><code>conda install jupyter\n</code></pre>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-tried-to-use-python-matplotlib-to-plot-but-got-an-error-of-no-module-named-_tkinter","title":"I tried to use python matplotlib to plot, but got an error of \"No module named '_tkinter'\"","text":"<p>If you use the default python module (<code>/opt/software/Python/3.6.4-foss-2018a/bin/python</code>) on a dev-node, you need to load the Tkinter module before using python in order to proceed without errors. Run: <code>module load Tkinter/3.6.4-Python-3.6.4</code></p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#getting-help","title":"Getting help","text":"","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#can-you-keep-me-posted-on-the-current-status-of-the-hpcc","title":"Can you keep me posted on the current status of the HPCC?","text":"<p>Yes. Users are encouraged to follow the HPCC Announcements blog to keep updated on the status of HPCC (such as scheduled downtimes and urgent notices).</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-am-looking-for-help-to-troubleshoot-my-problem-how-do-i-share-my-codefiles-with-you","title":"I am looking for help to troubleshoot my problem. How do I share my code/files with you?","text":"<p>We do not go to your directory to view files or test your code for that matter. Please send your files along with your reply to the ticket email.</p>","tags":["reference","FAQ"],"boost":5},{"location":"GATK4/","title":"GATK4","text":"<p>Be sure to read this Quick Start before using GATK4. In particular, note the following statement from the developers:</p> <p>Once you have downloaded and unzipped the package (named <code>gatk-[version]</code>), you will find four files inside the resulting directory:</p> <p><code>gatk</code> <code>gatk-package-[version]-local.jar</code> <code>gatk-package-[version]-spark.jar</code> <code>README.md</code></p> <p>Now you may ask, why are there two jars? As the names suggest, <code>gatk-package-[version]-spark.jar</code> is the jar for running Spark tools on a Spark cluster, while <code>gatk-package-[version]-local.jar</code> is the jar that is used for everything else (including running Spark tools \"locally\", i.e. on a regular server or cluster).</p> <p>So does that mean you have to specify which one you want to run each time? Nope! See the gatk file in there? That's an executable wrapper script that you invoke and that will choose the appropriate jar for you based on the rest of your command line. You could still invoke a specific jar if you wanted, but using gatk is easier, and it will also take care of setting some parameters that you would otherwise have to specify manually.</p> <p>On the HPCC, after login to a dev-node, run: <code>module load GATK/4.0.5.1-Python-3.6.4</code>. As a tip, if you happen to run a <code>module purge</code> command in the middle of your work, and want to go back to the original login environment, please type the command: <code>exec bash -l</code></p> <p>A simple test on the HPCC is provided below.</p> <pre><code>module load GATK/4.0.5.1-Python-3.6.4\ngatk --java-options \"-Xmx8G\" HaplotypeCaller -R /opt/software/GATK/3.3-0-Java-1.7.0_80/resources/exampleFASTA.fasta -I /opt/software/GATK/3.3-0-Java-1.7.0_80/resources/exampleBAM.bam -O gatk_test.vcf\n</code></pre>"},{"location":"GPU_resources/","title":"The HPCC's GPU Resources","text":"<p>The HPCC offers several generations of GPUs as noted in the general Cluster Resources page. More information about these devices is provided in the table below. The cluster types that each GPU is associated with correspond to the cluster types listed in the Cluster Resources table.</p> GPU Cluster Type Number per Node GPU Memory Architecture Compute Capability Connection Type NVLink <code>a100</code> amd21* 4 81920 MB Ampere 8.0 SXM Yes intel21 4 40960 MB Ampere 8.0 PCIe No <code>v100</code> amd20 4 32768 MB Volta 7.0 PCIe Mixed intel18 8 32768 MB Volta 7.0 PCIe Yes <code>k80</code> intel16 8 12206 MB Kepler 3.7 PCIe No <code>k20</code> intel14 2 4743 MB Kepler 3.5 PCIe No <p>*The amd21 cluster contains some nodes that belong the to Data Machine.</p>","tags":["reference","GPU"]},{"location":"GPU_resources/#architecture-compute-capability","title":"Architecture &amp; Compute Capability","text":"<p>Currently, all of the HPCC's GPUs are manufactured by NVIDIA. They are designed following multiple architectures. Knowing a GPU's architecture aids in researching their technical specifications. A GPU's architecture is abbreviated in it's name; for example, the V100 GPUs follow the Volta architecture. </p> <p>Specific architectures and models of GPUs are able to meet certain compute capabilities (CC): sets of features that applications can leverage when executing on that GPU. Newer GPUs offer more advanced features and therefore adhere to a newer version of NVIDIA's compute capabilities. An explanation of what features are available for each compute capability can be found on both as part of the CUDA documentation (CC &gt; 5.0 only) and compiled on Wikipedia.</p> <p>Developers may use the CUDA programming language to utilize our GPUs in their software applications. See our page on Compiling for GPUs for more information on which versions of CUDA may be used for each of the HPCC's GPUs and their respective compute capabilities.</p>","tags":["reference","GPU"]},{"location":"GPU_resources/#connection-type","title":"Connection Type","text":"<p>Most of the HPCC's GPUs communicate with the CPUs of their host node via the PCIe (Peripheral Component Interconnect Express) bus. This bus is the primary channel by which data and instructions are transferred to and from the GPU. As such, the speed of this bus can affect the speed of GPU applications where large amounts of data transfer are a concern. In contrast, the A100 GPUs associated with the amd21 clusters are connected using SXM (Server PCI Express Module) sockets which offer higher connection speeds. Research the specifications of the particular GPU you are planning to use to learn more specifics about their bus's bandwidth.</p>","tags":["reference","GPU"]},{"location":"GPU_resources/#nvlink","title":"NVLink","text":"<p>While PCIe and SXM refer to the connection between the CPU and GPU, some of the HPCC's V100 and A100 GPUs are also connected to each other using NVIDIA's NVLink technology. NVLink allows GPUs to directly share data with each other. Without NVLink, transferring data from one GPU to another would require that the data first pass through the CPU. Using the CPU as a data transfer \"middleman\" adds to overall time the transfer takes and may also delay the CPU from communicating additional data and instructions to the GPUs. If you plan to use multiple GPUs for your job, consider requesting resources that support NVLink as indicated in the table above.</p> <p>Some of the amd20 nodes support NVLink while others do not. You can check whether or not a given node supports NVLink by requesting a job on that node and connecting to it. Specific nodes can be requested with the <code>-w</code> or <code>--nodelist</code> option; see the list of job specifications for more. Then, once connected, run <code>nvidia-smi nvlink -s</code> to check the status of the node's NVLink connection.</p>","tags":["reference","GPU"]},{"location":"Gaussian_Access/","title":"Gaussian Access","text":"<p>To obtain access to this software on HPCC complete\u00a0and sign the appropriate\u00a0Gaussian Confidentiality Agreement. Please ensure that all required sections are filled out legibly or else the form will be returned to you.</p> <ul> <li>Research Group Leader Form\u00a0\u2014\u00a0Only one form needs     to be completed per research group.</li> <li>User/Researcher Form\u00a0\u2014 Each user of the software     needs to complete this form.\u00a0Please ensure that your group leader     has completed the Research Group Leader form before submitting     this.\u00a0</li> <li>Coursework Form\u00a0\u2014 This form is to be completed by     the each student enrolled in an MSU course. Please note that the     course instructor should have completed the Research Group Leader     Form above. Access ends after the class is completed.\u00a0</li> </ul> <p>Email the completed form to\u00a0general@rt.hpcc.msu.edu.\u00a0 Please make the subject of your email, \"Gaussian Confidentiality Agreement.\" Your completed agreement will then be sent to Gaussian for the final approval. Once approved, the HPCC System Admin team will act upon the request and grant access.</p> <p>Note: This process can take up to two weeks from the date of submission of the Confidentiality Agreement to Gaussian giving approval of the software so we ask that you please be patient. Without a personally signed Confidentiality Agreement, you will not be granted access to Gaussian.</p>"},{"location":"Gaussian_Job_Script/","title":"Gaussian Job Script","text":"<p>Here is a simple job script <code>g16.sb</code> for running Gaussian job <code>g16.com</code>:</p> <p>g16.sb</p> <pre><code>#!/bin/bash --login\n\n#SBATCH \u2013-job-name=GaussianJob\n#SBATCH \u2013-ntasks=1\n#SBATCH \u2013-cpus-per-task=4\n#SBATCH --mem=7G\n#SBATCH \u2013-time=00:10:00 \n\nInputFile=g16.com\nOutputFile=g16.log\n\nmodule load Gaussian/g16 powertools\n# GAUSS_SCRDIR=&lt;your preferred Gaussian scratch space&gt;\n# mkdir -p ${GAUSS_SCRDIR}\n\ng16 &lt; ${InputFile} &gt; ${OutputFile}\n\n### write job information to SLURM output file\nscontrol show job $SLURM_JOB_ID \n\n# Print out resource usage  \njs -j $SLURM_JOB_ID           ### powetools command\n</code></pre> <p>where the Gaussian input file <code>g16.com</code> can be found from the\u00a0previous section Running Gaussian by Command Lines.</p> <p>For the resource request (<code>#SBATCH</code> lines) above, since Gaussian can only run in parallel with shared memory in HPCC system, only 1 task (with 1 node) is requested in line 2. The number of CPUs requested in line 3 is the same as the setting of \"<code>%NProcShared</code>\" (<code>=4</code>) in the Gaussian input file. The memory request in line 4 should be larger than the setting of \"<code>%Mem</code>\" (<code>=5GB</code>) in the Gaussian input file in case the job runs out of memory. Please also make sure the walltime request in line 5 is longer enough to finish the job.</p> <p>In the command line, you need to make sure Gaussian/g16 is loaded as in line 10. If you would like to use scratch directory other than <code>/mnt/scratch/$USER</code> for the Gaussian scratch files, you could set up a different one with line 11 and 12. The calculation of the Gaussian job is executed in line 14 with input file <code>g16.com</code> and output file <code>g16.log</code>. Once the calculation is done, line 17 and 20 will be executed to print out the job information and resource usage respectively to the SLURM output file ( with file name: <code>slurm-&lt;JobID&gt;.out</code>).</p>"},{"location":"Gaussian_Job_with_Checkpointing_Run/","title":"Gaussian Job with Checkpointing Run","text":"<p>For running a large system with Gaussian, it usually takes a long time and many resources to complete. It is a good idea to set up checkpointing so the calculation can keep going in case of any interruption due to walltime limit or possible system malfunction. The checkpointing function can save a snapshot of a Gaussian running state so it can restart from the previous calculation. Users can also divide a long-time job into many 4-hour short jobs since jobs with walltime less than or equal to 4 hours can use the buy-in nodes (55% of all nodes) on the HPCC.</p> <p>In order to have an appropriate checkpointing run with Gaussian, an unified read-write file setting (<code>%RWF</code>) should be in the Link 0 section of the input file. An example <code>water.gjf</code> is in the following:</p> <p>water.gjf</p> <pre><code>%NProcShared=2\n%Mem=3GB\n%RWF=water.rwf\n%NoSave\n%chk=water.chk\n#P opt b3lyp/aug-cc-pVTZ\n\nwater molecules\n\n0 1\nO   -2.12123400  1.99409800 -1.27381200\nH    1.52438600  0.53672100  0.67508800\nH    1.76493000 -0.81527300 -0.18137000\nO   -1.12977500 -0.31430400 -0.37860700\nH   -1.76492800 -0.81528500  0.18137200\nH   -1.52439700  0.53670800 -0.67510100\nO    2.89125300 -1.69896600 -1.06351900\nO    1.12976700 -0.31428900  0.37859300\nH    2.99568600 -1.73945400 -2.01677200\nH    3.39746100 -2.42787400 -0.69708600\nO   -2.89123000 -1.69896400  1.06353700\nH   -2.99563400 -1.73945600  2.01679300\nH   -2.43456700  2.07972500 -2.17761600\nH   -2.58174600  2.66131900 -0.75942800\nH   -3.39743400 -2.42788000  0.69711700\n</code></pre> <p>The input file requests geometry optimization of 5 water molecules with a very large basis set <code>aug-cc-pVTZ</code>. It will take about 25 CPU hours to finish the whole calculation. We have the setting on <code>%RWF</code> which specifies <code>water.rwf</code> file for the checkpointing function besides the <code>water.chk</code> file. Since the specification <code>%RWF</code> is placed before the <code>%NoSave</code> line, the rwf file will be deleted if the calculation is normally completed without any error.</p> <p>In order to have several restarts running after the first run stops, we can build a restart Gaussian input file <code>restart.gjf</code> simply as</p> <p>restart.gjf</p> <pre><code>%NProcShared=2\n%Mem=3GB\n%RWF=waters.rwf\n%NoSave\n%chk=waters.chk\n#P Restart\n</code></pre> <p>Since all information about the calculation is recorded in the rwf file, a line with \"Restart\" is enough for Gaussian to restart from the previous job. This restart input file can also be created by the commands:</p> <pre><code>grep '^%' waters.gjf &gt; restart.gjf\necho -e '#P Restart\\n' &gt;&gt; restart.gjf\n</code></pre> <p>where we simply \"<code>grep</code>\" the lines starting with \"<code>%</code>\" sign in <code>water.gjf</code> and put them in the Gaussian restart file with \"<code>#P Restart</code>\" line in the end.</p> <p>Now we need a job script to submit the Gaussian calculation. The script needs to keep submitting jobs to restart the previous calculation until it is completed. Here is a job script <code>water.sb</code> which can do the work:</p> <p>water.sb</p> <pre><code>#SBATCH \u2013-job-name=LongJob\n#SBATCH \u2013-ntasks=1\n#SBATCH \u2013-cpus-per-task=2\n#SBATCH --mem=5G\n#SBATCH \u2013-time=04:00:00\n\nmodule load Gaussian/g16 powertools\nOutputFile=\"water-${SLURM_JOBID}.log\"             # Gaussian output file name for each job\n\n# How many seconds before end of job to submit another\nBeforeEnd=300                                       # 5 minutes\n\n# The background script to keep job submission until calculation is completed\n(sleep $((4*60*60 - BeforeEnd))                     # sleep until the time before end of job\njs -j ${SLURM_JOBID}                                # print out resource usage\ncat ${OutputFile} &gt;&gt; water.log                      # collect Gaussian outputs into one file\necho -e \"\\n\\n====== Gaussian calculation on job ${SLURM_JOBID} stops. ======\\n\\n\" &gt;&gt; water.log\necho \"The Gaussian calculation has not completed. Submit another job to keep doing it.\"\nsbatch water.sb                                     # submit another job\nscancel ${SLURM_JOBID}  )&amp;                          # job stops if g16 command is not finished\n\n# Whether this is a restart job or not\nif [ -f water.rwf ] &amp;&amp; [ -f water.chk ]; then\n   InputFile=\"restart.gjf\"\nelse\n   InputFile=\"water.gjf\"\nfi\n\ng16 &lt; ${InputFile} &gt; ${OutputFile}\n\n# The following commands are not executed unless g16 command is completed.\n# Print out resource usage \njs -j $SLURM_JOB_ID           ### powetools command\n\ncat ${OutputFile} &gt;&gt; water.log \necho -e \"\\n\\n====== Gaussian calculation is completed on job ${SLURM_JOBID}. ======\\n\\n\" &gt;&gt; water.log\n</code></pre> <p>where a background script in <code>(---)&amp;</code> from line 14 to 20 is added to keep submitting job<code>s</code>.</p> <p>Once the job is started, the background script is running at the same time as the foreground script. The background script is in sleep for 3 hours and 55 minutes first. During this time, the foreground script runs the Gaussian calculation or restarts the previous calculation if the checkpointing files <code>water.rwf</code> and <code>water.chk</code> exist. After 5 minutes before the end of the job, the background is awake to print out the resource usage and Gaussian output. It submits another job and stops the current running job in line 19 and 20 if the g16 command in line 29 is not completed. If the g16 command is finished before the background script is awake, the job will keep executing all command lines after line 30 and finish. There will be no more jobs submitted.</p> <p>Since the rwf file usually takes a lot of file space, it is suggested to run checkpointing jobs in scratch space in case your home or research space is over quota. Users can create a directory in their scratch space. Copy all files (<code>water,gjf</code>, <code>restart.gjf</code> and <code>water.sb</code>) and submit the job script there. Please check your job status frequently. Make sure to copy necessary files back to your home or research directory from time to time since files on scratch will be purged if they have not been modified for 45 days.</p> <p>Note</p> <p>The time for running the background script needs to be longer than the time needed for a cycle of Gaussian analysis to avoid restarting from the point of privious run again. The checkpointing is done between cycles.  </p>"},{"location":"Gaussian_on_HPCC/","title":"Gaussian on HPCC","text":"<p>Here is the slides pdf for the Gaussian workshop.</p>"},{"location":"Getting_Started_with_Grace_Hopper_and_Grace_Grace/","title":"Getting started with Grace Hopper and Grace Grace","text":"<p>ICER has received four NVIDIA Grace Hopper systems that MSU researchers have purchased as well as one NVIDIA Grace Grace CPU system.</p> <p>All nodes are available in SLURM and are available to the research community under the same buy-in rules as the rest of our buy-in hardware. In particular, users that are not part of a node's buy-in are restricted to submitting jobs less than four hours.</p>"},{"location":"Getting_Started_with_Grace_Hopper_and_Grace_Grace/#node-listing","title":"Node listing","text":"Cluster Type Node Count Processors Cores Memory Disk Size GPUs (Number) Node Name Grace Hopper 3 Grace CPU (Arm Neoverse v2) 72 480 GB 1.5TB GH200 96 GB (1) <code>nch-[000-002]</code> Grace Hopper 1 Grace CPU (Arm Neoverse v2) 72 480 GB 3.2TB GH200 96 GB (1) <code>nch-003</code> Grace Grace 1 Grace CPU (Arm Neoverse v2) 144 480 GB 3.5TB <code>ncc-000</code>"},{"location":"Getting_Started_with_Grace_Hopper_and_Grace_Grace/#differences-from-other-nodes","title":"Differences from other nodes","text":"<p>The Grace systems are currently considered \"beta\" with very minimal installations. Users should be aware:</p> <ol> <li>These are ARM-based (<code>aarch64</code>) systems. Existing code compiled for our <code>x86_64</code> nodes (including conda environments) will not work on this system. To see if an executable is compiled, use <code>file executablename</code>. If it mentions <code>x86_64</code> it is not compatible with the Grace Hopper systems.</li> <li>These nodes are currently running Ubuntu 22.04, instead of our default CentOS 7.9 OS. Libraries and tools provided by the OS (including <code>libc</code> and <code>gcc</code>) are going to be newer versions.  </li> <li>These nodes have slower-than-normal home directory and research space access. Researchers may wish to stage data or code to the node in <code>/tmp</code>. </li> <li>These nodes do not have access to global scratch (<code>gs21</code>) due to a Storage Scale compatibility issue. We are investigating.</li> <li> <p>Software pre-installed by ICER is different than what is available on other nodes. To access the <code>module</code> command and all software currently built for the Grace Hopper systems, run </p> <pre><code>source /cvmfs/software.eessi.io/versions/2023.06/init/bash &amp;&amp; module use /opt/modules/all\n</code></pre> <p>Please note that you may see references to modules on the main cluster being inactive, as SLURM copies these references over before the line above deactivates them. These modules are non-functional on the Grace nodes.</p> </li> <li> <p>Before running code that has been compiled on the Grace nodes, you need to load the <code>Compiled</code> module after all other dependent modules. For example, assuming that a code is compiled using the <code>GCC/12.2.0</code>, <code>Cabana/0.6.1-foss-2022b-CUDA-12.1.1</code>, and <code>CUDA/12.1.1</code> modules, you should use the following lines to run your code:</p> <pre><code>module purge\nmodule load GCC/12.2.0 Cabana/0.6.1-foss-2022b-CUDA-12.1.1 CUDA/12.1.1\nmodule load Compiled\n\n./my-compiled-code\n</code></pre> </li> </ol> <p>We will update this page as we address these issues.</p>"},{"location":"Getting_Started_with_Grace_Hopper_and_Grace_Grace/#submitting-jobs","title":"Submitting jobs","text":"<p>The primary mechanism to schedule jobs on Grace nodes is to use the SLURM option <code>--constraint NOAUTO:grace</code>.</p>"},{"location":"Getting_Started_with_Grace_Hopper_and_Grace_Grace/#interactive-jobs","title":"Interactive jobs","text":"<p>To start an interactive job, use the template</p> <pre><code>salloc --constraint=NOAUTO:grace --time=3:00:00 --gpus=1 --cpus-per-task=72 --mem=10G\n</code></pre> <p>If you would like to avoid using your default shell environment (due to any of the potential incompatibilities, see above), you should use <code>srun</code> with option <code>--pty</code> and command <code>/bin/bash</code>,  e.g.,</p> <pre><code>srun --constraint=NOAUTO:grace --time=3:00:00 --gpus=1 --cpus-per-task=72 --mem=10G --pty /bin/bash\n</code></pre> <p>If you have buy-in access to a Grace node, you should additionally add the option <code>--account=&lt;buy-in-name&gt;</code>. By using the <code>--gpus:1</code> option, jobs are restricted to only running on Grace Hopper nodes. Removing this options allows jobs to run on any Grace node.</p>"},{"location":"Getting_Started_with_Grace_Hopper_and_Grace_Grace/#job-script-template","title":"Job script template","text":"grace_hopper_template.sb<pre><code>#!/bin/bash\n\n#SBATCH --constraint=NOAUTO:grace  # Only run on Grace nodes\n#SBATCH --time=3:00:00             # Run for three hours\n#SBATCH --gpus=1                   # Request one GPU (restricts to Grace Hopper)\n#SBATCH --cpus-per-task=72         # Request all CPUs on a Grace Hopper node \n#SBATCH --mem=10GB                 # Request 10GB of (non-GPU) memory\n\n# Gain access to the module system\nsource /cvmfs/software.eessi.io/versions/2023.06/init/bash &amp;&amp; module use /opt/modules/all\n\n# Load modules\nmodule load CUDA-Samples\n\n# Run code (GPU examples from CUDA-Samples)\nmatrixMul\nmatrixMulCUBLAS\n\n# Output debugging information\nscontrol show job $SLURM_JOB_ID\n</code></pre>"},{"location":"Getting_Started_with_Grace_Hopper_and_Grace_Grace/#see-also","title":"See also","text":"<p>Users may wish to refer to NVIDIA's documentation:</p> <ul> <li>NVIDIA Grace Docs </li> <li>NVIDIA Grace Hopper Docs</li> </ul>"},{"location":"Guidelines_for_Choosing_File_Storage_and_I_O/","title":"Guidelines for Choosing File Storage and I/O","text":"<p>HOME, RESEARCH and SCRATCH  are referred to as networked file systems.\u00a0Each node must go through the network switch to access these spaces. The LOCAL storage options at <code>/tmp</code> and <code>/mnt/local</code> are locally accessible in the hard drive of each node and are not affected by the network. All of these are larger than RAMDISK (<code>/dev/shm</code>) which is located inside the node\u2019s RAM. However, RAMDISK is the closest (and therefore fastest) storage location for files. Files stored here take up some of the node\u2019s memory space and are counted when Slurm calculates the memory a job is using.</p> <p>The table below provides detailed information about each type of storage\u00a0on HPCC. (<code>$USER</code> is your login username and GROUP is your research group name). Please use the table below to choose which file system is best for your job.\u00a0The HOME and RESEARCH systems are the  only systems with automatic offsite disaster recovery protection. The SCRATCH, LOCAL, and RAMDISK systems all have automatic purge policies. </p> <p>The \"nodr portion of HOME/RESEARCH\" column is similar to the \"HOME\" or \"RESEARCH\" columns, but refers to the portion of those directories that have been requested to move to <code>nodr</code> space. Note that this space is NOT protected by automatic DR protection! See the Home space or Research space pages for more information.</p> HOME RESEARCH nodr portion of HOME/RESEARCH SCRATCH LOCAL RAMDISK Primary Private files or data storage for each user Shared files or data storage for group users same as the standard HOME/RESEARCH Temporary large files or data storage for users and groups Temporary small files or data usage for job running same as LOCAL with very fast I/O Access location Automatic login <code>$HOME</code>\u00a0 or\u00a0<code>/mnt/home/$USER</code> <code>/mnt/research/GROUP</code> <code>/mnt/ufs18/nodr</code> <code>$SCRATCH</code> or <code>/mnt/scratch/$USER</code> <code>/mnt/local</code> or <code>/tmp</code> (at each node) <code>$TMPDIR</code> (used in a job as <code>/tmp/local/$SLURM_JOBID</code>) <code>/dev/shm</code> (at each node) Size 50GB upto 1TB, 1 million files, ($90/year for each additional TB) 50GB upto 1TB, 1 million files, ($90/year for each additional TB) as a portion of HOME or RESEARCH by user's request. No limit on the number of files 50TB and 1 million files ~400GB for intel14, ~170GB for intel16, ~400GB for intel18 Note: userIDs are restricted from consuming no more than 95% of the total available space in <code>/tmp</code> \u00bd of RAM I/O Best Practice low I/O using\u00a0 single or multiple nodes Same as HOME same as HOME or RESEARCH heavy I/O on files of large size using single or multiple nodes frequent I/O operations on many files in one node frequent and fast I/O operations on small files in one node Careful with Watch for quota. Avoid heavy parallel I/O. Same as HOME. In addition, need to set umask or file permission so files can be shared in group. Be aware of no automatic DR protection.\u00a0 Avoid frequent I/O on many small files(&lt; 1MB), such as untarring a tar file to create many small files in a short time. Move files to HOME or RESEARCH before purge period elapses. Need to copy or move files to HOME or RESEARCH before job completes. Only local access available. Users are not able to store files in one node and gain I/O access to them from other nodes. Same as LOCAL. Request extra memory in your job script so you'll have enough space for file storage. Command to check quota <code>quota</code> <code>quota</code> <code>quota</code> <code>quota</code> <code>#SBATCH --tmp=20gb</code> to reserve 20gb in <code>$TMPDIR</code>. \u00bd of the memory requested by job Disaster Recovery Yes Yes No No No No Purge policy No No No Yes. (Files not accessed or modified for more than 45 days may be removed) Yes (at completion of job) Yes (RAM may be reused by other jobs)","tags":["reference","files","I/O"]},{"location":"HPCC-Job-Submission-Workflow_40337480.html/","title":"HPCC Job Submission Workflow","text":"<p>Goal: Understand basic workflow for submitting jobs to the clusters.</p> <p>Task: Run hello.c on one core and one compute node.</p> <ul> <li>Login to HPCC</li> <li>Load the powertools module.</li> <li>Create a copy the helloworld getexample in your current directory.</li> <li>Change into the helloworld directory created.</li> <li>View the submission script in the helloworld directory.</li> <li>Compile the hello C program on the development node.</li> <li>Submit the script to the SLURM scheduler.</li> <li>Check the queue for your job.</li> <li>Examine your job</li> <li>Check the output of your job</li> <li>View the output of your job</li> </ul> <p>Answer \u00a0Expand source</p> <pre><code>#Login to HPCC\nssh -XY msu_netid@hpcc.msu.edu\nssh dev-intel18\n\n#Load powertools module\nmodule load powertools\n\n#Copy the helloworld getexample\ngetexample helloworld\nls -l\n\n#Change\u00a0into the helloworld directory\ncd helloworld\nls -l\n\n#View the submission script in the helloworld directory\ncat hello.sb\n\n#Compile the C program &lt;hello&gt; on the development node\ngcc hello.c -o hello\n\n#Submit the jobscript hello.sb to the SLURM scheduler\nsbatch hello.sb\n\n#Check the queue for your job\nqstat \u2013u $USER\n\n#Examine your job\nscontrol &lt;job_id&gt;\n\n#View the output of your job\nless &lt;slurm-jobid.out&gt;\n</code></pre>"},{"location":"HPC_Glossary/","title":"HPC Glossary","text":"<p>Program \u2013 code stored on a computer intended to fulfill a certain task</p> <ul> <li>There are many types of programs:<ul> <li>Part of the operating system and help computer function</li> <li>Fulfill a particular job are called applications</li> </ul> </li> <li>Typically stored on disk (g., hard drive)</li> <li>A program needs memory and various operating system resources     (g., peripheral interfaces) to run</li> </ul> <p>Operating System \u2013 manages all resources needed for a program (e.g., macOS)</p> <p>Process \u2013 program with all necessary resources loaded into memory</p> <ul> <li>When a program is run, it is loaded into memory which makes it     accessible for processing by the computer\u2019s central processing unit     (CPU)</li> <li>There can be multiple instances of a single program, and each     instance of that running program is a process</li> <li>Each process has a separate memory address space, which means that a     process runs independently and is isolated from other processes</li> <li>This independence of processes is valuable so that a problem with     one process cannot cause havoc with another process</li> </ul> <p>Central processing unit (CPU) - logical hardware unit capable of executing a single process (i.e., gets instructions then performs calculations)</p> <ul> <li>Made up of:<ul> <li>Processor is a device that processes program instructions to     manipulate data</li> <li>Socket is an array of pins that connect processor to     motherboard</li> </ul> </li> <li>Individual CPU processors now contain multiple cores for more     efficient multi-tasking and parallel computing<ul> <li>Core is the smallest hardware unit capable of performing a     processing task</li> <li>Ex: dual-core processor has two cores</li> </ul> </li> </ul> <p></p> <p></p> <p>Thread (of execution) is the smallest set of programmed instructions that can be managed independently by an operation system. In general, one thread is handled by one core.</p> <p>As video gaming popularity increased, so did the need for more computing power. To accomplish this a CPU can work with a graphics processing unit (GPU), usually found on a graphics card docked into the motherboard, to quickly render high-resolution images and video concurrently. A GPU gets its intense computing power from hundreds of smaller cores capable of crunch application data in parallel. Multiple GPUs can be installed on one graphics card or multiple graphics card can be installed in one node to further improve computation power through parallelism. After GPUs became popular for gaming, they were made fully programmable to be useful in processing big science data. The resulting general-purpose graphics processing unit (GPGPU) is used extensively in supercomputing to increase speed and improve analysis of scientific data.</p> <p>Node is a single computer comprised of 1+ CPUs, memory, network interfaces, etc.</p> <p>Cluster is a group of nodes networked together so that a program can run on them in parallel</p> <p>Parallel computing is an umbrella term describing the use of multiple computers or computers made up of multiple processors in combination to solve a single problem</p> <p>Within HPCC there are different types of nodes:</p> <ul> <li>Gateway nodes are nodes used to enter the computer system:<ul> <li>Login - login and non-intensive compute tasks (e., moving     files)</li> <li>rsync - data transfer to/from HPCC</li> </ul> </li> <li>Development (dev) nodes are nodes used to navigate file systems     and compile, test, and schedule heavy computational tasks (e.,     jobs)</li> <li>Compute nodes are clusters that perform scheduled jobs</li> <li>Accelerator nodes are nodes equipped with accelerated cards     (g., GPU or phi nodes)</li> </ul> <p>Secure Shell (SSH) - network protocols and implementing suite of utilities that provide a secure way to access and execute commands on a remote computer over an unsecured network</p> <p>Remote synchronization (rsync) - software utility for Linux systems that efficiently sync files and directories between two hosts or machines making ideal for transferring large files</p> <p>File system \u2013 tree-like directory organization for storing many files</p>"},{"location":"HPC_Glossary/#for-more-information-on-these-terms-check-out-the-following-videos","title":"For more information on these terms, check out the following videos:","text":"<p>What is ICER\u2019s HPCC (11min)</p> <p>HPCC System Layout (7min)</p>"},{"location":"HPC_s_entire_layout_at_ICER/","title":"The HPCC's Layout","text":"<p>The HPCC is comprised of three different kinds of nodes: the \"gateway\" and \"rsync\" entry nodes, development nodes, and compute nodes. In a typical workflow, users will connect to the HPCC through the an entry node, then connect to a development node to compile and test code before submitting jobs to the SLURM queue to be run on a compute node. This workflow is demonstrated in the diagram below. </p> <p>Each node type is explained in more detail in the following sections. Information on the HPCC's filesystems is available within a separate section.</p> <p></p>","tags":["explanation"]},{"location":"HPC_s_entire_layout_at_ICER/#entry-nodes","title":"Entry Nodes","text":"<p>The gateway and rsync nodes are the only nodes directly accessible over the internet. Users connect to these nodes from their personal computers using <code>ssh</code> before accessing other parts of the HPCC.</p>","tags":["explanation"]},{"location":"HPC_s_entire_layout_at_ICER/#gateway-nodes","title":"Gateway Nodes","text":"<p>These nodes are the default accessed via <code>ssh &lt;username&gt;@hpcc.msu.edu</code> as in the top fork of the diagram above. The gateway nodes are not meant for compilig or running software, accessing the scratch space, or connecting to compute nodes. Users should only use the gateway nodes to <code>ssh</code> to development nodes. Alternatively, users may set up SSH tunneling to automate the process of passing through the gateway to a development node.</p>","tags":["explanation"]},{"location":"HPC_s_entire_layout_at_ICER/#rsync-nodes","title":"Rsync Nodes","text":"<p>These nodes are accessed via <code>ssh &lt;username&gt;@rsync.hpcc.msu.edu</code> as in the bottom fork of the diagram above. Unlike the gateway nodes, the scratch file system is accessbile from the rsync nodes. This is because these nodes are primarily intended for file transfer. Large amounts of data should be transferred via the rsync nodes to avoid slowing down the gateway nodes.</p> <p>These nodes are named for the popular command line file transfer utility <code>rsync</code>. Users can use this utility to transfer files via the rsync gateway by following this command pattern: <code>rsync &lt;local path&gt; &lt;username&gt;@rsync.hpcc.msu.edu:&lt;remote path&gt;</code>. Other commands such as <code>scp</code> may also be used with the rsync gateway, or a GUI such as MobaXterm may be used instead.</p>","tags":["explanation"]},{"location":"HPC_s_entire_layout_at_ICER/#development-nodes","title":"Development Nodes","text":"<p>From the gateway node, users can connect to any development node to compile their jobs or and run short tests. They may also access files on the scratch file system. Jobs on the development nodes are limited to two hours of CPU time. More information is available on the development node page.</p> <p>Each development node is configured to match the compute nodes of the same cluster. If you would like your job to be able to run on any cluster (as is the default for the queue; see the section on Automatic Job Constraints) you should not compile with  architecture-specific tuning (e.g. <code>-march</code> or <code>-x</code>).</p> <p>Warning</p> <p>Code compiled on older development nodes (dev-intel14 and dev-intel14-k20) may have errors when running on the latest clusters due to an outdated instruction set. To avoid this, compile your code on a newer development node or specify <code>--constraint=intel14</code> in your SLURM batch script.</p>","tags":["explanation"]},{"location":"HPC_s_entire_layout_at_ICER/#compute-nodes","title":"Compute Nodes","text":"<p>ICER maintains several clusters worth of compute nodes. Users submit jobs to the SLURM scheduler which assigns compute nodes based on the resources requested. </p> <p>A user may see which nodes their job is running on using <code>squeue -u &lt;username&gt;</code>. Not providing a username to <code>squeue</code> will show all jobs currently running on the system. Users may <code>ssh</code> directly to a compute node only if they have a job running on that node. See our page on connecting to compute nodes for more.</p>","tags":["explanation"]},{"location":"HPC_s_entire_layout_at_ICER/#comparison-to-a-personal-computer","title":"Comparison to a personal computer","text":"Laptop/Desktop HPCC Clusters Number of Nodes 1 979 Sockets per node 1 2, 8 Cores per node 4, 8, or 16 20, 28, 40, 128 or 144 Cores total 4, 8, or 16 50,084 Core Speed 2.7 - 3.5 ghz 2.5-3 ghz RAM memory 8, 16 or 32 GB 64, 128, 92, 500 GB or 6TB File Storage 250, 500 GB or 1TB 1TB(Home), 50TB(Scratch) Connection to other computers Campus ethernet1 Gbit/sec \"Infiniband\" 100 Gbit/sec Users 1 ~2,000 Schedule On Demand 24/7 via queue","tags":["explanation"]},{"location":"HTSeq/","title":"HTSeq","text":"<p>HTSeq is a Python package for analysis of high-throughput sequencing data. It's available upon a fresh login to a dev-node. For example, you can call command <code>htseq-count</code> directly without needing to load specific modules. This is because HTSeq is installed to the default Python distribution (you can run module list\u00a0to see the list of modules loaded by default).</p> <p>If you happen to run a module purge\u00a0command in the middle of your work, you need to load the default Python back again in order to use HTSeq. To do so:</p> <pre><code>module purge\nmodule load GCC/6.4.0-2.28 OpenMPI/2.1.2 Python/3.6.4\n</code></pre> <p>Tip: if you want to go back to the original shell environment immediately after your login, you can run: <code>exec bash -l</code></p>"},{"location":"Home_Space/","title":"Home Space","text":"<p>Each user account is given a home space for personal file storage located at <code>/mnt/home/$USER</code>, where <code>$USER</code> is the environment variable of the user's login name. Alternatively, it can be accessed at <code>~/</code>, though this shorthand may not work in some scripts. By default,\u00a0it is only accessible to the user.  It is often referred to as the \u201chome directory\u201d since this is the beginning directory after login of any HPCC node.</p> <p>Every home space starts with a 50 GB limit for file storage space and can not contain more than 1 million files. To check the quota and used space of your home directory, see the Space quota section.  You can request to increase your quota up to 1TB by completing Quota Increase Request form.  Storage space greater than 1TB is available for an annual fee paid through a MSU financial account. Users can find the fee and submit their request by completing the Large Quota Increase Request form.   If you would like to store more than 1 million files in your home space, please refer to the section Limit on number of files.</p> <p>All home directories are stored in the IBM General Parallel File System (GPFS). It is automatically backed up except files saved in the <code>nodr</code> space. To restore any file from backup, please submit a ticket and let us know the paths to the files or the directory with the time frame you would like them restored.</p> <p>For the system security and user data privacy, we recommend that users do NOT open home directory access permission to others. When you report an issue about files saved in home, please attach them to your message for reference. ICER staff cannot access any files or directories in your home directory.</p> <p>Warning</p> <p>Currently our home file system check quota function will sometimes cause a user's directory\u00a0over the quota due to incorrect calculation of used space.\u00a0 If you see this please open a ticket and we will work with you to temporarily increase your quota.\u00a0 We continue to work with our vendor to correct this issue.</p>","tags":["explanation","quota","files"]},{"location":"Home_Space/#space-quota","title":"Space quota","text":"<p>The only way to get quota information of home space is to run the command <code>quota</code>:</p> <pre><code>$ quota\nhome directory: Space    Space   Space     Space     Files     Files     Files     Files        \n                 Quota    Used    Available % Used    Quota     Used      Available % Used\n-----------------------------------------------------------------------------------------------\n/mnt/home/$USER  50G      32G     18G       64%       1048576   432525    616051    59%\n</code></pre> <p>where all file spaces accessible to the user are listed, including home, research, and scratch. In each space, the information of quota, usage and availability on space size and number of files can be found. If \"Free\" or \"Available\" is a negative value (such as the \"Space Available\" column in the above example), the usage is over the quota, please remove, transfer or compress some files so the used space or the file count can be lower than the \"Quota\" value.</p>","tags":["explanation","quota","files"]},{"location":"Home_Space/#actual-disk-usage-du-different-from-quota-results","title":"Actual disk usage (<code>du</code>) different from quota results","text":"<p>The GPFS system set the smallest file block size to 64KiB. This means that files between 2KiB and 64KiB will occupy 64KiB of space. This causes space usage to be greatly inflated for users with large amounts of small files. One suggested solution would be to compress many small files into one large file (larger than 64KiB). In this way, the number of files can also be reduced. If you still have any difficulty, a temporarily larger quota can be requested if your quota is at 1TB with many small files.</p>","tags":["explanation","quota","files"]},{"location":"Home_Space/#limit-on-number-of-files","title":"Limit on number of files","text":"<p>Besides the quota on the size of space, users are also limited to 1 million files in their home or research directory. We need to set this limit because, with a great number of files, the file system will spend too much time on backups to be able to function normally. If possible, users can compress many files into one to reduce the file number.</p> <p>If users do not wish to have the limit, they can request to have a portion of their home quota (or research space) moved under <code>nodr</code> space (<code>/mnt/ufs18/nodr/</code> or <code>/mnt/ufs18/nodr/research/</code>) by submitting a  help ticket  with the drop-down subject 'other' and secondary subject 'nodr request'.  There is no limit on the file count, but there is no backup of these files either.  Users will be responsible for their own backup of files in the <code>nodr</code> space.</p> <p>By default, one half of the current home quota is assigned to the requested <code>nodr</code> space. The original quota under <code>/mnt/home/</code> (or <code>/mnt/research/</code>) is then reduced to half the original quota so the total space quota remains the same. A different size of <code>nodr</code> space can also be assigned based on the user's request. Once this space is created, the path and the quota information on both home and nodr space  can be found using <code>quota</code> command mentioned above.</p>","tags":["explanation","quota","files"]},{"location":"Home_Space/#backups","title":"Backups","text":"<p>All home space files are periodically, automatically backed up (except  those files that a user has opted to store in a specially requested  <code>nodr</code> space).\u00a0To access file backups, please submit a  help ticket containing the file paths and the period, i.e. the time frame, from  which the files should be restored.  </p>","tags":["explanation","quota","files"]},{"location":"How_Jobs_are_Scheduled/","title":"How Jobs are Scheduled","text":"","tags":["reference","slurm"]},{"location":"How_Jobs_are_Scheduled/#schedulers","title":"Schedulers","text":"<p>SLURM schedules jobs in two ways: the main scheduler and the backfill scheduler. The main scheduler constantly tries to start high priority jobs. The backfill scheduler considers all jobs, and starts any jobs that won't defer the start time of a higher priority job.</p> Scheduler Function When it Runs Run Time Main Launches high priority jobs that can start immediately. Stops evaluating jobs once it encounters a job that cannot be started.\u00a0 About every 2 seconds 0.08-2 seconds Backfill Evaluates the entire queue. Launches jobs that won't interfere with the start time of a higher priority job. Sets jobs'\u00a0StartTime and SchedNodeList. 20 seconds after the last backfill cycle completes 2-15+ minutes","tags":["reference","slurm"]},{"location":"How_Jobs_are_Scheduled/#starttime-and-schednodelist","title":"StartTime and SchedNodeList","text":"<p>The backfill scheduler sets the StartTime and SchedNodeList parameters on jobs that can start within the next 7 days. These parameters can be viewed in the output of <code>scontrol show job &lt;jobid&gt;</code>. StartTime estimates when a job will start and SchedNodeList shows the nodes this job might start on. StartTime is only an estimate. These values are updated every time the backfill scheduler runs and may change as running jobs complete and new jobs are submitted.</p>","tags":["reference","slurm"]},{"location":"How_Jobs_are_Scheduled/#minimum-job-requirements-to-avoid-deferment","title":"Minimum Job Requirements to Avoid Deferment","text":"<p>Jobs must meet certain criteria before the backfill scheduler will avoid potentially deferring them through starting lower priority jobs. These thresholds allow the backfill scheduler to cycle faster and maintain high system utilization.</p> Criteria Minimum Description Priority 3000 Jobs require a minimum priority of 3000 is require to avoid potential deferment in scheduling. Buy-in account jobs are never below this threshold. Age 30 minutes Jobs must be queued for at least 30 minutes to avoid potential deferment in scheduling. This applies to all jobs.","tags":["reference","slurm"]},{"location":"How_Jobs_are_Scheduled/#job-priority-factors","title":"Job Priority Factors","text":"<p>A job's priority is determined by a combination of several priority factors. Age, size, fairshare, and whether it was submitted to a buy-in account all contribute to the job\u2019s priority.</p> Priority Factor Description Maximum Contribution to Priority Age Starts at zero at job submission, then increases linearly to a maximum of 60000 after 30 days 60000 after 30 days Fairshare Starts at 60000 and decreases and users' recent usage goes up. Usage for this calculation is decayed 50% each day 60000 for no recent cluster usage Size Scales linearly with the amount of CPU and memory requested by a job. 100 per CPU, 20 per GB. 52000+ depending on memory requested QOS Adds 3000 to buy-in jobs to ensure they are always above backfill schedulers minimum priority for reserving resources 3000","tags":["reference","slurm"]},{"location":"How_to_find_and_load_software_modules/","title":"Searching software modules","text":"<p>This page contains a basic overview of common interactions with the module system. For a guided tutorial, please see our Introduction to the Module System.</p>","tags":["reference"]},{"location":"How_to_find_and_load_software_modules/#general-search-using-module-spider","title":"General search using <code>module spider</code>","text":"<p>To search for a particular software module (e.g. \"ABC\"), you would run</p> <pre><code>module spider ABC # can also be abc, ABc...\n</code></pre> <p>Once you find it, and want to load a specific version (say 1.1.1), run</p> <pre><code>module spider ABC/1.1.1 # should only be ABC\n</code></pre> <p>The resulting output information will tell you what prerequisites modules are needed before loading your <code>ABC/1.1.1</code>. </p>","tags":["reference"]},{"location":"How_to_find_and_load_software_modules/#searching-with-a-partial-name","title":"Searching with a partial name","text":"<p>You don't need to know the full name of the software. For example, let's search for all modules related to VCF:</p> <pre><code>$ module spider vcf\n\n------------------------\n  VCFtools:\n------------------------\n    Description:\n      The aim of VCFtools...\n\n     Versions:\n        VCFtools/0.1.15-Perl-5.26.0\n        VCFtools/0.1.15-Perl-5.26.1\n        VCFtools/0.1.15-Perl-5.28.0\n\n------------------------\n  For detailed information...\n\n     $ module spider VCFtools/0.1.15-Perl-5.28.0\n------------------------\n\n------------------------\n  vcflib: vcflib/1.0.2\n------------------------\n    Description:\n      vcflib provides...\n\n\n    You will need to load all module(s) on any one of the lines below before the \"vcflib/1.0.2\" module is available to load.\n\n      GCC/10.2.0\n\n    Help:\n      ...\n</code></pre>","tags":["reference"]},{"location":"How_to_find_and_load_software_modules/#other-ways-to-search","title":"Other ways to search","text":"<p>The output of <code>module spider</code> may also suggest other ways to search for modules.</p> <p>See the example for PCRE below, which suggests both \"other possible module matches\" and \"other possible module matches.\"</p> <pre><code>$ module spider PCRE\n\n--------------------------\n  PCRE:\n--------------------------\n    Description:\n      The PCRE library...\n\n     Versions:\n        PCRE/8.38\n        ...\n        PCRE/8.41\n     Other possible modules matches:\n        PCRE2\n\n--------------------------\n  To find other possible module matches execute:\n\n      $ module -r spider '.*PCRE.*'\n\n--------------------------\n</code></pre>","tags":["reference"]},{"location":"How_to_find_and_load_software_modules/#loading-a-specific-version","title":"Loading a specific version","text":"<p>In this example, we want to load HDF5 version 1.10.7. To learn how to load this module, run:</p> <pre><code>$ module spider HDF5/1.10.7\n\n--------------------------\n  HDF5: HDF5/1.10.7\n--------------------------\n    Description:\n      HDF5 is...\n\n\n    You will need to load all module(s) on any one of the lines below before the \"HDF5/1.10.7\" module is available to load.\n\n      GCC/10.2.0  CUDA/11.1.1  OpenMPI/4.0.5\n      GCC/10.2.0  OpenMPI/4.0.5\n      GCC/10.3.0  OpenMPI/4.1.1\n      iccifort/2020.1.217  impi/2019.7.217\n      iccifort/2020.4.304  impi/2019.9.304\n</code></pre> <p>Each line of the above about gives a different set of dependencies. Let's pick the third line, which requires GCC version 10.3.0 and OpenMPI version 4.1.1.</p> <p>To load HDF5 1.10.7, run</p> <pre><code>module purge # a MUST-HAVE\nmodule load GCC/10.3.0 OpenMPI/4.1.1\nmodule load HDF5/1.10.7\n</code></pre> <p>Note that <code>module purge</code> is always needed before you start loading your own modules. This command will clear the default modules and prevent version conflicts.</p>","tags":["reference"]},{"location":"How_to_find_and_load_software_modules/#saving-and-restoring-module-sets","title":"Saving and restoring module sets","text":"<p>Your currently loaded modules can be saved for easy access at a later time.</p> <p>To save your modules, run <pre><code>module save &lt;collection_name&gt;\n</code></pre> where <code>&lt;collection_name&gt;</code> is replaced with your desired name.</p> <p>You can then re-load this collection of modules with <pre><code>module restore &lt;collection_name&gt;\n</code></pre></p> <p>To see all saved collections, use <pre><code>module savelist\n</code></pre></p> <p>To see the contents of a collection, use <pre><code>module describe &lt;collection_name&gt;\n</code></pre></p> <p>A saved collection can be removed with <pre><code>module disable &lt;collection_name&gt;\n</code></pre></p>","tags":["reference"]},{"location":"How_to_find_and_load_software_modules/#advanced-skill-searching-for-modules-in-module-file-hierarchy-using-module-avail","title":"Advanced skill: Searching for modules in module file hierarchy using \"module avail\"","text":"<p>If you start with a minimum set of loaded modules (most commonly a compiler-MPI pair, or a compiler alone), and want to know what software packages are available to load in the current <code>MODULEPATH</code> (run <code>echo $MODULEPATH</code> to see the paths), you can run <code>module avail</code>. </p> <p><code>module avail</code> is different from <code>module spider</code> described above. The <code>module spider</code> command searches all possible modules, while <code>module avail</code> only shows those modules which can be seen in the current <code>MODULEPATH</code>. </p> <p>To check the availability of a particular module, use <code>module avail &lt;keyword&gt;</code>. If the keyword (such as \"openmpi\") is long and distinct, the search output will normally be clean. However, for modules like \"R\" whose name is a single letter that can appear in almost any module names, we need to use regular expression to fully specify the pattern when running <code>module avail</code>. See below:</p> <pre><code>$ module purge\n$ module load GCC/8.3.0 OpenMPI/3.1.4\n$ module avail # a large amount of output will be printed on your screen and they are omitted here.\n\n# Now we search for available R versions under the current GCC-OpenMPI pair.\n$ module -r avail '^R$'\n--------------------------------------- /opt/modules/MPI/GCC/8.3.0/OpenMPI/3.1.4 ----------------------------\n   R/3.6.2    R/3.6.3    R/4.0.2.bak    R/4.0.2.test    R/4.0.2    R/4.1.0 (D)\n\n  Where:\n   D:  Default Module\n</code></pre>","tags":["reference"]},{"location":"How_to_find_and_load_software_modules/#troubleshooting","title":"Troubleshooting","text":"<p>Sometimes, <code>module spider</code> doesn't work because your personal module cache is out of date. To clear it, do <code>rm -r ~/.lmod.d/.cache</code></p>","tags":["reference"]},{"location":"ICER_HPC_Classroom_Support/","title":"Classroom Support","text":"<p>This document provides a defined pathway for instructors at MSU to utilize ICER/HPCC resources for classroom education.</p>","tags":["reference"]},{"location":"ICER_HPC_Classroom_Support/#services-available","title":"Services Available","text":"<p>ICER will work with instructors to provide the following services during the class:</p> <ul> <li> <p>Training workshops on using the HPCC, via our asynchronous Desire2Learn-based training modules.</p> </li> <li> <p>HPCC student accounts and research space for use in classes and for sharing data files</p> </li> <li> <p>Access to OnDemand, a web-based portal to use Python via Jupyter notebooks,  RStudio, Matlab, Stata, and an interactive Linux desktop. OnDemand has access to all of the HPCC's file systems.</p> </li> <li> <p>Software installation for teaching purposes</p> </li> <li> <p>Reservations for computing resources in the cluster so that the students can instantly have access to the resources to finish the assignments</p> </li> <li> <p>Helpdesk tickets (submitting your questions via our online form)</p> </li> </ul>","tags":["reference"]},{"location":"ICER_HPC_Classroom_Support/#request-classroom-support","title":"Request Classroom Support","text":"<p>Requests should be submitted two weeks in advance, to allow for time for account creation, specialized software installation, etc.</p>","tags":["reference"]},{"location":"ICER_HPC_Classroom_Support/#request-individual-student-user-accounts","title":"Request individual student user accounts","text":"<p>Instructors need to submit a New Account Request online form for all students enrolled in the course. Please note that instructors are also responsible for requesting the accounts be closed when students are no longer enrolled in the course or when the course is finished. The following information should also be included in the request.</p> <ul> <li>MSU NetIDs of students enrolled in the course</li> <li>Course name for the group name of the student accounts</li> <li>Software installation, CPU/GPU core reservation (as needed)</li> <li>Start and end dates of the course</li> </ul>","tags":["reference"]},{"location":"ICER_HPC_Classroom_Support/#request-a-research-space-for-course-use","title":"Request a research space for course use","text":"<p>Instructors can also submit a Research request to create a research space for the course group and add all students enrolled in the request form.</p>","tags":["reference"]},{"location":"ICER_HPC_Classroom_Support/#note-on-storage-quota","title":"Note on storage quota","text":"<p>Both the home directory of a user and the research directory of a group are initialized with 50 GB storage quota. Additional space up to 1 TB may be requested for home directory or research space. Beyond 1 TB, ICER will need to charge for the additional storage.</p>","tags":["reference"]},{"location":"ICER_HPC_Classroom_Support/#class-account-agreement","title":"Class Account Agreement","text":"<p>Instructors are expected to make good faith efforts towards implementing the following policies:</p> <ul> <li> <p>Awareness of ICER and MSU policies -- Instructors using ICER     resources are expected to have an understanding of ICER and MSU IT     policies. Broadly, they should be cognizant of MSU's data sharing     policies, wait times on queued jobs and the possibility of     unscheduled outages. These factors should be taken into     consideration while designing assignments and projects that utilize     ICER resources. ICER/HPCC policies are emailed to users upon     account creation.</p> </li> <li> <p>Contacting ICER -- Instructors should provide clear policies in     their syllabus about when students should contact ICER staff. \u00a0ICER     will provide account, hardware and system software support. Very limited applications software support for the course     TAs and instructors is available. However, so as not to overwhelm     the ticketing system with course-specific questions, we ask that all     student questions be routed through the TA or course instructor who     will then determine whether to forward these to ICER's ticketing     system. While students are encouraged to visit ICER research     consultants during office hours, these hours are meant for research     support and are not designed to be used as a means of TA support. In     particular, students should be aware that submitting queries to ICER     that seek answers to homework problems will be considered cheating     and a violation of the Honor Code.</p> </li> <li> <p>Planning for outages -- ICER resources may become unavailable as     a result of an unscheduled system outage. Instructors are advised     not to depend on ICER resources for final exams and/or projects that     require a short turnaround time.</p> </li> <li> <p>Storing data -- Instructors should advise students against     storing data on the \"scratch\" space. Files are typically purged on     scratch after 45 days if no modification has been made, and cannot be recovered. Instructors may     request a research space for students to store their class related     data for the duration of the semester. Students may also store their     data in their home directory. Accounts for course work are typically     limited to 50 GB.</p> </li> <li> <p>Terminating educational accounts -- Student HPCC accounts, linked     home folders, and group folders created for courses will be removed     30 days after the end of the class. This applies only to     education-sponsored accounts and NOT research-sponsored student     accounts. However, students who wish to convert their     educated-sponsored account to a research-sponsored account after the     course is completed must have their research supervisor submit a     request for membership change no later than 30 days after the     semester in which the course was completed in order to retain the     data saved in the education-sponsored student account.</p> </li> <li> <p>Acknowledging ICER -- Instructors and students are encouraged to     acknowledge the use of ICER/HPCC upon publication of data related to the     resources used during the course.</p> </li> </ul> <p>Last updated: Aug 2022</p>","tags":["reference"]},{"location":"Installing_Local_Perl_Modules_with_CPAN/","title":"Installing Local Perl Modules with CPAN","text":"<p>CPAN\u00a0is a convenient way to build and install perl modules, but many people have difficulty knowing how to do this if they lack \"root\" permissions. \u00a0This tutorial will demonstrate how to install Perl modules to a local user space using CPAN.</p>","tags":["tutorial","Perl"]},{"location":"Installing_Local_Perl_Modules_with_CPAN/#procedure","title":"Procedure","text":"<p>First start the CPAN shell:</p> <pre><code>$ cpan\n\nTerminal does not support AddHistory.\n\ncpan shell -- CPAN exploration and modules installation (v1.9402)\n\nEnter 'h' for help.\n\ncpan[1]&gt;\n</code></pre> <p>Next determine where you want to install your local Perl modules. Let's assume we are going to place them in: /mnt/home/myUid/perlmods</p> <p>Now from within the CPAN shell, enter the following three (3) commands:</p> <pre><code>cpan[1]&gt; o conf mbuildpl_arg \"--install_base /mnt/home/myUid/perlmods\"\n\n     mbuildpl_arg       [--install_base /mnt/home/myUid/perlmods]\n\n   Please use 'o conf commit' to make the config permanent!\n\n\ncpan[2]&gt; o conf makepl_arg \"PREFIX=/mnt/home/myUid/perlmods\"\n\n    makepl_arg         [PREFIX=/mnt/home/myUid/perlmods]\n\n  Please use 'o conf commit' to make the config permanent!\n\n\ncpan[3]&gt; o conf prefs_dir \"/mnt/home/myUid/.cpan/prefs\"\n\n    prefs_dir          [/mnt/home/myUid/.cpan/prefs]\n\n  Please use 'o conf commit' to make the config permanent!\n</code></pre> <p>If you want to make the settings above permanent, enter \"o conf commit\". \u00a0Otherwise, bear in-mind you will need to reset this value every time you restart CPAN. If you do make the setting permanent, you can always change it later and re-commit as shown above.</p> <p>Now to install a module (lets assume we want to build \"Math::GMP\") simply enter:</p> <pre><code>cpan[4]&gt; install Math::GMP\n</code></pre> <p>Respond to any prompts for information that might be requested. When you are finished, enter:</p> <pre><code>cpan[5]&gt; quit\n</code></pre>","tags":["tutorial","Perl"]},{"location":"Installing_Local_Perl_Modules_with_CPAN/#setting-the-perl5lib-path","title":"Setting the PERL5LIB Path","text":"<p>Now that you've successfully installed a local Perl module, you will need to tell Perl where to find them. \u00a0This can be easily accomplished by setting the environmental path variable \"PERL5LIB\". For example:</p> <pre><code>$ export PERL5LIB=/mnt/home/myUid/perlmods:$PERL5LIB\n</code></pre> <p>You can add this export to your .bashrc file if you'd like to ensure it is always loaded upon login. In addition, for any scripts that you write that utilize these local Perl modules that you run on the HPCC cluster, you should add this export statement to your job script, or create a\u00a0custom user module\u00a0that does that for you, and which can be loaded from within your job script.</p>","tags":["tutorial","Perl"]},{"location":"Installing_TensorFlow_using_anaconda/","title":"Installing TF using anaconda","text":"<p>Warning</p> <p>TensorFlow requires specific instructions for a fully functional installation. As such, the instructions and recommendations on this page may differ slightly from other pages in ICER's documentation, but have been fully tested as of March 2023. For more general Conda and Python usage, please see our page on Using Conda.</p>","tags":["how-to guide","TensorFlow","Conda"]},{"location":"Installing_TensorFlow_using_anaconda/#install-tensorflow-using-conda","title":"Install TensorFlow using conda","text":"<p>In this tutorial, we will first install Anaconda in our home directory, then install TF in a newly created conda environment, and finally run a few TF commands to verify the installation.</p>","tags":["how-to guide","TensorFlow","Conda"]},{"location":"Installing_TensorFlow_using_anaconda/#installing-anaconda-in-your-home-directory","title":"Installing Anaconda in your home directory","text":"<p>A full guide of downloading anaconda and installing it in your home directory is here. Following the guide, below we show a sequence of commands that will download and configure conda in one's home directory on the HPCC (say <code>/mnt/home/user123/</code>).</p> <pre><code># Install anaconda3 in /mnt/home/user123/ (replace user123 with your HPCC account name)\nwget https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh\nbash Anaconda3-2022.10-Linux-x86_64.sh\nsource /mnt/home/user123/anaconda3/bin/activate\nconda init\nconda config --set auto_activate_base false\n</code></pre> <p>Notes</p> <ul> <li> <p>We recommend downloading Anaconda 3 which corresponds to Python 3. </p> </li> <li> <p>In the guide, step 8, it says Anaconda recommends entering \"yes\".  However, we recommend a \"No\" so as to not modify your <code>~/.bashrc</code>. After that, you will need to run  <code>source /mnt/home/user123/anaconda3/bin/activate</code> and <code>conda init</code> as shown above.</p> </li> <li> <p>The last command above is to disable automatic base environment activation. This is necessary.</p> </li> <li> <p>By default, your anaconda will be installed in <code>/mnt/home/user123/anaconda3/</code>. You can specify an alternate installation path during this interactive process.</p> </li> <li> <p>Above, the link after <code>wget</code> can be replaced by a more recent version of script in https://repo.anaconda.com/archive/</p> </li> <li> <p>If you encounter any errors, check your quota first, by running <code>quota</code>. Make sure your home directory has enough space. Always fully delete previously installed anaconda if you are going to re-install by repeating the steps.</p> </li> </ul>","tags":["how-to guide","TensorFlow","Conda"]},{"location":"Installing_TensorFlow_using_anaconda/#installing-tf-in-a-conda-environment","title":"Installing TF in a conda environment","text":"<p>After you've successfully installed Anaconda in your home directory, you can follow the commands below to install TF and troubleshoot some errors. After initial login,  run  <code>ssh dev-amd20-v100</code> to log into our GPU dev-node.</p> <p>Warning</p> <p>Installing TensorFlow while on <code>dev-amd20-v100</code> will restrict you to amd20 nodes with GPUs. You must specify <code>amd20</code> as a constraint when submitting a batch job or starting an OnDemand session.</p> <p>If you are not familiar with basic conda commands (e.g., <code>conda create/activate/install/deactivate</code>), check out this conda cheatsheet. After creating a new conda environment (namely <code>tf_gpu_Feb2023</code> below) and activating it, the environment variable <code>$CONDA_PREFIX</code> will point to <code>/mnt/home/user123/anaconda3/envs/tf_gpu_Feb2023</code>. </p> <p>Minimally, you only need to modify the first line below, that is, <code>export PATH=...</code>, so that the path points to the <code>bin</code> folder in your anaconda installation. The rest of the commands can be directly copied and run in your terminal.</p> <pre><code>export PATH=/mnt/home/user123/anaconda3/bin:$PATH\nconda create --name tf_gpu_Feb2023 python=3.9\nconda activate tf_gpu_Feb2023\n\nconda install -c conda-forge cudnn=8.1.0 --yes\nconda install -c nvidia cuda-nvcc --yes \n\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\npip install --upgrade pip\npip install tensorflow==2.11.0 # compatible with CuDNN v8.1.0\npip install nvidia-pyindex\npip install nvidia-tensorrt\n\n# To fix the error of \"Could not load dynamic library 'libnvinfer.so.7'\". The trick is to create a symlink.\ncd $CONDA_PREFIX/lib/python3.9/site-packages/tensorrt_libs\nln -s libnvinfer.so.8 libnvinfer.so.7\nln -s libnvinfer_plugin.so.8 libnvinfer_plugin.so.7\n\n# To fix the error of \"Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice.\" \nmkdir -p $CONDA_PREFIX/lib/nvvm/libdevice\ncp $CONDA_PREFIX/lib/libdevice.10.bc $CONDA_PREFIX/lib/nvvm/libdevice\n\nconda deactivate\n</code></pre>","tags":["how-to guide","TensorFlow","Conda"]},{"location":"Installing_TensorFlow_using_anaconda/#verifying-the-installation-using-simple-commands","title":"Verifying the installation using simple commands","text":"<p>Now we'll run a few one-liners to test out, right from the shell command line. Again, you need to be logged onto <code>dev-amd20-v100</code> the GPU dev-node. If no errors pop up when executing these commands, you should be all set.</p> <p>Note</p> <p>You'll need to run the first four lines every time you want to start using TensorFlow. This includes any SLURM scripts you write to launch TF jobs.</p> <pre><code>export PATH=/mnt/home/user123/anaconda3/bin:$PATH\nconda activate tf_gpu_Feb2023\n\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/lib/:/lib64/:$CONDA_PREFIX/lib/:$CONDA_PREFIX/lib/python3.9/site-packages/tensorrt_libs\nexport XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib\n\n# Simple one-liner test commands\npython3 -c \"import tensorflow as tf; print (tf.__version__)\" # check TF version\npython3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\" # verify GPU devices\npython3 -c \"import tensorrt; print(tensorrt.__version__); assert tensorrt.Builder(tensorrt.Logger())\" # test TensorRT installation\n\nconda deactivate\n</code></pre> <p>More complicated testing code can be found in our TensorFlow model training examples.</p>","tags":["how-to guide","TensorFlow","Conda"]},{"location":"Installing_TensorFlow_using_anaconda/#using-tensorflow-in-an-ondemand-jupyter-notebook","title":"Using TensorFlow in an OnDemand Jupyter notebook","text":"<p>If you would like to use TensorFlow from an Open OnDemand Jupyter notebook, you'll first need to install Jupyter.</p> <pre><code>export PATH=/mnt/home/user123/anaconda3/bin:$PATH\nconda activate tf_gpu_Feb2023\n\nconda install jupyter\n</code></pre> <p>Then, you need to edit a particular file to set up the <code>LD_LIBRARY_PATH</code> and <code>XLA_FLAGS</code> environment variables in the same way they are set above. First, we'll make a backup of this file as demonstrated below.</p> <pre><code>cd $CONDA_PREFIX/share/jupyter/kernels/python3/\n\ncp kernel.json kernel.json.bak\n</code></pre> <p>With your favorite text editor, open <code>kernel.json</code>. Look for the following pattern at the end of the file <pre><code> }\n}\n</code></pre> and add the following (note the commas!) <pre><code> },\n \"env\": {\n  \"XLA_FLAGS\":\"--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib\",\n  \"LD_LIBRARY_PATH\":\"$LD_LIBRARY_PATH:/lib/:/lib64/:$CONDA_PREFIX/lib/:$CONDA_PREFIX/lib/python3.9/site-packages/tensorrt_libs\"\n }\n}\n</code></pre></p> <p>Note</p> <p>If you open a notebook and get a message about no kernel being available, make sure you added the comma after the first curly bracket.</p> <p>When you request a Jupyter notebook through OnDemand, make sure to  do the following:</p> <ul> <li>Request more than the minimum amount of memory (on the order of GB)</li> <li>Select \"Launch Jupyter Notebook using the Anaconda installation in my home directory\"</li> <li>Enter the full path to your Anaconda installation; e.g, <code>/mnt/home/user123/anaconda3</code></li> <li>Enter the name of your TF Conda environment; e.g., <code>tf_gpu_Feb2023</code></li> <li>Select \"Advanced Options\"</li> <li>Set the node type to <code>amd20</code></li> <li>Request 1-4 GPUs</li> </ul> <p>Even if you have requested less than 4 hours of wall time, your  job may spend more time in the queue than you may used to. This is normal given the specific resources we have requested.</p> <p>You can test that TensorFlow will run in your notebook by running the following: <pre><code>import tensorflow as tf\nimport tensorrt\n\nprint(\"TF Version:\", tf.__version__)\nprint(\"GPUs:\\n\", tf.config.list_physical_devices('GPU'))\nprint(\"TensorRT Version:\", tensorrt.__version__)\n\nassert tensorrt.Builder(tensorrt.Logger())\n</code></pre></p>","tags":["how-to guide","TensorFlow","Conda"]},{"location":"Installing_pytorch_using_anaconda/","title":"Installing Pytorch/Pytorch Lightning Using Anaconda","text":"<p>This guide will walk you through installing Pytorch and/or Pytorch Lighting using conda. It assumes you have already installed either Anaconda or Miniconda. See the guide on using conda for more.</p>","tags":["how-to guide","Conda","PyTorch"]},{"location":"Installing_pytorch_using_anaconda/#setup-checking-python","title":"Setup - Checking Python","text":"<p>If you installed conda on your own and not following our using conda guide, the HPCC may be trying to use the system python installation instead of your own. To test if this is the case, run <pre><code>which python\n</code></pre> If the output starts with <code>/opt/software</code>, you will need to run <code>module unload Python</code>. If the output starts with the path to your Anaconda or Miniconda installation, you don't need to do anything else.</p> <p>Note</p> <p>If you are affected by the above issue, you will have to run <code>module unload Python</code> every time you wish to use your own python installation. You may wish to add the <code>module unload Python</code> command to your <code>$HOME/.bashrc</code> file.</p>","tags":["how-to guide","Conda","PyTorch"]},{"location":"Installing_pytorch_using_anaconda/#installing-pytorch","title":"Installing Pytorch","text":"<p>Since Pytorch works best when using a GPU, it needs to be installed on a development node with a GPU. We recommend using <code>dev-amd20-v100</code> for the latest hardware. Run <pre><code>ssh dev-amd20-v100\n</code></pre></p> <p>Note</p> <p>You will be restricted to running Pytorch on nodes with v100 GPUs. See the page on cluster resources and SLURM job specifications for more.</p> <p>You will also need the CUDA compiler, so load this using our module system: <pre><code>module load CUDA/11.8.0\n</code></pre></p> <p>It's best practice to use Conda environments to organize your Python packages. Create a new conda environment with the name <code>pytorch</code> run <pre><code>conda create --name pytorch\n</code></pre></p> <p>To switch to this new environment, run <pre><code>conda activate pytorch\n</code></pre></p> <p>Now that you are on a GPU development node, have loaded the CUDA module, and activated your new environment, you can install Pytorch with the following command: <pre><code>conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n</code></pre></p>","tags":["how-to guide","Conda","PyTorch"]},{"location":"Installing_pytorch_using_anaconda/#installing-pytorch-lightning","title":"Installing Pytorch Lightning","text":"<p>It's best to install Pytorch following the instructions above before installing Pytorch Lightning, or GPU-support may not function correctly.</p> <p>After Pytorch has been installed, Pytorch Lightning can be installed to the same <code>pytorch</code> environment using <pre><code>conda install pytorch-lightning -c conda-forge\n</code></pre></p>","tags":["how-to guide","Conda","PyTorch"]},{"location":"Interactive_Job/","title":"Interactive Job","text":"<p>It is helpful to run your work and see the response of the commands right away to check if there is any error in your work flow. To use the interactive mode with resources more than the limit imposed on the dev nodes, HPCC users can submit an interactive job using the <code>interact</code>  powertool, or the <code>salloc</code>/<code>srun</code> commands, with options of resource requests.</p>","tags":["reference","slurm"]},{"location":"Interactive_Job/#interact-powertool","title":"<code>interact</code> powertool","text":"<p>The <code>interact</code> powertool provides sensible defaults to launch interactive jobs on the HPCC. It is loaded as part of the default modules. Alternatively you can access it with <code>module load powertools</code>. The default resource request when you  run <code>interact</code> is 1 task on 1 core on 1 node for 1 hour. Your job will be queued.  Do not close your terminal. Once the job has queued, you will be transported to a  command prompt on the compute node assigned to your job. You can close your  terminal and reconnect to the interactive session following the information at  Connections to compute nodes.</p>","tags":["reference","slurm"]},{"location":"Interactive_Job/#specifying-other-resources","title":"Specifying other resources","text":"<p>To request resources beyond the defaults, you can use the following options:</p> Option Alternate Description <code>-t &lt;Time&gt;</code> <code>--time</code> Set a limit on the total run time. <code>-gpu</code> <code>--gpu</code> Allocate 1 gpu and 16 CPUs <code>-N &lt;Nodes&gt;</code> <code>--nodes</code> Number of nodes <code>-c &lt;ncores&gt;</code> Number of cores <code>-n &lt;ntasks&gt;</code> Number of tasks (spread over all Nodes) <code>--ntasks-per-node=&lt;ntasks&gt;</code> Number of tasks, 1 per core per node. <code>--mem=&lt;MB&gt;</code> Real memory required per node in MegaBytes <p>You can view the other options that <code>interact</code> accepts using the command <code>interact -h</code>. </p>","tags":["reference","slurm"]},{"location":"Interactive_Job/#salloc-command","title":"salloc command","text":"<p>For salloc, the command line</p> <pre><code>salloc -N 1 -c 2 --time=1:00:00\n</code></pre> <p>will allocate a job with resources of 1 node, 2 cores and walltime 1 hour. The execution will first wait until the job controller can provide the resources.</p> <pre><code>[username@dev-intel18 WorkDir]$ salloc -N 1 -c 2 --time=1:00:00\nsalloc: Required node not available (down, drained or reserved)\nsalloc: job 7625 queued and waiting for resources\n</code></pre> <p>Once that happens, the terminal will be transported to a command prompt on a compute node assigned to the job.</p> <pre><code>[username@dev-intel18 WorkDir]$ salloc -N 1 -c 2 --time=1:00:00\nsalloc: Required node not available (down, drained or reserved)\nsalloc: job 7625 queued and waiting for resources\nsalloc: job 7625 has been allocated resources\n[username@test-skl-000 WorkDir]$\n</code></pre> <p>where \"test-skl-000\" after the symbol\u00a0@ is the name of the assigned compute node.</p>","tags":["reference","slurm"]},{"location":"Interactive_Job/#salloc-and-gpus","title":"salloc and GPUs","text":"<p>GPUs requested for an interactive job can now be used without submitting an additional srun. See our pages on our GPU resources and requesting GPUs for more information.</p> <pre><code>[username@dev-intel18 ~]$ salloc --gpus=k80:1 --time=1:00:00\nsalloc: Pending job allocation 28241766\nsalloc: job 28241766 queued and waiting for resources\nsalloc: job 28241766 has been allocated resources\nsalloc: Granted job allocation 28241766\nsalloc: Waiting for resource configuration\nsalloc: Nodes lac-195 are ready for job\n[username@lac-195 ~]$ nvidia-smi\nThu Jan  4 14:34:03 2024       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla K80           On   | 00000000:0B:00.0 Off |                    0 |\n| N/A   40C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>","tags":["reference","slurm"]},{"location":"Interactive_Job/#srun-command","title":"srun command","text":"<p>A similar way can also be used with srun command:</p> <pre><code>[username@dev-intel18 WorkDir]$ srun -N 4 --ntasks-per-node=2 -t 00:60:00 --mem-per-cpu=1000M --pty /bin/bash\nsrun: Required node not available (down, drained or reserved)\nsrun: job 7636 queued and waiting for resources\nsrun: Granted job allocation 7636\n[username@test-skl-000 WorkDir]$ \n</code></pre> <p>As we can see, the specification \"--pty /bin/bash\" is required for srun command to request an interactive mode. Any command executed in this kind of interactive jobs will be launched parallelly with the number of task requested. srun can also be used in a command line without the specification \"--pty /bin/bash\". You may refer to the srun web site for more details.</p>","tags":["reference","slurm"]},{"location":"Interactive_Job/#job-with-graphical-application","title":"Job with graphical application","text":"<p>To schedule an interactive job able to use graphical user interface (GUI) software, the specification <code>--x11</code> for X11 forwarding needs to be specified with the command (salloc or srun).\u00a0\u00a0You must use the <code>-X</code> parameter with <code>ssh</code> to allow X11 forwarding when connecting to both gateway  and development nodes prior to running the salloc command.\u00a0 If you are using Mac Terminal, you must have Xquartz installed. If you are on Windows and using Moba Xterm to log in, these instructions will work with the <code>-X</code> parameter.\u00a0 Putty does not support X11 and so this will not work with putty.\u00a0</p> <p>The other option is to first log into our web-based remote desktop, and run the terminal there. See Web Site Access to HPCC for GUI software.\u00a0</p> <pre><code>[username@gateway-03 ~]$ ssh -X dev-intel18\n[username@dev-intel18 ~]$ cd WorkdDir  # this is optional, but you may want to select your work directory, for example\n[username@dev-intel18 WorkDir]$ salloc --ntasks=1 --cpus-per-task 2 --time 00:30:00 --x11\nsalloc: Granted job allocation 7708\nsalloc: Waiting for resource configuration\nsalloc: Nodes css-076 are ready for job\n[username@css-076 WorkDir]$ module load MATLAB\n[username@css-076 WorkDir]$ matlab\nMATLAB is selecting SOFTWARE OPENGL rendering.\nOpening log file:  /mnt/home/username/java.log.7159\n</code></pre> <p></p>","tags":["reference","slurm"]},{"location":"Intro_to_MobaXterm/","title":"Intro to MobaXterm","text":""},{"location":"Intro_to_MobaXterm/#setting-up-mobaxterm-on-your-local-computer","title":"Setting up MobaXterm on your local computer","text":"<p>To obtain a copy of MobaXterm, go to the MobaXterm website and selected 'Download' at the top of the page</p> <p></p> <p>On the next page, choose \"Download Now\" under Home Edition</p> <p></p> <p>Finally, click the blue \"Portable Edition\" button. </p> <p></p> <p>The Portable Edition is convenient in that all you will need to do is unzip the file once the download is finished, no need to install anything.</p> <p></p> <p>Go into the unzipped folder and click on the MobaXterm application to run the program (which should be named something like \"MobaXterm_Personal_XX.Y\" where XX.Y is the verison number)</p> <p></p>"},{"location":"Intro_to_MobaXterm/#setting-up-and-ssh-connection-session-to-hpcc","title":"Setting up and SSH connection session to HPCC","text":"<p>Inside MobaXterm, you can define a 'SSH Session' to simplify the process of connecting to HPCC. First, start MobaXterm, then click on the \"Session\" button in the upper left. </p> <p></p> <p>Next, choose the \"SSH\" option from pop-up menu</p> <p></p> <p>In the following menu, fill in \"hpcc.msu.edu\" as the remote host. Then check the \"Specify username\" box and fill in your MSU NetId. Also, make sure the X11 fowarding box is checked under the \"Advanced SSH Settings\" tab, as this will allow to launch GUI programs through your session. Click \"OK\" once you are finsihed. You may get a pop-up window asking if you trust the host you have entered; click \"Accept.\"</p> <p></p> <p>Now, if you click the yellow star on the left pannel, you should see a line in the 'User sessions' column named something like \"hpcc.msu.edu ()\". <p></p> <p>Click this and you  will be prompted to enter your NetId password, after which you will be logged into one of the gateway nodes on HPCC. From here you can connect to any of the development nodes.</p>"},{"location":"Intro_to_modules/","title":"Module System Tutorial","text":"<p>The HPCC has a large amount of software installed in order to support its diverse users. This can include multiple versions of the same software. The module system exists to manage all of this by making software available to users and preventing version conflicts.</p> <p>In this tutorial, you'll learn how to use the module system to:</p> <ul> <li>See which modules are currently loaded</li> <li>Search for available software versions</li> <li>Check requirements for particular modules</li> <li>Loading modules</li> <li>Saving currently loaded modules to easily reload</li> </ul> <p>For the purposes of this tutorial, we'll be trying to load version 4.1.0 of the R interpreter.</p>","tags":["tutorial"]},{"location":"Intro_to_modules/#viewing-currently-loaded-modules","title":"Viewing Currently Loaded Modules","text":"<p>Several modules are already loaded by default once you log on to a development node. These include several commonly used packages such as Python, MATLAB, and the GNU compiler.</p> <p>Run <code>module list</code> to see all currently available modules. The output will look like the following:</p> <pre><code>Currently Loaded Modules:\n  1) GCCcore/6.4.0     9) ScaLAPACK/2.0.2-OpenBLAS-0.2.20  17) SQLite/3.21.0\n  2) binutils/2.28    10) bzip2/1.0.6                      18) GMP/6.1.2\n  3) GNU/6.4.0-2.28   11) zlib/1.2.11                      19) libffi/3.2.1\n  4) OpenMPI/2.1.2    12) Boost/1.67.0                     20) Python/3.6.4\n  5) tbb/2018_U3      13) CMake/3.11.1                     21) Java/1.8.0_152\n  6) imkl/2018.1.163  14) ncurses/6.0                      22) MATLAB/2018a\n  7) OpenBLAS/0.2.20  15) libreadline/7.0                  23) powertools/1.2\n  8) FFTW/3.3.7       16) Tcl/8.6.8\n</code></pre> <p>Unfortunately, R isn't loaded by default. We'll have to figure out what's needed to load it ourselves.</p>","tags":["tutorial"]},{"location":"Intro_to_modules/#searching-for-available-modules","title":"Searching for Available Modules","text":"<p>The <code>module</code> command accepts a variety of \"sub-commands\". The example we used above, <code>list</code>, is an example of a sub-command.</p> <p>What other sub-commands are available? Run <code>module</code> by itself to find out. </p> <p>You'll see a long list of available sub-commands printed to the screen. Scroll up until you see the portion on listing and searching: <pre><code>Listing / Searching sub-commands:\n---------------------------------\n  list                              List loaded modules\n  list                s1 s2 ...     List loaded modules that match the pattern\n  avail | av                        List available modules\n  avail | av          string        List available modules that contain \"string\".\n  spider                            List all possible modules\n  spider              module        List all possible version of that module file\n  spider              string        List all module that contain the \"string\".\n  spider              name/version  Detailed information about that version of the\n                                    module.\n  whatis              module        Print whatis information about module\n  keyword | key       string        Search all name and whatis that contain \"string\".\n</code></pre></p> <p>Here we see the sub-command <code>list</code>, which we've already encountered. We'll cover <code>avail</code> and <code>keyword</code> in other documentation. For now, let's focus on the <code>spider</code> sub-command.</p> <p>The <code>spider</code> sub-command is the most useful way to search through available modules. Its name isn't obvious, but you can think of sending a spider to walk through a tangled web of modules to find the ones matching your request.</p> <p>The list of sub-commands from <code>module</code> shows four options for the <code>module spider</code> sub-command:</p> Argument Output None All possible modules <code>module</code> All versions of that module <code>string</code> All modules containing <code>string</code> <code>name/version</code> Details about a module version <p>We'll cover <code>module</code> and <code>string</code> search in the next section on searching by module name. After that we'll cover the <code>name/version</code> search for loading a specific version.</p>","tags":["tutorial"]},{"location":"Intro_to_modules/#searching-by-module-name","title":"Searching by Module Name","text":"<p>For this tutorial we want to search for the versions of the R interpreter. Within the module system on the HPCC, \"R\" with a capital R is the formal name of the module.</p> <p>Run <code>module spider R</code>. An abbreviated output is reproduced below: <pre><code>----------\n\n  R:\n----------\n    Description:\n      R is a free software environment for statistical computing and graphics.\n\n     Versions:\n        R/3.3.1\n        ...\n        R/4.1.0\n        R/4.1.2\n        R/4.2.2\n     Other possible modules matches:\n        ADMIXTURE  AMDuProf  APR  APR-util  Abaqus_parallel  AdapterRemoval  Advisor  ...\n\n----------\n  To find other possible module matches execute:\n\n      $ module -r spider '.*R.*'\n\n----------\n  For detailed information about a specific \"R\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider R/4.2.2\n----------\n</code></pre></p> <p>We see a full list of all available R versions as well as some other helpful information:</p> <ul> <li>Other possible module names we may have been searching for</li> <li>How to search for all modules containing the string \"R\"</li> <li>How to get detailed information on a specific version</li> </ul> <p>The first two points reference searching by <code>string</code> in the table above, rather than searching by <code>module</code>. The third point references the <code>name/version</code> search.</p> <p>What's the difference between searching by <code>module</code> and searching by <code>string</code>? As mentioned at the start of this section, \"R\" is the formal name of the module. The module system tries to be case insensitive, but it can have odd results. </p> <p>Run <code>module spider r</code> with a lowercase r and you'll see we've executed a <code>string</code> search, returning all modules that include the letter R!</p> <p>This is probably more information than you would like. Press <code>q</code> to quit and return to the terminal.</p> <p>That said, searching by <code>string</code> is powerful if you don't know the precise name of the module you are looking for.</p>","tags":["tutorial"]},{"location":"Intro_to_modules/#details-on-a-module-version","title":"Details on a Module Version","text":"<p>All modules listed by <code>module spider</code> follow the <code>name/version</code> format. We can use this format to get more information on a specific version.</p> <p>We're interested in using R 4.1.0, so run <code>module spider R/4.1.0</code>.</p> <p>You'll again see a long list of output. The portion we are interested in is this: <pre><code>You will need to load all module(s) on any one of the lines below before the \"R/4.1.0\" module is available to load.\n\n      GCC/8.3.0  OpenMPI/3.1.4\n</code></pre></p> <p>This section tells us the required dependencies for our module. A module may have multiple possible sets of dependencies; for instance, different combinations of compilers and MPI libraries. Each different set will be listed on a new line.</p> <p>For R 4.1.0 however there is only one possible dependency: version 8.3.0 of the GCC module and version 3.1.4 of the Open MPI module.</p> <p>Now that we understand the dependencies, let's move on to loading our module.</p>","tags":["tutorial"]},{"location":"Intro_to_modules/#loading-modules","title":"Loading Modules","text":"<p>As we saw in the last section, when checking module details, R 4.1.0 requires the GCC module with version 8.3.0 and the Open MPI module with version 3.1.4.</p> <p>Let's check the output of <code>module list</code> again:</p> <pre><code>Currently Loaded Modules:\n  1) GCCcore/6.4.0     9) ScaLAPACK/2.0.2-OpenBLAS-0.2.20  17) SQLite/3.21.0\n  2) binutils/2.28    10) bzip2/1.0.6                      18) GMP/6.1.2\n  3) GNU/6.4.0-2.28   11) zlib/1.2.11                      19) libffi/3.2.1\n  4) OpenMPI/2.1.2    12) Boost/1.67.0                     20) Python/3.6.4\n  5) tbb/2018_U3      13) CMake/3.11.1                     21) Java/1.8.0_152\n  6) imkl/2018.1.163  14) ncurses/6.0                      22) MATLAB/2018a\n  7) OpenBLAS/0.2.20  15) libreadline/7.0                  23) powertools/1.2\n  8) FFTW/3.3.7       16) Tcl/8.6.8\n</code></pre> <p>Currently we have <code>GCCcore/6.4.0</code> and <code>OpenMPI/2.1.2</code> loaded. Notice <code>GCCcore</code> is not the same as the <code>GCC</code> module. </p> <p>Both GCC core and Open MPI configure several compilers. Their many versions are dependencies for many modules. Since we already have several modules loaded by default which may or may not require both GCC core 6.4.0 and Open MPI 2.1.2, it's best to start with a clean module environment. </p> <p>Run <code>module purge</code> to remove all currently loaded modules. You can confirm all modules have been unloaded with <code>module list</code>.</p> <p>What happens if we just try to load R directly? Run <code>module load R/4.1.0</code> and you should see the following error: <pre><code>Lmod has detected the following error:  These module(s) or extension(s) exist\nbut cannot be loaded as requested: \"R/4.1.0\"\n   Try: \"module spider R/4.1.0\" to see how to load the module(s).\n</code></pre></p> <p>We cannot load a module without first loading its dependencies! The error message gives us a helpful reminder about how to learn what those requirements are, but we already know that R 4.1.0 requires <code>GCC/8.3.0</code> and <code>OpenMPI/3.1.4</code>.</p> <p>Run <code>module load GCC/8.3.0 OpenMPI/3.1.4 R/4.1.0</code> to load R along with its dependency.</p> <p>You will not see any output after this command. Run <code>module list</code> if you would like to confirm it worked.</p>","tags":["tutorial"]},{"location":"Intro_to_modules/#saving-and-restoring-loaded-modules","title":"Saving and Restoring Loaded Modules","text":"<p>We often use the same pieces of software over and over on the HPCC. Remembering all the modules we need every time we log in can be a hassle.</p> <p>Thankfully, the module system lets us save and restore different configurations.</p>","tags":["tutorial"]},{"location":"Intro_to_modules/#saving-a-configuration","title":"Saving a Configuration","text":"<p>Now that we only have GCC, OpenMPI, and R version 4.1.0 loaded, let's save a configuration to easily access these modules later. We'll name it <code>R-example</code>.</p> <p>Type <code>module save R-example</code>. You should see the following confirmation message: <pre><code>Saved current collection of modules to: \"R-example\"\n</code></pre></p>","tags":["tutorial"]},{"location":"Intro_to_modules/#restoring-a-configuration","title":"Restoring a Configuration","text":"<p>Log out of the HPCC and log back in to a development node. This will reset your loaded modules to the default; run <code>module list</code> to confirm this is the case.</p> <p>Run <code>module savelist</code> to see our stored configurations. Confirm that you see <code>R-example</code>: <pre><code>Named collection list :\n  1) R-example \n</code></pre></p> <p>Now, run <code>module restore R-example</code>. The existing modules will automatically be purged and your desired modules loaded!</p>","tags":["tutorial"]},{"location":"Intro_to_modules/#further-resources","title":"Further Resources","text":"<p>You should now understand the basics of the module system. For a refresher on searching for modules, see Searching software modules.</p>","tags":["tutorial"]},{"location":"Job-Arrays---Run-multiple-similar-jobs-simultaneously_40337504.html/","title":"Job Arrays - Run multiple similar jobs simultaneously","text":"<p>Job array is an efficient way to submit and manage a collection of jobs that differ from each other by only a single index parameter. All jobs in a job array should have the same resource request (ex. size, time, etc.). For example, in the task below we wish to run python script \"hello.py\" ten times, each with a different input parameter for.  Instead of creating 10 separate Slurm job scripts and submitting them separately, we can create an array job in a script and submit it once for all.</p> <ul> <li> <p>Creat Python script file \"hello.py\" as show below.</p> <p>hello.py</p> </li> </ul> <pre><code>    #!/usr/bin/python\n\n    # import sys library (needed for accepted command line args)\n    import sys\n\n    # example of \"hello world!\" in Python\n    print('Hello World! This is the task number', sys.argv[1])\n</code></pre> <ul> <li> <p>Creat a job array script file \"hello.sb\" as show below.</p> <p>hello.sb </p> </li> </ul> <pre><code>    #!/bin/bash\n\n    # Example of running python script in a batch mode\n\n    #SBATCH -J hello.py                     # job name\n    #SBATCH -c 1                            # use one CPU core\n    #SBATCH --mem=1M                        # request memory\n    #SBATCH -t 10:00                        # 10 minutes time limit\n    #SBATCH -o hello-%A_%a.out              # output file name pattern\n                                            # %A is jobID and %a is task index.\n\n    #SBATCH --array=1-10                    # run array of 10 tasks with index 1, 2, ... 10.\n\n    # Load default version of Python\n    module purge\n    module load Python\n\n    # Run array of python script\n    python hello.py $SLURM_ARRAY_TASK_ID\n</code></pre> <ul> <li>Submit the job array as shown in following line: </li> </ul> <pre><code>    sbatch hello.sb\n</code></pre> <p>Now you can see there are 10 jobs submitted into your job queue. </p> <p>This example was modified from the example at:\u00a0https://rcpedia.stanford.edu/topicGuides/jobArrayPythonExample.html</p> <p>User can also download job array examples from our collection of examples by running following commands:</p> <pre><code>    module load powertools\n    getexample basic_array_job\n</code></pre> <p>A directory named \"basic_array_job\" containing two simple examples of job array will be downloaded to user's current directory.</p>"},{"location":"Job_Constraints/","title":"Job Constraints","text":"<p>Constraints are set to restrict which node features are required for a given job. Use\u00a0-C\u00a0or\u00a0--constraint = &lt;list&gt;\u00a0when submitting your job to specify a constraint.\u00a0</p>"},{"location":"Job_Constraints/#operators","title":"Operators","text":"<p>The following operators can be used to combine multiple features when specifying constraints.</p> Operator Function Description Example feature&amp;feature AND Nodes allocated for the job must have both features lac&amp;ib feature|feature OR Each node allocated for the job can have one feature or the other. lac|vim [feature|feature] XOR All nodes allocated for the job must have one feature or the other. Useful for multi-node shared memory jobs. [intel16|intel18]"},{"location":"Job_Constraints/#multi-node-jobs","title":"Multi-Node Jobs","text":"<p>Multi-node jobs that use constraints to control which cluster hardware they run on must use the XOR syntax to specify multiple clusters, e.g. <code>[intel18|intel16]</code>, and not the OR syntax, e.g. <code>intel18|intel16</code>. Multi-node jobs using OR instead of XOR may experience a considerable performance decrease or may be killed by HPCC staff without warning.</p> <p>In most cases where OR is used, the job would be better served by XOR. For this reason, constraints using OR are automatically re-written to use XOR. If an OR constraint is required, prepend the constraint request with <code>NOAUTO:</code>.</p>"},{"location":"Job_Constraints/#automatic-job-constraints","title":"Automatic Job Constraints","text":"<p>When no constraints are specified, a job will get a default constraint of\u00a0<code>[intel14|intel16|intel18]</code>. This\u00a0ensures the job runs on only one type of cluster hardware. User specified constraints that don't use the <code>feature|feature</code> or <code>[feature|feature]</code> syntax are automatically combined with\u00a0<code>[intel14|intel16|intel18]</code>. If a more complex constraint is required, <code>NOAUTO:</code> must be prepended to the constraint.</p> User Specified Constraint Literal Constraint Effective Constraint Result None [intel14|intel16|intel18] [intel14|intel16|intel18] Run this job on nodes that are all the same cluster type <code>--constraint=lac</code> lac&amp;[intel14|intel16|intel18] lac Run this job on only Laconia nodes <code>--constraint=</code><code>\"lac&amp;ib\"</code> lac&amp;ib&amp;[intel14|intel16|intel18] lac&amp;ib Run this job on only nodes with both lac and ib features <code>--constraint=</code><code>\"intel16|intel18\"</code> [intel16|intel18] [intel16|intel18] Run this job on nodes that are either all intel16 or all intel18 <code>--constraint=</code><code>\"NOAUTO:lac|vim\"</code> lac|vim lac|vim Run this job on nodes the each have either the lac or vim feature <code>--constraint=</code><code>\"[intel16&amp;ib|intel18&amp;ib]\"</code> [intel16&amp;ib|intel18&amp;ib] [intel16&amp;ib|intel18&amp;ib] Run this job on nodes that are either all intel16&amp;ib or all intel18&amp;ib"},{"location":"Job_Script_and_Job_Submission/","title":"Writing and submitting job scripts","text":"<p>The HPCC uses the SLURM system to manage computing resources. Users access these resources by submitting batch jobs.</p> <p>This tutorial will walk you through the process of writing and submitting a job submission script for a parallel job that uses multiple cores across several nodes.</p>","tags":["tutorial","slurm","job script"]},{"location":"Job_Script_and_Job_Submission/#setup","title":"Setup","text":"<p>First, make sure you're on a development node and in your home directory.</p> <p>Clone and compile the example we're going to use: <pre><code>getexample MPI_OpenMP_GPU\ncd MPI_OpenMP_GPU\n\nmodule purge\nmodule load GCC/10.3.0 OpenMPI/4.1.1\n\nmake\n</code></pre></p> <p>This directory contains example C++ codes using several forms of parallelism. These examples may be useful if you find yourself developing your own software, and interested users should read the accompanying README.</p> <p>For now we'll just use the <code>hybrid</code> example. This example combines MPI and OpenMP. MPI allows multiple processes to communicate with each other, while OpenMP allows multiple CPUs to \"collaborate\" on the same process.</p> <p>We would like to run 4 processes, each on their own node, with 2 CPUs per process. That means we'll need a total of 8 CPUs.</p>","tags":["tutorial","slurm","job script"]},{"location":"Job_Script_and_Job_Submission/#writing-a-job-script","title":"Writing a job script","text":"<p>A job script is a plain text file. It's composed of two main parts:</p> <ul> <li>The resource request</li> <li>The commands for running the job</li> </ul> <p>Using <code>nano</code> or your preferred text editor, create and open <code>hybrid.sb</code>: <pre><code>nano hybrid.sb\n</code></pre></p>","tags":["tutorial","slurm","job script"]},{"location":"Job_Script_and_Job_Submission/#resource-request","title":"Resource request","text":"<p>The <code>hybrid</code> example likely uses a more complex set of resource requests than you will need for your own jobs, but it useful for illustrative purposes.</p> <p>Recall from the previous section that we'd like to run <code>hybrid</code> over 4 processes with 2 CPUs per process. Each process will also run on its own node. This outlines the resources we want to request.</p> <p>Let's type up the first part of the job script, the resource request. </p> <p>The very first line specifies the interpreter we want to use for our commands; in this case, it's the bash shell.</p> <p>Then, each resource request line begins with <code>#SBATCH</code>. All resources must be requested at the top of the file, before any commands, or they will be ignored.</p> <p>The request lines are as follows:</p> <ol> <li>Wall clock limit - how long will the job run? This job will run for 10 minutes.</li> <li>The number of nodes; here, 4</li> <li>The number of tasks, also known as processes, running on each node. Here we want 1.</li> <li>The number of CPUs per task. The default is one, but we've requested 2.</li> <li>The amount of memory to use per CPU. We are requesting 1 GB each.</li> <li>The name of the job, so we can easily identify it later.</li> </ol> <pre><code>#!/bin/bash --login\n\n#SBATCH --time=00:10:00\n#SBATCH --nodes=4\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=2\n#SBATCH --mem-per-cpu=1G\n#SBATCH --job-name hybrid_example\n</code></pre>","tags":["tutorial","slurm","job script"]},{"location":"Job_Script_and_Job_Submission/#job-commands","title":"Job commands","text":"<p>The resource request is just one part of writing a job script. The second part is running the job itself.</p> <p>To run our job we need to:</p> <ol> <li>Load required modules</li> <li>Change to the appropriate directory</li> <li>Run <code>hybrid</code></li> </ol> <p>We'll add the following lines to <code>hybrid.sb</code>:</p> <pre><code>module purge\nmodule load GCC/10.3.0 OpenMPI/4.1.1\n\ncd ${HOME}/MPI_OpenMP_GPU\n\nsrun -c 2 hybrid\n</code></pre> <p>Notice that we use the <code>srun</code> command to run our <code>hybrid</code> executable. This command prepares the parallel runtime environment, setting up the requested 4 processes across 4 nodes and their associated CPUs.</p> <p>You may already be familiar with <code>mpirun</code> or <code>mpiexec</code>. While <code>srun</code> is similar to these commands it is preferred for use on the HPCC because of its connection to the SLURM scheduler.</p> <p>We can also add a couple of optional commands that will save data about our job:</p> <pre><code>### write job information to SLURM output file.\nscontrol show job $SLURM_JOB_ID\n\n### write resource usage to SLURM output file (uses a powertools command).\nmodule load powertools\njs -j $SLURM_JOB_ID\n</code></pre> <p>You have now completed your job script. </p> <p>If you used <code>nano</code> to write it, hit Ctrl+X followed by Y to save, then press Enter to accept the filename.</p>","tags":["tutorial","slurm","job script"]},{"location":"Job_Script_and_Job_Submission/#final-notes","title":"Final notes","text":"<p>As was previously said, your job is most likely going to use much simpler resource specifications than shown above. You can see our example job scripts for more ideas.</p> <p>By default, SLURM will try to use the settings if overriding commands aren't specified: </p> <ul> <li><code>--nodes=1</code></li> <li><code>--tasks-per-node=1</code></li> <li><code>--cpus-per-task=1</code></li> <li><code>--time=00:01:00</code></li> <li><code>--mem-per-cpu=750M</code> </li> </ul> <p>See the curated List of Job Specifications or the <code>sbatch</code> Documentation for more options.</p>","tags":["tutorial","slurm","job script"]},{"location":"Job_Script_and_Job_Submission/#batch-job-submission","title":"Batch job submission","text":"<p>Now that we have our job script, we need to submit it to the SLURM scheduler. For this, we use the <code>sbatch</code> command:</p> <pre><code>$ sbatch hybrid.sb\nSubmitted batch job 8929\n</code></pre> <p>If the command has been submitted successfully, the job controller will issue a job ID on the screen. This ID can be used with, for example, <code>scancel</code> to cancel the job or <code>sacct</code> to look up stats about the job after it ends.</p> <p>Note the <code>sbatch</code> command only runs on development and compute nodes - it will not work on any gateway node.</p>","tags":["tutorial","slurm","job script"]},{"location":"Job_Script_and_Job_Submission/#checking-our-job-status","title":"Checking our job status","text":"<p>Once the job has been submitted, we can see it in the queue with <code>sq</code> powertool: <pre><code>module load powertools\nsq\n</code></pre></p> <p>This will show us the following information:</p> <ol> <li>The job's ID number</li> <li>The job's name, which we specified in the script</li> <li>The job's submitting user (should be your username)</li> <li>The job's state (pending, running, or completed)</li> <li>The job's current walltime</li> <li>The job's allowed walltime</li> <li>The number of nodes requested and/or allocated to the job</li> <li>The reason why the job has the status it has</li> </ol>","tags":["tutorial","slurm","job script"]},{"location":"Job_Script_and_Job_Submission/#viewing-job-outputs","title":"Viewing job outputs","text":"<p>Every SLURM job creates a file that contains the standard output and standard error from the job.</p> <p>The default name is <code>slurm-&lt;jobid&gt;.out</code> where <code>&lt;jobid&gt;</code> is the job ID assigned when the job was submitted.</p> <p>Find the output log from your job and view it with <code>less &lt;filename&gt;</code>. You should see several lines printing the thread and process information for each CPU involved.</p> <p>The SLURM log files are essential for investigating whether or not your job ran successfully and for finding out why it failed.</p>","tags":["tutorial","slurm","job script"]},{"location":"Job_with_Checkpointing_Run/","title":"Job with Checkpointing Run","text":"<p>Checkpointing is a function to save a snapshot of an application's running state, so it can restart from the saved point in case job running fails or reaches the time limit. Some applications might already have this feature for long-term computation. If users develop their own program, it is encouraged to implement checkpointing as a part of their codes. They can develop a function to write result variables to file systems at regular intervals and a function to read those variables in when restart.</p> <p>However if the program you used does not and can not include the feature, you may consider using \"Distributed MultiThreaded CheckPointing\" (or DMTCP) installed on HPCC nodes. DMTCP is a tool for transparently checkpointing the state of a distributed program spread across many machines without modifying the user's program or the operating system kernel.</p> <p>On the HPCC, you can use DMTCP directly or use the <code>longjob</code> powertool to automate this process for you. For more details about DMTCP, please refer to their web site.</p>","tags":["explanation","checkpointing"]},{"location":"Jupyter_Notebook_in_VS_Code/","title":"Running Jupyter Notebooks on the HPCC through VS Code","text":"<p>VS Code supports Jupyter notebooks. This how-to guide will show you how to run these notebooks on the HPCC while using VS Code on your personal/local computer.</p>","tags":["how-to guide","VS Code","VScode","Jupyter"]},{"location":"Jupyter_Notebook_in_VS_Code/#prerequisites","title":"Prerequisites","text":"<ol> <li>You'll need to connect VS Code to the HPCC over SSH. Pay special note that this requires setting up SSH tunneling.</li> <li>Building off the SSH tunneling needed for the previous step, you'll also need to set up tunneling to compute nodes.</li> <li>(Optional) Set up SSH keys to connect to the HPCC without needing to enter a password. For Windows users, use the MobaXterm GUI instructions (or ensure that your private SSH key is saved to <code>C:\\Users\\&lt;Account_Name&gt;\\.ssh\\id_rsa</code>).</li> <li>Install either the <code>jupyter</code> package or (if you don't plan on using Jupyter through OnDemand) the <code>ipykernel</code> package to the Python environment you plan on using. We recommend installing <code>jupyter</code> to avoid future confusion.</li> <li>Install the Python and Jupyter extensions in VS Code.</li> </ol>","tags":["how-to guide","VS Code","VScode","Jupyter"]},{"location":"Jupyter_Notebook_in_VS_Code/#start-an-interactive-job","title":"Start an Interactive Job","text":"<p>If we want to run Python and/or Jupyter on the HPCC, it's best to do so from an interactive job. This will give us a compute node on which to run our calculations. While a development node may seem more convenient, remember that jobs using more than 2 hours of CPU time are automatically cancelled! For example, we can request a single CPU for four hours: <pre><code>salloc -t 04:00:00 --mem=4GB\n</code></pre> It is strongly suggested that you request a few GB of memory. The default is only 750 MB.</p> <p>Once your interactive job starts, run the command <code>hostname</code>; e.g. <pre><code>bash-4.2$ hostname\nlac-046\n</code></pre> to get the name of the compute node you have been allocated. Copy or write down the output of this command for later.</p>","tags":["how-to guide","VS Code","VScode","Jupyter"]},{"location":"Jupyter_Notebook_in_VS_Code/#connect-vs-code-to-the-interactive-host","title":"Connect VS Code to the Interactive Host","text":"<p>Inside VS Code, press F1 and select 'Remote-SSH: Connect to Host...' You may need to start typing this option to get it to appear.</p> <p></p> <p>Note</p> <p>If this option is not available, please follow the steps in this how-to guide.</p> <p>Instead of selecting a host from the drop down list, enter the host of your interactive job into the textbox.</p> <p></p> <p>A new VS Code window will pop up. Follow any of the prompts that follow, selecting \"Continue\" and \"Linux\" as applicable. When you are successfully connect, the bottom left of the VS Code window will show the hostname you just connected to. You may open files and folders as you normally would when connecting VS Code with SSH.</p>","tags":["how-to guide","VS Code","VScode","Jupyter"]},{"location":"Jupyter_Notebook_in_VS_Code/#set-your-python-kernel","title":"Set your Python Kernel","text":"<p>Open an existing Jupyter Notebook file or create one by going to the File menu, selecting \"New File..\", and choosing the \"Jupyter Notebook\" option.  With your Notebook open, click \"Select Kernel\" in the upper right and then choose \"Python Environments.\" If a Kernel was automatically chosen for you, you'll first need to choose \"Select Another Kernel.\"</p> <p></p> <p>Pay attention to the paths given for each entry.  In the screenshot below, for example, the \"Recommended\" Python interpreter corresponds to the base Conda environment. We can also see our other Conda environments (in this example, there is only one called \"astro-analysis\") as well as system installations of Python in <code>/bin</code>, <code>/usr/bin</code>, and <code>/opt/software</code>. These system installation are likely not the Python versions you want to use.</p> <p></p> <p>You may now execute cells in your Jupyter Notebook while running on the HPCC! VS Code will remember your choice of kernel next time you open that workspace.</p>","tags":["how-to guide","VS Code","VScode","Jupyter"]},{"location":"Linux_Command_Line_Interface_for_Beginners_I/","title":"Linux Command Line Interface for Beginners I","text":"<p>The operating system of the MSU HPC is CentOS, which is a distribution of Linux. So if you want to use our system, it is essential to equip some basic knowledge of Linux. Even though Linux supports a GUI, most works are done on a terminal via text. The Linux command line is a text interface to Linux.\u00a0</p> <p>We will walk through some practical exercises to become familiar with a few basic commands and concept.</p>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#navigation","title":"Navigation","text":"<p>Let's run the first command. Type <code>pwd</code> and pressing the Enter or Return key to run it (From now, I'll not mention pressing Enter/Return key to run a command).</p> <pre><code>$ pwd\n/mnt/home/iamsam/\n</code></pre> <p>You will see a path such as <code>/mnt/home/user/your_id</code></p> <p><code>pwd</code>\u00a0is an abbreviation of 'print working directory'. It prints out the shell's current working directory. You can change the working directory using the <code>cd</code> command, an abbreviation for 'change directory'.</p> <pre><code>$ cd /\n$ pwd\n/\n</code></pre> <p>Now your working directory is '/' which is the root directory.\u00a0There is nothing much you can do on the root directory, so let's go to your 'home' directory.</p> <pre><code>$ cd \n$ pwd\n/mnt/home/iamsam/\n</code></pre> <p>Regardless of your location, when you just type <code>cd</code>, you will be home. You can also type <code>cd \\~</code> instead of <code>cd</code> to be back your home.</p> <p>To go the previous directory, type <code>cd -</code></p> <pre><code>$ cd -\n$ pwd\n/\n</code></pre> <p>The root directory has many subdirectories including your home directory. Let's go to the 'bin' directory.</p> <pre><code>$ cd bin\n$ pwd\n/bin\n</code></pre> <p>To go up to the parent directory (it is / for us now), use the special syntax of two dots with <code>cd</code> such as</p> <pre><code>$ cd ..\n$ pwd\n/\n</code></pre> <p>To go up to the previous directory, use <code>-</code> with <code>cd</code> such as</p> <pre><code>$ cd -\n$ pwd\n/bin\n</code></pre> <p>You can use <code>..</code> more than once if you have to move up multiple levels of parent directories.</p> <pre><code>$ cd ../..\n$ pwd\n</code></pre>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#relative-and-absolute-paths","title":"Relative and absolute Paths","text":"<p>A path is an address of a directory. Most of the examples we've looked at so far use relative paths. So your final location is decided based on your current working directory. However, sometimes you want to use an absolute path than a relative one. Your home's absolute path at HPC is <code>/mnt/home/your_id</code>. See the example to find how to use a relative and absolute path.</p> <pre><code>$ cd /\n$ pwd\n/\n$ cd -\n$ pwd\n/mnt/home/iamsam\n$ cd /\n$ cd /mnt/home/iamsam\n$ pwd\n/mnt/home/iamsam\n</code></pre>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#creating-and-removing-directories","title":"Creating and removing directories","text":"<p>To make a directory, use <code>mkdir</code> (short for 'make directory') such as</p> <pre><code>$ mkdir temp01\n$ls\ntemp01\n</code></pre> <p><code>ls</code> is a command to list files and folder. We will learn it in a minute. You can create multiple directories as well.</p> <pre><code>$ mkdir temp02 temp03 temp04\n$ ls\ntemp01 temp02 temp03 temp04\n</code></pre> <p>To make a subdirectory, use with <code>-p</code> option such as</p> <pre><code>$ mkdir -p temp04/temp05/temp06\n$ ls temp04\ntemp05\n</code></pre> <p>To remove directory use <code>rm</code> command with <code>-r</code>. Without <code>-r</code> option, rm will not delete directories (but you can delete files). <code>-r</code> means recursive.</p> <pre><code>$ rm temp04\nrm: cannot remove 'temp04': Is a directory\n$ rm -r temp04\n</code></pre>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#creating-and-removing-files","title":"Creating and removing files","text":"<p>You can use editors to create files, but it is out of scope of this tutorial. Let's use <code>ls</code> and a pipe <code>&gt;</code>\u00a0(we will explain pipes later).</p> <pre><code>$ ls\ntemp01 temp02 temp03\n$ ls &gt; list.txt\n$ ls\nlist.txt temp01 temp02 temp03\n</code></pre> <p>Now we have three directories (<code>temp01</code>, <code>temp02</code>, <code>temp03</code>) and one file (<code>list.txt</code>). We already know how to delete directories. To delete files, we use command <code>rm</code> such as</p> <pre><code>$ rm list.txt\n$ ls\ntemp01 temp02 temp03\n</code></pre>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#copying-and-renaming-directories-and-files","title":"Copying and renaming directories and files","text":"<p>To copy files or directories, use <code>cp</code> such as</p> <pre><code>$ cp list.txt list2.txt\n$ ls\nlist.txt list2.txt temp01 temp02 temp03\n</code></pre> <p>With the <code>-r</code> option, you can copy files and directories recursively, i.e., copy subdirectories and files.</p> <p>The <code>mv</code> command moves files/directories or renames them.</p> <pre><code>$ mv list2.txt temp01\n$ ls\nlist.txt temp01 temp02 temp03\n\n$ ls temp01\nlist2.txt\n\n$ cd temp01\n$ mv list2.txt list3.txt\n$ ls\nlist3.txt\n</code></pre>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#listing-directories-and-files","title":"Listing directories and files","text":"<p>We already used ls. To list directories and files, us\u00a0ls (short for 'list').\u00a0</p> <pre><code>$ ls\ntemp01 temp02 temp03\n</code></pre> <p>There are many options for <code>ls</code>. Most frequently used options are</p> <ul> <li><code>-a</code>: list all files and directories including hidden contents</li> <li><code>-h</code>: print sizes in human readable format (e.g.: 1K, 2.4M, 3.1G)</li> <li><code>-l</code>: list with a long listing format</li> <li><code>-t</code>: sort my modification time</li> </ul> <p>You can use options separately like <code>ls -l -a -t</code> or together like <code>ls -lat</code>.</p> <pre><code>$ ls -l -a -t\ntotal 8\n-rw-r--r--    1 iamsam  staff    30 Mar 23 15:37 list.txt\ndrwxr-xr-x    6 iamsam  staff   192 Mar 23 15:37 .\ndrwxr-xr-x    2 iamsam  staff    64 Mar 23 15:37 temp03\ndrwxr-xr-x    2 iamsam  staff    64 Mar 23 15:37 temp02\ndrwxr-xr-x    2 iamsam  staff    64 Mar 23 15:37 temp01\ndrwxr-xr-x+ 113 iamsam  staff  3616 Mar 23 15:21 ..\n\n$ ls -lat\ntotal 8\n-rw-r--r--    1 iamsam  staff    30 Mar 23 15:37 list.txt\ndrwxr-xr-x    6 iamsam  staff   192 Mar 23 15:37 .\ndrwxr-xr-x    2 iamsam  staff    64 Mar 23 15:37 temp03\ndrwxr-xr-x    2 iamsam  staff    64 Mar 23 15:37 temp02\ndrwxr-xr-x    2 iamsam  staff    64 Mar 23 15:37 temp01\ndrwxr-xr-x+ 113 iamsam  staff  3616 Mar 23 15:21 ..\n</code></pre>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#exercises","title":"Exercises","text":"<p>Now let's do some exercises.\u00a0</p> <ul> <li>Log in your MSU HPC account and go to any dev-node.\u00a0</li> <li>Create a <code>linux_tutorial</code>\u00a0dir on your home.</li> <li>Copy a folder and contents for this tutorial from\u00a0</li> <li><code>/mnt/research/common-data/workshops/intro2Linux_iamsam</code> to   <code>linux_tutorial</code>\u00a0dir on your home</li> <li>Go to <code>linux_tutorial</code></li> <li>Find a hidden directory and rename it to <code>not_hidden</code></li> <li>Check the contents of <code>not_hidden</code></li> <li>Create a new directory called <code>new_dir</code></li> <li>Copy the file <code>youfoundit.txt</code> into <code>new_dir</code></li> <li>Remove <code>garbage</code> dir</li> </ul> This is an answer (not including the login process). <pre><code>$ mkdir linux_workshop\n$ cp -r /mnt/research/common-data/workshops/intro2Linux_iamsam linux_tutorial\n$ cp linux_tutorial\n$ ls -a\n$ mv .hidden not_hidden\n$ ls not_hidden\n$ mkdir new_dir\n$ cp not_hidden/youfoundit.txt new_dir\n$ rm -r garbage\n</code></pre>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/","title":"Linux Command Line Interface for Beginners II","text":"","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#files-and-folders","title":"Files and folders","text":"<p>Linux has a single directory 'tree', separated by slash. The top of the tree is the 'root' directory . All additional disks are connected on <code>/mnt</code> ('mounted'). In Linux, there is no concept of drive letters.</p> <p><code>tree</code> is a recursive directory listing program which prints a depth indented listing of files. With no arguments, tree lists the files in the current directory. You can change the depth with <code>-L</code> argument.</p> <p>The example shows the structure of the <code>linux_tutorial</code> directory which we used for an exercise.</p> <pre><code>$ tree\n.\n\u251c\u2500\u2500 000.txt\n\u251c\u2500\u2500 001.txt\n\u251c\u2500\u2500 002.txt\n\u251c\u2500\u2500 009.txt\n\u251c\u2500\u2500 010.txt\n\u251c\u2500\u2500 a.txt\n\u251c\u2500\u2500 A.txt\n\u251c\u2500\u2500 b.txt\n\u251c\u2500\u2500 B.txt\n\u251c\u2500\u2500 c.txt\n\u251c\u2500\u2500 C.txt\n\u251c\u2500\u2500 garbage\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 garbage_01\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 garbage_02\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 garbage_03\n\u251c\u2500\u2500 my_data.dat\n\u251c\u2500\u2500 my-data.txt\n\u251c\u2500\u2500 z.txt\n\u2514\u2500\u2500 Z.txt\n\n$ tree -L 1\n.\n\u251c\u2500\u2500 000.txt\n\u251c\u2500\u2500 001.txt\n\u251c\u2500\u2500 002.txt\n\u251c\u2500\u2500 009.txt\n\u251c\u2500\u2500 010.txt\n\u251c\u2500\u2500 a.txt\n\u251c\u2500\u2500 A.txt\n\u251c\u2500\u2500 b.txt\n\u251c\u2500\u2500 B.txt\n\u251c\u2500\u2500 c.txt\n\u251c\u2500\u2500 C.txt\n\u251c\u2500\u2500 garbage\n\u251c\u2500\u2500 my_data.dat\n\u251c\u2500\u2500 my-data.txt\n\u251c\u2500\u2500 z.txt\n\u2514\u2500\u2500 Z.txt\n</code></pre> <p><code>tree</code> command does not show hidden files/directory by default. To see hidden files/directories, use <code>-a</code> argument.</p> <pre><code>$ tree -a\n.\n\u251c\u2500\u2500 000.txt\n\u251c\u2500\u2500 001.txt\n\u251c\u2500\u2500 002.txt\n\u251c\u2500\u2500 009.txt\n\u251c\u2500\u2500 010.txt\n\u251c\u2500\u2500 a.txt\n\u251c\u2500\u2500 A.txt\n\u251c\u2500\u2500 b.txt\n\u251c\u2500\u2500 B.txt\n\u251c\u2500\u2500 c.txt\n\u251c\u2500\u2500 C.txt\n\u251c\u2500\u2500 garbage\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 garbage_01\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 garbage_02\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 garbage_03\n\u251c\u2500\u2500 .hidden\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 youfoundit.txt\n\u251c\u2500\u2500 my_data.dat\n\u251c\u2500\u2500 my-data.txt\n\u251c\u2500\u2500 z.txt\n\u2514\u2500\u2500 Z.txt\n\n2 directories, 19 files\n</code></pre>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#file-permissions","title":"File permissions","text":"<p>Linux has a method to keep files private and safe. When you type\u00a0<code>ls -a</code> you will get a lot of information of your directory including files such as</p> <p></p> <p>The first letter shows a type of a file.</p> <ul> <li><code>-</code>: normal file</li> <li><code>d</code>: directory</li> <li><code>l</code>: symbolic link</li> </ul> <p>The next nine characters shows permission; first three is for you, next three is for your group members, and last three is for a whole world. Let's look at the first file in the picture which shows <code>d rwxr-xr-x</code>.</p> <p>The first character is 'd', and therefore, even though the name is 'hello.txt', it is not a file, but a directory. Next nine characters represent the settings for the three sets of permissions.</p> <ul> <li>The first three characters show the permissions for the user who   owns the file (user permissions).</li> <li>The middle three characters show the permissions for members of the   file\u2019s group (group permissions).</li> <li>The last three characters show the permissions for anyone not in the   first two categories (other permissions).</li> </ul> <p>The letters represent:</p> <ul> <li><code>r</code>: Read permissions. The file can be opened, and its content viewed.</li> <li><code>w</code>: Write permissions. The file can be edited, modified, and deleted.</li> <li><code>x</code>: Execute permissions. If the file is a script or a program, it can   be run (executed).</li> </ul> <p>In our screenshot, the first three characters are <code>rwx</code> which means you (owner) can read, write/modify or execute (it means you can go inside the directory here). Next\u00a0three characters are <code>r_x</code> which means your group members can read, or execute, but can not modify the contents. Last\u00a0three characters are <code>r_x</code> which means everyone else can read, and execute.</p> <p>Another examples:</p> <ul> <li><code>---------</code>: it means no permissions have been granted at all.</li> <li><code>rwxrwxrwx</code>: it means full permissions have been granted to everyone.</li> </ul> <p>Permissions can be set with <code>chmod</code> command, and ownership set with <code>chown</code>. You can only change these for files you are the owner of. To use <code>chmod</code>, we need to tell it 'who' we are setting permissions for, 'what' change we are making, 'which' permissions we are setting.</p> <p>The \u201cwho\u201d values can be:</p> <ul> <li><code>u</code>: User - the owner of the file.</li> <li><code>g</code>: Group - members of the group the file belongs to.</li> <li><code>o</code>: Others - people not governed by the u and g permissions.</li> <li><code>a</code>: All - all of the above.</li> </ul> <p>If none of these are used, <code>chmod</code> behaves as if <code>a</code> had been used.</p> <p>The \u201cwhat\u201d values can be:</p> <ul> <li><code>\u2013</code>: Minus sign. Removes the permission.</li> <li><code>+</code>: Plus sign. Grants the permission. The permission is added to the   existing permissions. If you want to have this permission and only   this permission set, use the = option, described below.</li> <li><code>=</code>: Equals sign. Set a permission and remove others.</li> </ul> <p>The \u201cwhich \u201d values can be:</p> <ul> <li><code>r</code>:\u00a0 The read permission.</li> <li><code>w</code>: The write permission.</li> <li><code>x</code>: The execute permission.</li> </ul> <p>You can also use a three three-digit numbers (total nine) to provide the permission with\u00a0<code>chmod</code>. The leftmost digit represents the permissions for the owner. The middle digit represents the permissions for the group members. The rightmost digit represents the permissions for the others. <code>x</code> is 1, <code>w</code> is 2, and <code>r</code> is 4, and the some of these three numbers are the permission.</p> <p>The digits you can use and what they represent are listed here:</p> <ul> <li>0: (000) No permission.</li> <li>1: (001) Execute permission.</li> <li>2: (010) Write permission.</li> <li>3: (011) Write and execute permissions.</li> <li>4: (100) Read permission.</li> <li>5: (101) Read and execute permissions.</li> <li>6: (110) Read and write permissions.</li> <li>7: (111) Read, write, and execute permissions.</li> </ul> <p>The tables shows a summary of the <code>chmod</code> command.</p> User Type Permission u - \u00a0user + add r - read (4) g - group - delete w - write (2) o - others = set exactly x - execute (1) a - all","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#examples","title":"Examples","text":"<p><code>chmod u=rx, og=r my_file1.txt</code> : set the user has read, and executable permissions; group/other have read permission only for <code>my_file1.txt</code>.</p> <p><code>chmod 544 my_file1.txt</code>: same as\u00a0<code>chmod u=rx, og=r my_file1.txt</code>.\u00a0</p> <p><code>chmod 750 my_file1.txt</code>:\u00a0set the user has read, write, and executable permissions; group has read/executable permission; others no permission.</p>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#concatenate-files","title":"Concatenate files","text":"<p>The <code>cat</code> (short for \"concatenate\") command is one of the most frequently used command in Linux. <code>cat</code> command allows us to create single or multiple files, view contain of file, concatenate files and redirect output in terminal or files.</p> <ul> <li>To display contents of a file: <code>cat filename</code></li> <li>To view contents of multiple files: <code>cat file1 file2</code></li> <li>To create a file: <code>cat\u00a0&gt; file2.txt</code></li> </ul> <p>If file content is large, and won't fit in a terminal screen, you can use <code>more</code> and <code>less</code> with <code>cat</code> command such as (we are using pipes,\u00a0<code>|</code>, which will be covered later)</p> <ul> <li><code>cat file2.txt | more</code></li> <li><code>cat file2.txt | less</code></li> </ul> <p><code>more</code>, <code>less</code>, and <code>most</code>\u00a0(<code>most</code> has more features than <code>more</code> and <code>less</code> commands) are commands to open a given file for interactive reading.</p>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#redirection","title":"Redirection","text":"<p>Redirection is a feature such that when you execute a command with it, you can change the standard input/output devices. The standard input (stdin) device is the keyboard, and the standard output (stdout) device is the screen. With redirection, stdin/stdout can be changed.</p> <ul> <li><code>&gt;</code>: Output redirection (overwriting)<ul> <li>e.g. <code>ls -la\u00a0&gt; list.txt</code> (<code>ls -la</code> results save as <code>list.txt</code> instead of   standard output (screen))</li> </ul> </li> <li><code>&gt;&gt;</code>: Output redirection (appending)<ul> <li>e.g.\u00a0<code>ls -l\u00a0&gt;&gt; list.txt</code> (<code>ls -l</code> result will be appended at the end of   the <code>list.txt</code>. If there is no <code>list.txt</code>, it works as <code>ls -la\u00a0&gt; list.txt</code>)</li> </ul> </li> <li><code>&lt;</code>: Input redirection</li> <li><code>&gt;&amp;</code>: Writing the output from one file to the input of another file<ul> <li>e.g. <code>matlab\u00a0&gt; outfile 2&gt;&amp;1</code> : send stdout and stderr to <code>outfile</code>. Here   1 and 2 are file descriptors. File descriptor 1 is the standard output   (stdout), and\u00a02 is the standard error (stderr).\u00a0<code>&gt;</code> is redirection,   and <code>&amp;</code> indicates that what follows and precedes is a file descriptor   and not a filename.</li> </ul> </li> </ul>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#wildcards","title":"Wildcards","text":"<p>Wildcards are symbols or special characters that represent other characters. You can use them with any command such as <code>ls</code>/<code>rm</code>/<code>cp</code> etc.</p> <ul> <li><code>*</code>: anything or nothing</li> <li><code>?</code>: single character</li> <li><code>[ ]</code>: any character or range of characters</li> <li><code>[! ]</code>: inverse the match of <code>[ ]</code></li> </ul>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#examples_1","title":"Examples","text":"<ul> <li><code>ls *.txt</code>: list all txt files</li> <li><code>ls *-?.txt</code>: list all files with <code>-</code> and with one character in front of <code>.txt</code></li> <li><code>ls [0-9]*.txt</code>: list all files starting with a number</li> <li><code>ls [A-Z]*.txt</code>: list all files starting with a capital letter?\u00a0<code>[A-Z]</code> can be different based on <code>LC_COLLATE</code> value. For further discussion, check here. In HPC at MSU, the default of\u00a0<code>[A-Z]</code> is\u00a0\u00a0a, A, b, B, c, C, ....y, Y, z, Z, which is standard collations (en_US).\u00a0</li> <li>Try <code>ls [[:lower:]].txt</code>;\u00a0 <code>ls [[:upper:]].txt</code>;\u00a0 <code>ls [[:lower:][:upper:]].txt</code></li> <li><code>ls [!a-Z]*.txt</code>: list txt files that don\u2019t begin with any letter.</li> </ul> <p>For more information, refer to\u00a0Regular Expressions.</p>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#manuals","title":"Manuals","text":"<p>Linux includes a built in manual for nearly all commands. Type <code>man</code> followed by the commands, e.g., <code>man man</code>.</p> <p>To navigate the man pages use the arrow keys to scroll up and down or use the enter key to advance a line, space bar to advance a page, letter u to go back a page. Use the q key to quit out of the display.</p> <p>The manual pages often include these sections:</p> <ul> <li>Name: a one line description of what it does</li> <li>Synopsis: basic syntax for the command line.</li> <li>Description: describes the program\u2019s functionalities.</li> <li>Options: lists command line options available for this program.</li> <li>Example: examples of some of the options available.</li> <li>See Also: list of related commands.</li> </ul> <p>Note that options can be with single dash <code>-</code> or double dash <code>--</code></p>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#commands-for-monitoring-files","title":"Commands for monitoring files","text":"<p>Following commands are used for monitoring files. </p> <ul> <li><code>wc</code>: count words, lines or characters<ul> <li>e.g. <code>who | wc -l</code>: number of users logged in</li> </ul> </li> <li><code>grep</code>: find patterns in files or data, returns just matching lines<ul> <li>e.g. <code>who | grep $USER</code>: find your username in users logged in</li> </ul> </li> <li><code>sort</code>: given a list of items, sort in various ways<ul> <li>e.g. <code>who | sort</code>: sort the users logged in</li> </ul> </li> <li><code>head</code>: list only top n (5 by default) lines of file<ul> <li>e.g. <code>who &gt; who.txt; head who.txt</code>: save the users logged in to a file and show the first five</li> </ul> </li> <li><code>tail</code>: list only last n (5 by default) lines of file</li> </ul>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#archiving-and-compression","title":"Archiving and compression","text":"<p>Following commands are used for archiving and compression: <code>zip</code>, <code>unzip</code>, <code>tar</code>.</p> <ul> <li><code>unzip</code>: unzip a file</li> <li><code>tar</code>: create (<code>tar -c</code>) or extract (<code>tar -x</code>) \u2018tape archive\u2019 file.</li> </ul>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#exercise","title":"Exercise","text":"<p>Using the man pages, find out what the default number of lines that head and tail will display, and how to limit those to just one line.</p> <p>Can you <code>tar</code> all files and folders in workshop folder? Question? Use <code>man</code>.\u00a0</p>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#environment-variables","title":"Environment variables","text":"<p>Shell maintains and you can set \u2018variables\u2019 that the shell uses for configuration and in your script.\u00a0Variables start with <code>$</code>, and can be seen with <code>echo $VARNAME</code>.</p> <p>Explore common variables with the echo command and list what they are\u00a0<code>$HOME</code>, <code>$USER</code>, <code>$SHELL</code>, <code>$PATH</code>.</p> <p>For more information, please refer to\u00a0Variables I.</p>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#see-also","title":"See also","text":"<ul> <li>Introduction to UNIX Course Online</li> <li>Unix Tutorial for Beginners</li> </ul>","tags":["tutorial","command line"]},{"location":"Linux_Shell/","title":"Linux shells","text":"<p>Note</p> <p><code>bash</code> is the only officially supported shell for the HPCC. For information about running shell scripts written for other shells (zsh, csh, etc) or using an alternative shell, see this Lab Notebook.  </p>","tags":["reference"]},{"location":"Linux_Shell/#linux-shells","title":"Linux shells","text":"<p>A Unix/Linux shell is a command-line interpreter which provides a user interface for the Unix/Linux operating system. Users control the operation of a computer by submitting single commands or by submitting one or more commands via a shell script. </p> <p>The default shell provided to HPCC users is the <code>bash</code> shell.</p> <p>To find out your current shell run <code>echo $SHELL</code>.</p>","tags":["reference"]},{"location":"Linux_Shell/#environment-variables","title":"Environment variables","text":"<p>Environment variables are a set of dynamically named values which can control the way running processes will behave on a computer. Many of the Unix commands and tools require certain environment variables to be set. Many of these are set automatically for the users when they log in or load applications via the module command. </p> <p>To view your current set of environment variables run <code>env</code>. </p> <p>To assign a new value to an environment variable in either <code>bash</code>: <code>export &lt;name&gt;=&lt;value&gt;</code></p> <p>To print the value of a variable: <code>echo $&lt;name&gt;</code></p>","tags":["reference"]},{"location":"Linux_Shell/#commonly-used-environment-variables","title":"Commonly used environment variables","text":"<p>Bash variables are preceded with $ and optionally enclosed in brackets when used e.g. <code>$USER</code> or <code>${USER}</code>.</p> <ul> <li><code>$HOSTNAME</code>:  Name of the computer currently running the shell or script. If used in a SLURM job, this should be one of the nodes listed in the variable <code>$SLURM_JOB_NODELIST</code> </li> <li><code>$USER</code>: User's name (NetID). Useful if you would like to dynamically generate a directory on some scratch space.</li> <li><code>$HOME</code>: User's home directory. Can also use <code>~/</code> symbol.</li> <li><code>$SCRATCH</code>: Your folder on the scratch disk.                                                   </li> <li><code>$TMPDIR</code>: Temporary working folder of a running job at <code>/mnt/local/$SLURM_JOBID</code>. </li> </ul> <p>To see all variables in the context of your job, add this line to your job script, which will list all variables that contain the word 'SLURM'</p> <pre><code>$ env | grep SLURM\n</code></pre>","tags":["reference"]},{"location":"List_Jobs_by_squeue_sview/","title":"List Jobs by squeue &amp; sview","text":""},{"location":"List_Jobs_by_squeue_sview/#squeue-command","title":"squeue command","text":"<p>After you submit jobs, you can check their information and status in the queue. The simplest way is to use squeue command to list all of your jobs:</p> <pre><code>$ squeue -l -u $USER\nThu Aug  2 14:45:57 2018\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n              9405 general-s     Job1   nobody  RUNNING       0:32  01:00:00      1 lac-198\n              9406 general-s Rmpi_tes   nobody  RUNNING       0:11     30:00      2 lac-[386-387]\n              9290 general-l  LongJob   nobody  PENDING       0:00  36:00:00     40 (Resources)\n</code></pre> <p>where the argument <code>-l</code> reports more of the available information (i.e. <code>l</code> for long format) and <code>-u</code> specifies which user's jobs to show. You may find a complete squeue specifications from the SLURM web site.</p> <p>HPCC staff also wrote some powertools commands so users can see their jobs conveniently. Before using\u00a0powertools commands, please make sure powertools module is loaded by using moudle list (By default,  the powertools module should be loaded unless the user has purged modules  with module purge)</p> <pre><code>$ module list\n\nCurrently Loaded Modules:\n  1) powertools/1.2\n</code></pre> <p>One of the commands sq works the same as the above squeue command:</p> <pre><code>$ sq                         # powertools command\nThu Aug  2 14:48:51 2018\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n              9405 general-s     Job1 username  RUNNING       0:35  01:00:00      1 lac-198\n              9406 general-s Rmpi_tes username  RUNNING       0:14     30:00      2 lac-[386-387]\n              9290 general-l  LongJob username  PENDING       0:00  36:00:00     40 (Resources)\n</code></pre> <p>where it shows the job IDs, job partition, job name, username, job state, elapsed time, walltime limit, total number of nodes and the node list or the waiting reason for the user's each jobs. Users can also use qs command to see more information:</p> <pre><code>$ qs                         # powertools command\nThu Aug  2 14:49:27 2019\n                                                                                        Start_Time/\n     JobID         User     Account     Name   Node CPUs  TotMem  Tres   WallTime  ST  Elapsed_Time  NodeList(Reason)\n-----------------------------------------------------------------------------------------------------------------------\n    60889405    MyHPCCAcc   general       Job1    1   16      2G  gpu:1    3:55:00  R      1:49:24   lac-198\n    60889496    MyHPCCAcc   general  Rmpi_test    2    8   1500M   N/A       30:00  R        29:46   lac-[386-387]         \n    60889290    MyHPCCAcc  classres    LongJob   40   80    750M  k80:1 3-00:00:00 PD 08-03T10:29:41 (Resources)\n</code></pre> <p>where more items, such as, total number of CPUs, memory, gpu per node and job start time (for pending jobs) or elapsed time (for running jobs) are shown. For a complete usage of squeue command, please refer to the SLURM web site.</p>"},{"location":"List_Jobs_by_squeue_sview/#sview-command","title":"sview command","text":"<p>Besides the text listing of the jobs, SLURM also offer a command to show the squeue information with a graphical interface. Use the command sview:</p> <pre><code>$ sview\n</code></pre> <p>You will see an image of a job list displaying all jobs in the queue:</p> <p></p> <p>Click on each job, it will pop out another window and show the detailed information. For a complete usage of sview command, please refer to the SLURM web site.</p>"},{"location":"List_of_Job_Specifications/","title":"List of Job Specifications","text":"<p>The following are lists of basic <code>#SBATCH</code> specifications, broken up by purpose. To see the complete list of options, please refer to the SLURM <code>sbatch</code> command page, but be aware that not all options are implemented for the HPCC.</p>","tags":["reference","slurm","job script"]},{"location":"List_of_Job_Specifications/#resource-requests","title":"Resource Requests","text":"<p>These options specify the computing resources needed for your job. Not all options need to be specified. See our example batch scripts.</p> Option Description Examples <code>-A</code>,<code>--account=&lt;account&gt;</code> This option tells SLURM to use the specified buy-in account. Unless you are an  authorized user of the account, your job will not run. <code>#SBATCH   -A &lt;account&gt;</code> <code>-C</code>,<code>--constraint=&lt;list&gt;</code> Request node feature. May be specified with symbol <code>&amp;</code> for and, <code>|</code> for or, etc.<p>Constraints using <code>|</code> must be prepended with <code>NOAUTO:</code>. See the Job Constraints page for more information. <code>#SBATCH -C NOAUTO:intel16|intel14</code> <code>-c</code>,<code>--cpus-per-task=&lt;ncpus&gt;</code> Require <code>&lt;ncpus&gt;</code> number of processors per task. Without this option, the controller will just try to allocate one processor per task.<p>Due to changes with <code>srun</code>, this option must also be specified when calling <code>srun</code> inside your script. <code>#SBATCH   -c 3</code> (3 cores per node) <code>-G</code>,<code>--gpus=[&lt;type&gt;:]&lt;number&gt;</code> Specify the total number of GPUs required for the job. An optional GPU type specification can be supplied. Valid GPU types are <code>k20</code>, <code>k80</code>, <code>v100</code> and <code>a100</code>. Note that type is optional, but the number of GPUs is necessary. The allocation has to contain at least one GPU per node. <code>#SBATCH   --gpus=k80:2</code> (request 2 k80 GPUs for entire job) <code>#SBATCH --gpus=2</code> (request 2 GPUs for entire job) <code>--gpus-per-node=[&lt;type&gt;:]&lt;number&gt;</code> Specify the number of GPUs required for the job on each node included in the job's resource allocation. GPUs are specified in the same format as <code>--gpus</code>. <code>#SBATCH   --gpus-per-node=v100:8</code> (request 8 v100 GPUs for each node requested by job) <code>#SBATCH --gpus-per-node=8</code> (request 8 GPUs for each node requested by job) <code>--gpus-per-task=[&lt;type&gt;:]&lt;number&gt;</code> Specify the number of GPUs required for the job on each task to be spawned in the job's resource allocation. GPUs are specified in the same format as <code>--gpus</code>.<p>This option requires an explicit task count, e.g. <code>-n</code>, <code>--ntasks</code> or <code>--gpus=X --gpus-per-task=Y</code>. This option will implicitly set <code>--gpu-bind=per_task:&lt;gpus_per_task&gt;</code>, but that can be overridden (see the full documentation). <code>#SBATCH   --gpus-per-task=k80:2</code> (request 2 k80 GPUs for each task requested by job)<code>#SBATCH --gpus-per-task=2</code> (request 2 GPUs for each task requested by job) <code>--mem=&lt;size[units]&gt;</code> Specify the real memory required per node. Default units are megabytes. Different units can be specified using the suffix [K|M|G|T]. <p> Mutually exclusive with <code>--mem-per-cpu</code> and <code>--mem-per-gpu</code>. <code>#SBATCH   --mem=2G</code> <code>--mem-per-cpu=&lt;size[units]&gt;</code> Memory required per allocated CPU. Default units are megabytes. Different units can be specified using the suffix [K|M|G|T]. <p> Mutually exclusive with <code>--mem</code> and <code>--mem-per-gpu</code>. <code>#SBATCH   --mem-per-cpu=2G</code> <code>--mem-per-gpu=&lt;size[units]&gt;</code> Memory required per allocated GPU. Default units are megabytes. Different units can be specified using the suffix [K|M|G|T]. <p> Mutually exclusive with <code>--mem</code> and <code>--mem-per-cpu</code>. Does not impact the amount of GPU memory users may access. See Requesting GPUs for more. <code>#SBATCH   --mem-per-gpu=2G</code> <code>-N</code>,<code>--nodes=&lt;minnodes[-maxnodes]&gt;</code> Request that a minimum of <code>minnodes</code> nodes be allocated to this job. A maximum node count may also be specified with <code>maxnodes</code>. If only one number is specified, this is used as both the minimum and maximum node count.<p>The job will be allocated as many nodes as possible within the range specified and without delaying the initiation of the job.  Note that the environment variable <code>SLURM_JOB_NUM_NODES</code> will be set to the count of nodes actually allocated to the job. If not specified, the default behavior is to allocate enough nodes to satisfy the other requested resources as expressed by per-job specification options. <code>#SBATCH   --nodes=2-4</code> (Request 2 to 4 different nodes) <code>-n</code>,<code>--ntasks=&lt;number&gt;</code> This option advises the Slurm controller that job steps run within the allocation will launch a maximum of <code>number</code> tasks and to provide for sufficient resources. Actual tasks must be launched by application within the job script.<p>The default is one task per node, but note that the <code>--cpus-per-task</code> option will change this default. Use options such as <code>--ntasks-per-node</code> for finer control over the distribution of tasks. <code>#SBATCH   -n 4</code> (All tasks could be in 1 to 4 different nodes) <code>--ntasks-per-node=&lt;ntasks&gt;</code> Specify the number of tasks to run per node. Meant to be used with the <code>--nodes</code> option.<p>This is related to <code>--cpus-per-task</code>, but does not require knowledge of the actual number of cpus on each node. If used with <code>--ntasks</code>, the <code>--ntasks</code> option will take precedence and <code>--ntasks-per-node</code> will be treated as a maximum count of tasks per node. <code>#SBATCH --ntasks 4</code> <code>-q</code><code>--qos=&lt;qos&gt;</code> Request a quality of service. The HPCC allows you to specify the scavenger QOS. <code>#SBATCH --qos=scavenger</code> <code>-t</code>,<code>--time=&lt;time&gt;</code> Set a limit on the total run time of the job allocation. The total run time in the form: HH:MM:SS or DD-HH:MM:SS <code>#SBATCH   -t 00:20:00</code> <code>-w</code>,<code>--nodelist=&lt;node name list&gt;</code> Request a specific list of your buy-in nodes. The job will contain all of these hosts and possibly additional hosts as needed to satisfy resource requirements.<p>The list may be specified as a comma-separated list of hosts, a range of hosts, or a filename. The host list will be assumed to be a filename if it contains a <code>/</code> character. <code>#SBATCH -w host1,host2,host3,...</code><code>#SBATCH -w host[1-5,7,...]</code><code>#SBATCH -w /mnt/home/userid/nodelist</code> <code>-x</code>,<code>--exclude=&lt;node name list&gt;</code> Explicitly exclude certain nodes from the resources granted to the job. The syntax follows that of <code>--nodelist</code>. <code>#SBATCH -x host[1-5]</code>","tags":["reference","slurm","job script"]},{"location":"List_of_Job_Specifications/#job-environment","title":"Job Environment","text":"<p>As opposed to the computing resources needed by a job, these parameters impact the computing environment in which a SLURM job is running. </p> Option Description Examples <code>-a</code>,<code>--array=&lt;indexes&gt;</code> Submit a job array with multiple jobs to be executed; that is, a group of jobs requiring the same set of resourcs. Each job has the same job ID (<code>$SLURM_JOB_ID</code>) but different array ID (<code>$SLURM_ARRAY_TASK_ID</code>). The <code>indexes</code> passed to the argument identify what array ID values should be used.<p>The indicies can be a mix of lists or ranges. A <code>:</code> can be used to identify the step for a range. The <code>%</code> separator specifies the maximum number of jobs running simultaneously. <code>#SBATCH -a 0-15</code>,<code>#SBATCH -a 0,6,16-32</code>,<code>#SBATCH -a 0-15:4</code> (same as <code>#SBATCH \u2013a 0,4,8,12</code>),<code>#SBATCH --array=0-15%4</code> (max 4 jobs running simultaneously) <code>-D</code>,<code>--chdir=&lt;directory&gt;</code> Set the working directory of the batch script to <code>&lt;directory&gt;</code> before it is executed. The path can be specified as full path or relative path to the directory where the command is executed. <code>#SBATCH   -D /mnt/scratch/username</code> <code>--export=[ALL,]&lt;environment variables&gt;</code><code>--export=NONE</code> Identify which environment variables are propagated to the launched application. By default, all are propagated. Multiple environment variable names should be comma-separated. If <code>NONE</code>, SLURM will attempt to load the user's environment on the node the job is being executed. <code>#SBATCH --export=ALL,EDITOR=/bin/emacs</code><code>#SBATCH --export=NONE</code> <code>-J</code>,<code>--job-name=&lt;jobname&gt;</code> Specify a name for the job allocation. <code>#SBATCH   -J MySuperComputing</code> <code>-L</code>,<code>--licenses=&lt;license&gt;</code> Specification of licenses (or other resources available on all nodes of the cluster) which must be allocated to this job. <code>#SBATCH   -L comsol@1718@lm-01.i</code>","tags":["reference","slurm","job script"]},{"location":"List_of_Job_Specifications/#job-io-and-notifications","title":"Job I/O and Notifications","text":"<p>When running interactively, users have access to  standard input, output, and error via the command line. When running as a batch job, SLURM handles each of these streams as files. By default, standard output and error are combined into the same <code>output</code> file.</p> Option Description Examples <code>-e</code>,<code>--error=&lt;filename&gt;</code> Instruct SLURM to connect the batch script's standard error directly to the file name specified.<p>By default both standard output and standard error are directed to the same file. See <code>-o</code>, <code>--output</code> for the default file name. <code>#SBATCH   -e /home/username/myerrorfile</code> <code>-i</code>,<code>--input=&lt;filename pattern&gt;</code> Instruct Slurm to connect the batch script's standard input directly to the file name specified in the \"filename pattern\". <code>#SBATCH -i /mnt/home/username/myinputfile</code> <code>-o</code>,<code>--output=&lt;filename pattern&gt;</code> Instruct Slurm to connect the batch script's standard output directly to the file name specified in the <code>&lt;filename pattern&gt;</code>.<p>The default file name is <code>slurm-%j.out</code>, where the <code>%j</code> is replaced by the job ID. For job arrays, the default file name is <code>slurm-%A_%a.out</code>, <code>%A</code> is replaced by the job ID and <code>%a</code> with the array index. Needs a file name or filename pattern; not just a directory. <code>#SBATCH   -o /home/username/output-file</code> <code>--mail-type=&lt;type&gt;</code> Notify user by email when certain event types occur. Valid type values are NONE, BEGIN, END, FAIL, REQUEUE, ALL (equivalent to BEGIN, END, FAIL, REQUEUE, and STAGE_OUT), STAGE_OUT (burst buffer stage out and teardown completed), TIME_LIMIT, TIME_LIMIT_90 (reached 90 percent of time limit), TIME_LIMIT_80 (reached 80 percent of time limit), TIME_LIMIT_50 (reached 50 percent of time limit) and ARRAY_TASKS (send emails for each array task). <code>#SBATCH   --mail-type=BEGIN,END</code> <code>--mail-user=&lt;user&gt;</code> User to receive email notification of state changes as defined by <code>--mail-type</code>. The default value is the submitting user. <code>#SBATCH   --mail-user=user@msu.edu</code> <code>-v</code>,<code>--verbose</code> Increase the verbosity of sbatch's informational messages. Multiple <code>v</code> will further increase sbatch's verbosity. By default only errors will be displayed.","tags":["reference","slurm","job script"]},{"location":"List_of_Job_Specifications/#start-conditions","title":"Start Conditions","text":"Option Description Examples <code>--begin=&lt;time&gt;</code> Submit the batch script to the Slurm controller immediately, like normal, but tell the controller to defer the allocation of the job until the specified time. Time may be of the form HH:MM:SS to run a job at a specific time of day (seconds are optional). <code>#SBATCH --begin=16:00</code><code>#SBATCH --begin=now+1hour</code> (default unit is seconds) <code>-d</code>,<code>--dependency=&lt;dependency_list&gt;</code> Defer the start of this job until the specified dependencies have been satisfied completed. <code>&lt;dependency_list&gt;</code> can have many forms:<p>- <code>after:job_id[:jobid...]</code> This job can begin execution after the specified jobs have begun execution.- <code>afterany:job_id[:jobid...]</code> This job can begin execution after the specified jobs have terminated.- <code>afterburstbuffer:job_id[:jobid...]</code> This job can begin execution after the specified jobs have terminated and any associated burst buffer stage out operations have completed.- <code>aftercorr:job_id[:jobid...]</code> A task of this job array can begin execution after the corresponding task ID in the specified job has completed successfully (ran to completion with an exit code of zero).- <code>afternotok:job_id[:jobid...]</code> This job can begin execution after the specified jobs have terminated in some failed state (non-zero exit code, node failure, timed out, etc).- <code>afterok:job_id[:jobid...]</code> This job can begin execution after the specified jobs have successfully executed (ran to completion with an exit code of zero).- <code>expand:job_id</code> Resources allocated to this job should be used to expand the specified job. The job to expand must share the same QOS (Quality of Service) and partition. Gang scheduling of resources in the partition is also not supported.- <code>singleton</code> This job can begin execution after any previously launched jobs sharing the same job name and user have terminated. <code>#SBATCH   -d after:&lt;JobID1&gt;:&lt;JobID2&gt;,afterok:&lt;JobID3&gt;</code> <code>-H</code>,<code>--hold</code> Specify the job is to be submitted in a held state (priority of zero). A held job can now be released using scontrol to reset its priority (e.g. <code>scontrol release &lt;job_id&gt;</code>). <code>#SBATCH -H</code> <code>--no-requeue</code> Request that a job not be requeued under any circumstances. Jobs are requeued by default if a node they are running on fails. This options may be useful for jobs in the scavenger queue that will not run properly after having run partially and failing. <code>#SBATCH   --no-requeue</code>","tags":["reference","slurm","job script"]},{"location":"List_of_Job_Specifications/#overriding-the-job-script","title":"Overriding the Job Script","text":"<p>Optionally, any job specification (by #SBATCH line) can also be requested by <code>sbatch</code> command line with an equivalent option. For instance, the #SBATCH\u00a0 --nodes=1-5 line could be removed from the job script,  and instead be specified from the command line:</p> <pre><code>$ sbatch --nodes=1-5 myjob.sb\n</code></pre> <p>Command line specifications take precedence over those in the job script.</p>","tags":["reference","slurm","job script"]},{"location":"Load_the_software/","title":"Load the software","text":"<p>When running <code>module spider orthomcl</code> on a dev-node, you will see the following:</p> <pre><code>----------\nOrthoMCL:\n----------\n    Description:\n      OrthoMCL is a genome-scale algorithm for grouping orthologous protein\n      sequences.\n\n     Versions:\n        OrthoMCL/2.0.9-custom-Perl-5.24.0\n        OrthoMCL/2.0.9-Perl-5.24.0\n</code></pre> <p>They are the same version of OrthoMCL, except that the custom one has fixed a possible error due to sequence identifier. See the\u00a0issue\u00a0reported in GitHub.</p> <p>Since OrthoMCL uses BLAST, it needs to be loaded as well. An example of loading OrthoMCL would be</p> <pre><code>module purge\nmodule load icc/2016.3.210-GCC-5.4.0-2.26  impi/5.1.3.181\nmodule load OrthoMCL/2.0.9-custom-Perl-5.24.0\nmodule load BLAST/2.2.26-Linux_x86_64\n</code></pre> <p>The next thing you need to do is to request and set up your MySQL configuration file, see\u00a0https://docs.icer.msu.edu/MySQL_configuration/</p> <p>After you have your config file (say <code>orthomcl.config</code>) ready, you need to run <code>orthomclInstallSchema</code> to install the required schema into the database:</p> <pre><code>orthomclInstallSchema orthomcl.config install_schema.log\n</code></pre> <p>For the rest of your analysis steps, please read\u00a0http://orthomcl.org/common/downloads/software/v2.0/UserGuide.txt</p>"},{"location":"Local_File_Systems/","title":"Local File Systems","text":"<p>The local file systems are available on each cluster compute node and development node. These file systems are directly connected to each node and may be faster than the home, research, and scratch filesystems which must be accessed over the network.</p> <p>There are two kinds of local file systems: the local hard drive (accessible from either <code>/tmp</code> or <code>/mnt/local</code>) and RAMDISK space (accessible from <code>/dev/shm</code>). Read more about each type of storage below.</p> <p>Warning</p> <p>Please limit the use of the local spaces. It is also used for MPI by the MPI runtime to implement fast communication between MPI processes. Users are advised to clean up the space after use. Files that over 2 weeks old will be removed without notice. If the space is over 90% full, we may clean up unused files without notice.</p>","tags":["reference"]},{"location":"Local_File_Systems/#local-hard-drive","title":"Local Hard Drive","text":"<p>Each node has local hard drive storage accessible from either <code>/tmp</code> or <code>/mnt/local</code>. The files on this space can be accessed locally on each node without going through network.  This space is a good choice for jobs using a single node or multiple nodes where I/O is processed only on each node's local file. \u00a0When network traffic is high, using this space will likely allow your program to run faster than running on Home, Research or Scratch space. </p> <p>Please note that this local space is\u00a0shared with all processes running on the same node\u00a0and there is\u00a0no direct I/O from other nodes. The space also has\u00a0no auto backup. It should be used as temporary storage space. When the execution of programs in a job is completed, any useful files in this space should be saved back to Home or Research space.</p> <p>To ensure the continued, reliable operation of each node, all user accounts will be prevented from writing data to local storage that consumes more than 95% of the <code>/tmp</code> directory.  Once a user account's usage of the <code>/tmp</code> directory exceeds 95% or the total available space for that  directory, it will be locked and prevented from writing any additional data to the <code>/tmp</code> directory until files and data are removed bringing the user account back under 95% space utilization.</p> <p>Note</p> <p>In addition to the space restrictions on <code>/tmp</code>, local user account quotas are also in place for another shared directory, the  <code>/var</code> directory. This directory is rarely accessed directly by user accounts, and shouldn't be a concern for  most users of the HPCC; however, if you receive any errors that your user account has exceeeded the <code>/var</code> directory  quota, please contact us right away, and we'd be glad to help troubleshoot the error with you.</p>","tags":["reference"]},{"location":"Local_File_Systems/#tmpdir","title":"$TMPDIR","text":"<p>The environment variable <code>$TMPDIR</code> references a directory that is automatically created when your job starts on the local node. This directory is automatically deleted after the job finishes. It is accessible from both <code>/mnt/local/$SLURM_JOBID</code> and <code>/tmp/local/$SLURM_JOBID</code>. Since local hard drive storage is shared amongst all jobs on a node, using <code>$TMPDIR</code> is a convenient way to keep your files separated and organized.</p> <p>Any files saved to <code>$TMPDIR</code> should be transferred to a Home or Research space before the job ends.</p>","tags":["reference"]},{"location":"Local_File_Systems/#ramdisk","title":"RAMDISK","text":"<p>RAMDISK space is a \u201clogical\u201d storage space; it sits inside a node\u2019s RAM, not the hard drive. Linux supports a system tool that provides an interface for users to intercept the I/O requests to <code>/dev/shm</code> with memory operations. \u00a0We may think of it as a virtual disk in memory. </p> <p>Due to this nature, access to this space is actually access to RAM. Since the bandwidth of the access is much higher, \u00a0the I/O operations are considerably faster than the local hard drive space. However, since programs take up some of the node's memory, the usable RAM space for program execution becomes less. </p> <p>This space is good for programs that do not require large memory and perform very frequent I/O on small files.</p>","tags":["reference"]},{"location":"Loops/","title":"Loops","text":"<p>Loops allow you to execute the same code on different values of a variable Below, we give the syntax and examples for <code>for</code> and <code>while</code> loops.</p>","tags":["reference","command line"]},{"location":"Loops/#for-loops","title":"<code>for</code> loops","text":"<p>General syntax:</p> <pre><code>for var in list\ndo\n   Statement(s) to be executed using $var\ndone\n</code></pre> <p>Here, <code>var</code> is a variable name, and <code>list</code> is a list of items separated by white space that <code>var</code> will take on in the loop iterations.</p> <p>Example:</p> test.sh<pre><code>#!/bin/sh\nfor i in 1 2 3 4 5\ndo\n  echo \"Looping ... number $i\"\ndone\n</code></pre> <p>This is the result of a sample run.</p> <pre><code>$ sh test.sh\nLooping ... number 1\nLooping ... number 2\nLooping ... number 3\nLooping ... number 4\nLooping ... number 5\n</code></pre> <p>The looping expression can also use a format similar to a C for loop. For example, this script also gives a same results.</p> <pre><code>#!/bin/sh\nfor ((i=1 ; i&lt;6; i++))\ndo\n  echo \"Looping ... number $i\"\ndone\n</code></pre>","tags":["reference","command line"]},{"location":"Loops/#while-loops","title":"<code>while</code> loops","text":"<p>General syntax:</p> <pre><code>while [ expression ]\ndo\n   Statement(s) to be executed\ndone\n</code></pre> <p>Here, <code>expression</code> is a conditional expression as described in Conditional statements.</p> <p>Example:</p> test.sh<pre><code>#!/bin/sh\nINPUT_STRING=hello\nwhile [ \"$INPUT_STRING\" != \"bye\" ]\ndo\n  echo \"Please type something in (bye to quit)\"\n  read INPUT_STRING\n  echo \"You typed: $INPUT_STRING\"\ndone\n</code></pre> <p>This is the result of a sample run.</p> <pre><code>$sh test.sh\nPlease type something in (bye to quit)\nhello\nYou typed: hello\nPlease type something in (bye to quit)\nhi\nYou typed: hi\nPlease type something in (bye to quit)\nbye\nYou typed: bye\n$\n</code></pre>","tags":["reference","command line"]},{"location":"Managing_File_Permissions_on_HPCC/","title":"Managing file permissions on HPCC","text":"<p>This is a list of techniques to manage file permissions and groups on the HPCC. For background on the concepts, please see the page on file permissions.</p>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#displaying-permissions-of-files-and-directories-with-ls-l","title":"Displaying permissions of files and directories with <code>ls -l</code>","text":"<p>To display permissions in the current directory, run:</p> <pre><code>ls -l\n</code></pre> <p></p> <p>You can also display the permissions of an individual file or directory by running:</p> <pre><code>ls -ld filename\n</code></pre> <p>For example, you can check the permissions of your home directory:</p> <pre><code>ls -ld ~\n</code></pre>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#changing-file-permissions-with-chmod","title":"Changing file permissions with <code>chmod</code>","text":"<p>In the normal UNIX security model, there are three levels that are evaluated when considering file or directory access: user owner, group owner, and everyone else on the system. These types are typically referred to as user (<code>u</code>), group (<code>g</code>) and other (<code>o</code>). Only the owner of a file or a directory is allowed to change its permissions or the group name (to one of the owner's groups).</p>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#change-user-permissions","title":"Change user permissions","text":"<p>To add all permissions for the user owner, run the following command:</p> <pre><code>chmod u+rwx FileName\n</code></pre> <p>Note that any file you create will already have the <code>rw</code> permission for your user account so that you will have the \"read\" and \"write\" permissions respectively. However to have a program script able to be run from the command line, you need to change the 'execute', or <code>x</code>,  permission with</p> <pre><code>chmod u+x FileName\n</code></pre>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#changing-group-and-other-permissions","title":"Changing group and other permissions","text":"<p>To allow anyone in the group that owns the file to be able to read that file, change the group read permission:</p> <pre><code>chmod g+r FileName\n</code></pre> <p>To allow anyone in the group to read and write the file, you can change the read and write permission</p> <pre><code>chmod g+wr FileName\n</code></pre> <p>If you have a file that is currently read and writeable by the group (g+wr) and you want to make it private, remove those permissions:</p> <pre><code>chmod g-rw FileName\n</code></pre> <p>To add the ability for other users to write to a file or directory (this allows all users on the HPC to see and read this file if it's in a shared folder which we don't recommend).</p> <pre><code>chmod o+w FileName\n</code></pre>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#changing-group-ownership-with-chgrp","title":"Changing group ownership with <code>chgrp</code>","text":"<p>To change the group ownership of a file or a directory, simply run</p> <pre><code>chgrp &lt;GroupName&gt; &lt;FileName&gt;\n</code></pre> <p>where <code>&lt;GroupName&gt;</code> is the group name which you would like to change to and <code>&lt;FileName&gt;</code> is the name and path of the file or directory.</p>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#working-with-non-primary-groups-and-permissions","title":"Working with non-primary groups and permissions","text":"","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#switching-groups-with-newgrp","title":"Switching groups with <code>newgrp</code>","text":"<p>If you have more than one group associated with your account, you can switch which group owns the files created by default with the <code>newgrp</code> command: <code>newgrp myothergroup</code>. If you need to do this frequently, you can contact HPCC staff to change your primary group or see the page on changing your primary group. </p>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#changing-default-group-for-new-files-with-the-set-group-id-bit-and-chmod","title":"Changing default group for new files with the set-group-ID bit and <code>chmod</code>","text":"<p>You can also change the default group for new files created in a directory by setting the set-group-ID setting. The <code>/mnt/research</code> HPCC Research file share spaces have this setting set by default. To set the set-group-ID bit on a directory:</p> <pre><code>chmod g+s DirectoryName\n</code></pre> <p>To remove the set-group-ID bit on a directory:</p> <pre><code>chmod g-s DirectoryName\n</code></pre>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#other-special-permissions","title":"Other special permissions","text":"<p>There are other group permissions beyond the scope of this document, primarily the set-user-ID bit and the \"sticky\" bit. For more information about special permissions, please review the GNU documentation, available on any HPCC system:</p> <pre><code>info chmod\n</code></pre>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#filesystem-specific-differences","title":"Filesystem-specific differences","text":"","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#home","title":"Home","text":"<p>Your home directory has default permissions that allow only you to have access. Other users, whether they are in your primary group or not, are not allowed access to the contents of your home directory by default. If you wish to allow other users access to your home directory, you will need to change permissions on it.  </p> <p>To allow every member of a group access to read your home directory, use:</p> <pre><code>chmod g+rx ~\n</code></pre> <p>It is strongly recommended that you do not allow all users to read the contents of your home directory.  Instead, to allow users outside of your UNIX group to read contents within your home directory, it is suggested  that you create a subdirectory within your home directory that is readable by all users, and then set the execute permission on your home directory to allow users to access the subdirectory. For example:</p> <pre><code>#Create a subdirectory in your home directory called \"my_sub_directory\"\nmkdir ~/my_sub_directory\n#Make the subdirectory readable by everyone\nchmod a+rx ~/my_sub_directory\n#Set the execute permission on your home directory to allow others to access the new subdirectory\nchmod o+x ~/\n</code></pre> <p>To allow every user outside your UNIX group to read your home directory, use (NOT RECOMMENDED):</p> <pre><code>chmod o+rx ~\n</code></pre> <p>To allow world-wide read access to your home directory (NOT RECOMMENDED):</p> <pre><code>chmod a+rx ~\n</code></pre>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#sharing-a-single-directory-inside-your-home-directory","title":"Sharing a single directory inside your home directory","text":"<p>If you wish to share only a single directory in your home directory and keep all other contents private,\u00a0 you can use the following technique (This is the recommended method for sharing home directory contents with users outside of your UNIX group):</p> <pre><code># create the shared folder\ncd ~\nmkdir shared\nchmod o+rwx shared\n# create a shared file in the shared folder\necho \"hello, iCER\" &gt; shared/sharefile.txt\nchmod o+rw shared/sharefile.txt\n# anyone can read this file using\ncat /mnt/home/&lt;netid&gt;/shared/sharefile.txt\n</code></pre>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#scratch","title":"Scratch","text":"<p>Directories are created as private to you by default. If you do not wish this to be the case, you can use the technique for sharing a single directory in your home directory above. Note if there are other directories above your shared directory (e.g. it's a sub-sub-directory like <code>~/project/data/shared</code>), then every directory in the path will need the execute bit set for everyone.</p>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#tmpdir-space","title":"TMPDIR space","text":"<p>Directories are created as world-readable by default, but the scheduler deletes the contents of <code>$TMPDIR</code> after a job exits. If you require additional security for this temporary space, manually setting the permissions of <code>$TMPDIR</code> is necessary. Here is an example to mimic the security of home directory space:</p> <pre><code>chmod go-rwx $TMPDIR\n</code></pre>","tags":["how-to guide","files","groups"]},{"location":"Mapping_HPC_drives_with_SSHFS/","title":"Mapping HPC drives with SSHFS","text":"<p>Besides mapping HPCC drives with SMB,  SSHFS can also enable mounting of HPCC file systems on a local computer. Different from SMB mapping, which can only map to home or research space via the MSU campus network, this method can also work on your scratch space and uses any internet network.</p> <p>Warning</p> <p>You will need to generate an authentication key by following the  direction of SSH Key-Based Authentication if you do not already have one set up. </p> Mac OSXLinuxWindows <ol> <li> <p>Download and install (or upgrade) the most recent versions of the following packages: FUSE for macOS and SSHFS from https://osxfuse.github.io.</p> </li> <li> <p>Reboot (required)</p> </li> <li> <p>Using the Terminal, create a directory (as <code>&lt;local_mount_point&gt;</code> in step 4) for each filesystem you wish to mount. \u00a0If you are creating the folder outside of your home directory, you may need to use <code>sudo</code> before each command (sudo = superuser do ).\u00a0</p> <pre><code>[MacBook-Pro:~ icer2]$ mkdir &lt;local_mount_point&gt;\n/* begin example home directory */\n[MacBook-Pro:~ icer2]$ mkdir /Users/icer2/hpcc_home\n/* end example home directory */\n/* begin example scratch directory in the Mac /Volumes folder where drives are mounted */\n[MacBook-Pro:~ icer2]$ mkdir /Volumes/scratch\n/* end example scratch directory */\n</code></pre> </li> <li> <p>Mount the directory using  the <code>sshfs</code> command. We suggest you add     these additional flags to the command to make it be more \"Mac-like\"     :\u00a0<code>-ocache=no</code> , \u00a0<code>-onolocalcaches</code> \u00a0and <code>-o volname=hpcc_home</code> .\u00a0     For the last option, '<code>-o volname</code>' is the name that displays     in the Finder title bar, so change it for difference file     folders (e.g. use\u00a0<code>-o volname=hpcc_scratch</code> for your scratch     folder). After running the command, enter the password for logging     into HPCC and the FUSE drive icon will show on the desktop of     your local Mac computer.</p> <pre><code>[MacBook-Pro:~ icer2]$ sshfs &lt;user_id&gt;@rsync.hpcc.msu.edu:&lt;remote_directory_to_mount&gt; &lt;local_mount_point&gt; -ovolname=hpcc -o allow_other,defer_permissions,follow_symlinks,reconnect -ocache=no -onolocalcaches\n/* begin example hpc's home directory, using /mnt/home/hpc/ */\n[MacBook-Pro:~ icer2]$ sshfs hpc@rsync.hpcc.msu.edu:/mnt/home/hpc/ /Users/icer2/hpcc_home -o volname=hpcc_home -o allow_other,defer_permissions,follow_symlinks,reconnect -ocache=no -onolocalcaches\n/* end example home directory */\n/* begin example hpc's scratch directory with authorized key file ~/.ssh/id_rsa, using /mnt/gs18/scratch/users/hpc */\n[MacBook-Pro:~ icer2]$ mkdir /Volumes/scratch\n[MacBook-Pro:~ icer2]$ sshfs hpc@rsync.hpcc.msu.edu:/mnt/gs18/scratch/users/hpc /Volumes/scratch -o volname=hpcc_scratch -o allow_other,defer_permissions,follow_symlinks,reconnect,IdentityFile=~/.ssh/id_rsa -ocache=no -onolocalcaches\n(No password input)\n/* end example scratch directory */\n</code></pre> <p>If <code>&lt;remote_directory_to_mount&gt;</code> is a static link, please make sure</p> </li> </ol> <p>to put <code>/</code> at the end of the directory path:</p> <pre><code>-   For your home space, please use `/mnt/home/&lt;user_id&gt;/` instead\u00a0 of\n`/mnt/home/&lt;user_id&gt;`.\n-   For your research space, please use `/mnt/research/&lt;group_name&gt;/`\ninstead of `/mnt/research/&lt;group_name&gt;`.\n\nAs the above example (starting from line 5), home space\n`/mnt/home/hpc/` is used on line 7 instead of `/mnt/home/hpc`.\n</code></pre> <ol> <li> <p>To unmount a filesystem, use the <code>umount</code> command. </p> <p>Note</p> <p>It's just the letter u before mount, NOT <code>unmount</code>.</p> <pre><code>    umount &lt;local_mount_point&gt;\n    /* begin example */\n    umount /Users/icer2/hpcc_home\n    /* end example */\n</code></pre> <p>You'll see these folders in the finder if you use the Go menu, but you</p> </li> </ol> <p>won't see them listed in the left side with the other mounted drives. \u00a0You must use the terminal and <code>umount</code> command to disconnect.</p> <p>References:</p> <p>https://www.digitalocean.com/community/tutorials/how-to-use-sshfs-to-mount-remote-file-systems-over-ssh</p> <p>Please refer to this web site:</p> <p>https://tecadmin.net/install-sshfs-on-linux-and-mount-remote-filesystem/</p> <p>for how to mount remote filesystem over SSH on Linux.</p> <p>We no longer recommend using SSHFS on Windows, please see instructions for using Samba instead </p>","tags":["tutorial","sshfs","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/","title":"Mapping HPC drives with Samba","text":"<p>Warning</p> <ol> <li> <p>Samba now uses campus AD for user authentication, if you are unable to login and have not updated your NetID password recently please try updating your NetID password before opening a ticket</p> </li> <li> <p>This will only work if your computer has a university IP address. If you are off campus, you can use the MSU VPN  to obtain an MSU IP, which is available to all graduate students, staff and faculty.\u00a0</p> </li> <li> <p>If file transfer speed is a concern please use our Globus endpoint.</p> </li> </ol> <p>The following tutorial will show you how to map your HPC home or research directory using SMB or CIFS File Sharing.</p>","tags":["tutorial","samba","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/#determining-your-network-path","title":"Determining your Network Path","text":"<p>We have the powertools command <code>show-samba-paths</code> to show all paths of your home and research space. Run these commands in your SSH client after logging on to the HPCC:</p> <pre><code>$ ml powertools                    # if powertools is not loaded\n$ show-samba-paths\n\n  HOME      |        Samba Path\n===================================================================\n  username  |  \\\\ufs.hpcc.msu.edu\\home-021\\username      (Windows)\n            |  smb://ufs.hpcc.msu.edu/home-021/username      (Mac)\n\n\n  RESEARCH        |        Samba Path\n===============================================================================\n  helpdesk        |  \\\\ufs.hpcc.msu.edu\\rs-011\\helpdesk              (Windows)\n                  |  smb://ufs.hpcc.msu.edu/rs-011/helpdesk              (Mac)\n------------------+------------------------------------------------------------\n  common-data     |  \\\\ufs-12-b.hpcc.msu.edu\\common-data             (Windows)\n                  |  smb://ufs-12-b.hpcc.msu.edu/common-data             (Mac)\n------------------+------------------------------------------------------------\n  BiCEP           |  \\\\ufs.hpcc.msu.edu\\rs-001\\BiCEP                 (Windows)\n                  |  smb://ufs.hpcc.msu.edu/rs-001/BiCEP                 (Mac)\n------------------+------------------------------------------------------------\n  education-data  |  \\\\ufs-12-b.hpcc.msu.edu\\education-data          (Windows)\n                  |  smb://ufs-12-b.hpcc.msu.edu/education-data          (Mac)\n</code></pre> <p>where the paths are the same for\u00a0Mac and Window computers but with different formats.</p>","tags":["tutorial","samba","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/#windows-10","title":"Windows 10","text":"","tags":["tutorial","samba","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/#step-1-enable-netbios-over-tcpip-on-windows","title":"Step 1. Enable NetBIOS over TCP/IP on Windows:","text":"<ul> <li>Click on the Network icon on the taskbar at the right hand side and     click on \"Network &amp; Internet settings\" </li> <li>Click on Change adapter options     </li> <li>Right click on your Network interface that is used to connect to the internet      (here, Ethernet 2) and click on Properties     </li> <li>Select the Internet Protocol version 4 (TCP/IPv4) </li> <li>Click the Advanced button under the General tab.     </li> <li>Click the WINS tab.  </li> <li>Click the Enable NetBIOS Over TCP/IP button.     </li> <li>Click Ok</li> </ul>","tags":["tutorial","samba","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/#step-2-disable-smb1","title":"Step 2. Disable SMB1","text":"","tags":["tutorial","samba","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/#disable-samba-v1-protocol-with-powershell","title":"Disable Samba V1 protocol with PowerShell","text":"This step must be completed or your client will not be able to map the drive. If you have other mounts on the HPC cluster and they are using samba V1 they will stop working.   <ol> <li> <p>Press the Windows start button</p> </li> <li> <p>In the search box type \"power shell\" </p> </li> <li> <p>Right click on the \"Windows PowerShell\" icon and select \"Run as     Administrator\" </p> </li> <li> <p>Select Ok when security warning appears </p> </li> <li> <p>Disable Samba V1 by entering the following command into the windows     power shell. </p> <pre><code>Set-ItemProperty -Path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\LanmanServer\\Parameters\" SMB1 -Type DWORD -Value 0 -Force\n</code></pre> </li> <li> <p>Ensure SMB V2 and SMB V3 are enabled by entering the following     command.\u00a0 In the past, on some versions of windows and for some file     systems we recommended the opposite of this setting.\u00a0 Running this     ensures it's enabled again.\u00a0</p> <pre><code>Set-ItemProperty -Path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\LanmanServer\\Parameters\" SMB2 -Type DWORD -Value 1 -Force\n</code></pre> </li> <li> <p>Navigate to \"This PC\" and click on the text labeled \"Map Network     Drive\" at the top of the screen, located under the \"Computer\" tab.</p> <p></p> <p>From this menu you need to type your Network Path. Please see\u00a0#Determining your Network Path\u00a0for help</p> </li> <li> <p>Once you have typed in your Network Path you need to click on     the box \"Connect using different credentials.\" This will open a     window where you type in your MSU NetID and     password:     </p> </li> </ol> <p>Warning</p> <p>If you aren't able to sign in,  You will need to add \"CAMPUSAD\\\" to the beginning of your username. An indicator of this issue is if Windows displays the error \"The specified network password is not correct\" in the username dialog window.</p> <p>For example: substitute \"CAMPUSAD\\sparty\" for username \"sparty\" in the username field. The slash character is a backslash. A forward slash character will not work.</p> <ol> <li> <p>Finally, select \"Finish\" and you will see your system trying to     connect</p> <p></p> </li> </ol>","tags":["tutorial","samba","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/#command-line-windows-netbios-commands","title":"Command-line Windows NetBIOS Commands","text":"<p>If you're working in Windows, you can use command line tools to manage your drive mapping.\u00a0 These commands also work in .bat files, if you're so inclined to connect/disconnect drives in that manner.\u00a0 Note you may also have to Disable SMBV1 and Enable SMB2 per instructions above.\u00a0</p> <ol> <li>From the Start Menu -&gt; Run -&gt; type 'cmd' in the box and hit enter,     the command shell should open.\u00a0 You can then use the following     commands to diagnose, disconnect and connect drives.</li> </ol> <pre><code># to show all mapped connections/drives\n# use 'net use'\n\nC:\\Documents and Settings\\Administrator&gt;net use\nNew connections will not be remembered.\n\n\nStatus       Local     Remote                    Network\n\n-------------------------------------------------------------------------------\nUnavailable  J:        \\\\MachineA\\consult$    Microsoft Windows Network\nUnavailable  P:        \\\\MachineB\\everyone$    Microsoft Windows Network\n             Z:        \\\\vmware-host\\Shared Folders\n                                                 VMware Shared Folders\nThe command completed successfully.\n\n# to map a drive use:\n# net use &lt;drive_letter&gt;: \\\\&lt;hostname&gt;\\&lt;mount&gt; /user:hpcc\\&lt;net_id&gt;\n# where &lt;hostname&gt; and &lt;mount&gt; were determined from above 'Determining Your Network Path' and &lt;net_id&gt; is your MSU Net ID\n\nC:\\Documents and Settings\\Administrator&gt;net use m: \\\\ufs-10-a.hpcc.msu.edu\\jal /user:hpcc\\jal\nThe command completed successfully.\n\n# You can use 'net use' again to show the drive mapping status.\n\nC:\\Documents and Settings\\Administrator&gt;net use\nNew connections will not be remembered.\n\n\nStatus       Local     Remote                    Network\n\n-------------------------------------------------------------------------------\nUnavailable  J:        \\\\MachineA\\consult$    Microsoft Windows Network\nOK           M:        \\\\ufs-10-a.hpcc.msu.edu\\jal\n                                                 Microsoft Windows Network\nUnavailable  P:        \\\\MachineB\\everyone$    Microsoft Windows Network\n             Z:        \\\\vmware-host\\Shared Folders\n                                                 VMware Shared Folders\nThe command completed successfully.\n\n# To disconnect a drive use:\n# net use &lt;drive_letter&gt;: /delete\n## NOTE: this will only drop the connection, not delete the share or any data on the share ##\n\nC:\\Documents and Settings\\Administrator&gt;net use m: /delete\nm: was deleted successfully.\n\n# you can now use 'net use' again to show your mapping status after the delete to ensure it's gone.\n\nC:\\Documents and Settings\\Administrator&gt;net use\nNew connections will not be remembered.\n\n\nStatus       Local     Remote                    Network\n\n-------------------------------------------------------------------------------\nUnavailable  J:        \\\\MachineA\\consult$    Microsoft Windows Network\nUnavailable  P:        \\\\MachineB\\everyone$    Microsoft Windows Network\n             Z:        \\\\vmware-host\\Shared Folders\n                                                 VMware Shared Folders\nThe command completed successfully.\n\n\nC:\\Documents and Settings\\Administrator&gt;\n</code></pre>","tags":["tutorial","samba","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/#macos-example","title":"MacOS Example","text":"<ol> <li>Open the Finder. </li> <li>Under \"GO\" click on \"Connect to Server\" </li> <li>From this menu you need to type your Network Path. Please     see\u00a0#Determining your Network     Path\u00a0for     help. </li> <li>Enter your MSU NetID and password for authentication and click     \"Connect\". </li> </ol>","tags":["tutorial","samba","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/#linux","title":"Linux","text":"<ol> <li> <p>Install smb-client</p> <p>Ubuntu / Debian</p> <pre><code>apt install smbclient\n</code></pre> <p>Red Hat / Fedora</p> <pre><code>yum install samba-client\n</code></pre> </li> <li> <p>Edit /etc/samba/smb.conf</p> <pre><code>sudo vi  /etc/samba/smb.conf\n</code></pre> </li> <li> <p>Add the following lines to disable samba V1</p> <p>This step must be completed or your client will not be able to map the drive. If you have other mounts on on the HPC cluster and they are using samba V1 they will stop working. In this case please use SSHFS.</p> <pre><code>client min protocol = SMB2\nclient max protocol = SMB3\n</code></pre> </li> </ol>","tags":["tutorial","samba","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/#ubuntu-mount-example","title":"Ubuntu Mount Example","text":"<ol> <li>Open a File Browser window. In the left pane select \"Other Locations\" </li> <li> <p>Type your network path in the server address box. Remember to get     your network path using\u00a0<code>show-samba-paths</code> on the HPCC      (Format is the same as the Mac format)  </p> <p> 3.  An authentication window will open. Select \"Registered User\". Enter your user ID and password (Domain can remain WORKGROUP)  and click connect. </p> <p>Note</p> <p>Your username must include @msu.edu.</p> <p> 4.  If connected properly the drive should appear in the file manager screen.  </p> <p></p> </li> <li> <p>After successfully mounting, you can unmount using the eject button in the file manager, and reconnect later using the dropdown list next to the Connect to Server box.</p> </li> </ol>","tags":["tutorial","samba","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/#more-information","title":"More Information","text":"<ul> <li>http://us3.samba.org/samba/</li> </ul>","tags":["tutorial","samba","drive mapping"]},{"location":"Matlab/","title":"MATLAB","text":"","tags":["tutorial","MATLAB"]},{"location":"Matlab/#about-matlab","title":"About MATLAB","text":"<ul> <li> <p>There is a Matlab portal for Michigan State University users. Users are encouraged to visit it for information and support provided by Mathworks.</p> </li> <li> <p>Several versions of\u00a0MATLAB are installed on the cluster. By default, MATLAB/2018a is loaded. Other available version of MATLAB can be discovered by typing</p> </li> </ul> <pre><code>hpc@dev-intel18:~&gt; module spider MATLAB\n</code></pre> <p>and then switching to a different version, for example, to switch from 2018a to 2019a, one can directly load the the version as</p> <pre><code>hpc@dev-intel18:~&gt; module load MATLAB/2019a \n</code></pre> <ul> <li>MATLAB's many\u00a0built-in functions\u00a0have multi-threaded capability. On     your personal computer, when a MATLAB\u00a0function with multi-threads is     called,\u00a0MATLAB\u00a0will automatically spawn as many threads as the     number of cores on the machine. To avoid over utilizing compute     nodes on HPCC, user should set the max number of threads by using     <code>maxNumCompThreads(N)</code> in matlab where N is the maximum number of     threads matlab would use in the session. User could also use option     <code>-SingleCompThread</code> to launch\u00a0matlab session that would only use a     single thread. Without these option, matlab\u00a0session will potentially     spawn as many as 28 (on intel16 nodes) or 20 (on intel14 nodes)     threads when a built-in multi-threaded function is called. To allow     the multi-thread functions in matlab, users need to do the     following:<ol> <li>Specify the maximum number of compute threads to be used with     \u00a0<code>maxNumCompThreads(N)</code>\u00a0at the beginning of the matlab     program where N is the maximum number of threads in the program.     For example,\u00a0<code>maxNumCompThreads(4)</code>\u00a0will set the maximum     number of threads used in the program to four. \u00a0</li> <li>If submitting to run as batch job, specify <code>--cpus-per-task=N</code>     in your job script where N should match the maximum number of     compute threads set in <code>maxNumCompThreads(N)</code>.</li> </ol> </li> </ul> <p>Warning</p> <p>Starting Nov. 1, 2018 on new HPCC system, the matlab default setting of using a single compute thread is changed. <code>matlab-mt</code> and <code>matlab</code> commands would be the same for 2018 and older versions and <code>matlab-mt</code> will no longer exist from later versions starting 2019.</p> <ul> <li>HPCC has many toolboxes installed. To see a list of installed     toolboxes and licenses, type</li> </ul> <pre><code>hpc@dev-intel18:~&gt; module load powertools\nhpc@dev-intel18:~&gt; licensecheck matlab\nChecking Licenses for matlab\nlmstat -a -c 27000@lm-02.i\n\nUsers of MATLAB:  (Total of 10000 licenses issued;  Total of 44 licenses in use)\nUsers of SIMULINK:  (Total of 10000 licenses issued;  Total of 1 license in use)\nUsers of Bioinformatics_Toolbox:  (Total of 10000 licenses issued;  Total of 0 licenses in use)\nUsers of Communication_Toolbox:  (Total of 10000 licenses issued;  Total of 0 licenses in use)\n\n......\n</code></pre>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#running-matlab-on-scratch-space","title":"Running MATLAB on Scratch Space","text":"<p>It is strongly recommended that iCER users run programs on the scratch space. This may improve the speed of job execution as well as the whole system's performance. MATLAB users should carefully check whether any temporary files involved in the program execution need to be stored in scratch space. For example, if you use the Matlab compiler to make a Matlab program into a standalone program, you need to set the environment variable <code>MCR_CACHE_ROOT</code> in the scratch directory with: <code>export MCR_CACHE_ROOT=$SCRATCH</code> before starting the execution. This line should be added to the job script before the line where you specify the task you want to run. This setting will override the default setting by MATLAB Compiler Runtime. By default, a directory for temporary data cache used by the MATLAB Compiler Runtime is created at user's home directory <code>$HOME</code>. Without this setting, users may run into the situation that the program running on scratch space\u00a0frequently accesses its cache space in the home space, which will greatly slow down the execution of the code, and may potentially slow down the whole system or even cause system instability. \u00a0</p>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#running-matlab-interactively","title":"Running MATLAB Interactively","text":"<p>In this document, we refer to an interactive session as one that involves a user typing commands into the MATLAB command windows.</p> <p>The simplest way to run MATLAB as a graphical application is to use an OnDemand session. However, if you only wish to use the command line or use the graphics through <code>ssh</code>, see the instructions below.</p>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#short-sessions-two-hours","title":"Short Sessions (&lt; two hours)","text":"<ul> <li> <p><code>ssh</code> to one of the dev nodes and run Matlab:</p> <pre><code>hpc@gateway:~&gt; ssh dev-intel16\nLast login: Mon Dec  4 12:54:44 2017 from gateway\n===\nThis front-end node is not meant for running big or long-running jobs.  Jobs\nthat need to run longer than a few minutes should be submitted to the queue.\nLong-running jobs on front-end nodes will be killed without warning.\n===\nhpc@dev-intel16:~&gt; matlab -nodisplay\n</code></pre> <p>More information about running jobs interactively on compute nodes can be reviewed at\u00a0Running Programs Interactively</p> </li> <li> <p>If you require graphics, please ensure that you have an Xserver     running. For Linux and MAC users,</p> <pre><code>ssh -X username@hpcc.msu.edu\n</code></pre> <p>If you want graphics, but don't want the desktop, type</p> <pre><code>hpc@dev-intel16:~&gt; matlab -nodesktop\n</code></pre> <p>This will let you run code from the command line instead of from the IDE interface, but will still allow you to use graphics (e.g., make plots).</p> </li> </ul> <p>Interactive MATLAB jobs running on development nodes are limited to a two hour wall time limit, and will be killed automatically after two hours.</p>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#long-sessions-two-hours","title":"Long Sessions (&gt; two hours)","text":"<p>Longer interactive sessions are possible, but are not recommended. Modify the following commands to suit your requirements.</p> <p>If graphics are not required,</p> <pre><code>hpc@dev-intel18:-&gt; salloc --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=2gb --time=04:00:00 \n\n\nsalloc: Granted job allocation 310982\n\nsalloc: Waiting for resource configuration\n\nsalloc: Nodes lac-376 are ready for job\n\nhpc@dev-intel18:-&gt; matlab -nodisplay \n</code></pre> <p>If graphics are required, add the option <code>\u2013x11</code> to the <code>salloc</code> command:</p> <pre><code>hpc@dev-intel18:-&gt; salloc --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=2gb --time=04:00:00 --x11\n</code></pre> <p>Warning</p> <p>The above commands submit a job to the cluster. If the resources are not immediately available, you will have to wait till the requested resources are available. Requesting a job for four hours or less will typically be scheduled relatively quickly. Users may need to adjust the resources accordingly with the usage of the MATLAB program.</p>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#running-matlab-non-interactively","title":"Running MATLAB Non-Interactively","text":"","tags":["tutorial","MATLAB"]},{"location":"Matlab/#short-jobs-two-hours","title":"Short Jobs (&lt; two hours)","text":"<p>A short job could be run on a development node without opening the matlab command window. From a development node, type</p> <pre><code>hpc@dev-intel16:~&gt; matlab -nodisplay -r \"myMatlab\" &amp;\n</code></pre> <p><code>myMatlab.m</code> will start running in the background.</p>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#long-jobs-two-hours","title":"Long Jobs (&gt; two hours)","text":"","tags":["tutorial","MATLAB"]},{"location":"Matlab/#single-matlab-job","title":"Single MATLAB Job","text":"<p>To submit jobs to the cluster, a job script needs to be written. And submitted to the queue. The following sample job script file can be modified to suit your needs:</p> <p><code>myJob.sbatch</code></p> <pre><code>#!/bin/bash\n\n### Specify the Resources Needed\n\n#SBATCH --time=02:05:00 \n#SBATCH --nodes=1 \n#SBATCH --ntasks=1 \n#SBATCH --cpus-per-task=1 \n#SBATCH --mem-per-cpu=1G        \n\n### Set SLURM Admin parameters\n\n#SBATCH --job-name=myJobName\n#SBATCH --output=%x-%j.SLURMout\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=myNetID@msu.edu\n\n### Navigate to your data directory and run Matlab\n\ncd $HOME/my                           \nmatlab -nodisplay -r \"myMatlab\"\n</code></pre> <ul> <li><code>--time=02:05:00</code> is the number of <code>hours:minutes:seconds</code> your job needs to     run. If it runs longer than this, it will be killed. If you request     more time than you need, your job may be delayed while the scheduler     finds a time to run it. If you don't know how long your job needs,     you will have to make a guess and use the real running time to     improve this number on future runs. The maximum walltime that can be     requested is <code>168:00:00</code>.</li> <li><code>--nodes=1</code> because you are running one matlab client in the job. If     you want put multiple matlab run in a single job script, you may     request more nodes for the job.</li> <li><code>--cpus-per-task=1</code> here because the matlab script <code>MyMatlab.m</code> does     not use multiple threads. If you are using multiple threads, you may     need to request more cpus.</li> <li><code>--mem-per-cpu=1G</code> reserves 1 gigabytes per CPU of memory for the job. We     recommend user to serve at least 1GB for each session plus the     total size of data variables used in the computation. User could     click here to see the recommendation of the matlab     requirement by Mathworks.</li> <li><code>myJobName</code> is a string to make your job easier to identify it when     managing or monitoring your jobs.</li> <li><code>--output=%x-%j.SLURMout</code> sets the SLURM output filename to <code>myJobName-IDnumber.SLURMout</code></li> <li><code>myMatlab</code> is the name of your matlab script without the .m extension</li> <li><code>--mail-type=ALL</code> tells SLURM to email you with <code>ALL</code> job scheduling events e.g., start, complete, exit-code</li> <li><code>--mail-user=myNetID@msu.edu</code> sets the email address SLURM uses; change <code>myNetID@msu.edu</code> to your prefered email address</li> </ul> <p>Submit your job with:</p> <pre><code>hpc@dev-intel16:~&gt; sbatch myJob.sbatch\n</code></pre> <p><code>myJob.sbatch</code> is the name of your job script as shown above.</p>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#using-the-matlab-parallel-computing-toolbox","title":"Using the MATLAB Parallel Computing Toolbox","text":"<p>The MATLAB Parallel Computing Toolbox (PCT) provides users several parallel computing features.\u00a0</p> <ul> <li> <p>Parallel for-loops (parfor) . \u00a0(User could run <code>module load     powertools; getexample MATLAB_parfor</code> to download a directory     \"MATLAB_parfor\" which contains an example of using parpool and     parfor with the \"local\" profile)</p> </li> <li> <p>Support for GPU computing</p> </li> <li> <p>Offload computing from your laptop to HPCC cluster (with MATLAB     Distributed Computing Server)\u00a0</p> </li> <li> <p>Distributed arrays and <code>spmd</code> (single-program-multiple-data) for large     dataset handling and data-parallel algorithms</p> <ol> <li>One GPU card will be used for each worker. In order to use multiple GPUs, user need to use <code>spmd</code> capability that each instance of the program will use one card and multiple instances of the program take multiple cards.</li> <li>If you use GPU capability, you need to have matlab run on a node with GPU. dev-intel16-k80 and dev-amd20-v100 are the development nodes with GPUs. To request GPUs, use <code>\u2013-gpus=&lt;type&gt;:&lt;number&gt;</code> to request a number and type of GPU. <code>#SBATCH ---gpus=k80:1</code> is an example to request one k20 GPU. Valid GPU type are k80 and v100. Note that type is optional, but the number of GPU is necessary.</li> </ol> </li> </ul>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#using-the-matlab-parallel-server","title":"Using the MATLAB Parallel Server","text":"<p>The MATLAB Parallel Server lets users solve computationally and data-intensive problems by executing MATLAB and Simulink based applications on the HPCC cluster and clouds. (see the document for more information). HPCC cluster has this product installed.</p> <p>We recommend that users prototype their applications using the Parallel Computing Toolbox, and then scale up to a cluster using MATLAB Parallel Server. To scale up to cluster, user does not need to recode the program. User only need to change the profile of the cluster.\u00a0</p>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#setup-and-validate-your-cluster-profile","title":"Setup and validate your cluster profile","text":"<p>In this step you define a cluster profile to be used in subsequent steps.</p> <ol> <li> <p>Start the Cluster Profile Manager from the MATLAB desktop by     selecting on the\u00a0Home\u00a0tab in the\u00a0Environment\u00a0area\u00a0Parallel\u00a0&gt; Create     and\u00a0Manage Clusters.\u00a0</p> </li> <li> <p>Create a new profile in the Cluster Profile Manager by selecting Add     Cluster Profile\u00a0&gt;\u00a0Slurm.</p> </li> <li> <p>With the new profile selected in the list, click\u00a0Rename\u00a0to edit the     profile name,\u00a0Press\u00a0Enter.</p> </li> <li> <p>Select a profile in the list, click Edit to edit the profile     accordingly. After finishing editing, click Done to save the     profile.</p> </li> <li> <p>Click validation to validate the profile. The profile could be used     when it pass all the validation tests.</p> </li> </ol> <p>Starting from version 2018a, MATLAB supports the SLURM scheduler. Please refer to Mathworks' document for how to configure the cluster with the SLURM scheduler here. Previous versions of MATLAB are not recommended for use with the HPCC scheduler.</p>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#using-the-matlab-thread-based-worker-pool","title":"Using the MATLAB thread-based worker pool","text":"<p>Started from Matlab version R2020a, thread-based worker pool is introduced. Please refer to this document\u00a0for more details. On HPCC, we provide our users an example showing how to use thread-base pool with <code>parfor</code>, as well as the comparison between process-based and thread-based pools. To obtain the example, module load <code>powertools</code> and <code>MATLAB/2020a</code>, then run <code>getexample MATLAB_threadPool</code>.\u00a0</p>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#running-the-matlab2020a-on-amd-nodes","title":"Running the MATLAB/2020a on AMD nodes","text":"<p>There is a bug in MATLAB/2020a that will lead to a \"segmentation fault\" on AMD node associated with the Java virtual machine. The patch may be introduced in the next release. If you find that the code works on other version but crashes in 2020a version, you may try the workaround that launch the matlab session without java virtual machine as the following:</p> <pre><code>[hpc@eval-epyc19 ~]$ matlab -nodisplay -nojvm -r \"myExample\"\n</code></pre>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#running-matlab-on-intel14-nodes","title":"Running MATLAB on intel14 nodes","text":"<p>There is an existing problem of running some functions on Intel14 nodes.\u00a0If you run into 'Illegal instruction detected' error, please use a node of other type.\u00a0Please add constrain to exclude intel14 type of compute nodes when submit to SLURM. For example:</p> <pre><code>#SBATCH --constraint=[intel16|intel18|amd20]\n</code></pre>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#using-matlab-with-python","title":"Using MATLAB with Python","text":"<p>Here is the link to the cheat sheets provided by the Mathworks for users' reference.</p>","tags":["tutorial","MATLAB"]},{"location":"Mothur/","title":"Mothur","text":"","tags":["tutorial","Mothur","bioinformatics"]},{"location":"Mothur/#loading-mothur","title":"Loading Mothur","text":"<p>Mothur version 1.40.3 on the HPCC has two running modes, with and without MPI functionality. You can load either one by:</p> <pre><code>module purge\nmodule load icc/2017.1.132-GCC-6.3.0-2.27 impi/2017.1.132 Mothur/1.40.3-nonMPI-Python-2.7.13        # Mothur non-MPI version\nmodule load icc/2017.1.132-GCC-6.3.0-2.27 impi/2017.1.132 Mothur/1.40.3-Python-2.7.13               # Mothur MPI version\n</code></pre> <ul> <li> <p>As of Feb 7, 2019, the highest version is\u00a0Mothur/1.41.3 (in MPI mode only).</p> </li> <li> <p>As reported by some Mothur users, when using Mothur and vsearch together, the only compatible version of vsearch is 1.8. So after loading Mothur, you would add a line of \"<code>module load vsearch/1.8.0</code>\".</p> </li> </ul>","tags":["tutorial","Mothur","bioinformatics"]},{"location":"Mothur/#running-mothur","title":"Running Mothur","text":"<p>Take a look at this example code <code>batch.m</code>:</p> <p><code>/mnt/research/common-data/Examples/mothur/batch.m</code>:</p> <pre><code>set.current(fasta=ex.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.subsample.fasta, count=ex.trim.contigs.good.unique.good.filter.unique.precluster.uchime.pick.pick.subsample.count_table,  processors=1)\ndist.seqs(fasta=current, cutoff=0.2, processors=8)\n</code></pre> <p>where we specified <code>processors=8</code> in line 2.\u00a0To be able to actually utilize 8 processors, you need to launch Mothur using either of the following commands, depending on whether MPI is enabled or not.</p> <pre><code>MPI: mpirun -np 8 mothur batch.m\nnon-MPI: mothur batch.m\n</code></pre>","tags":["tutorial","Mothur","bioinformatics"]},{"location":"Mothur/#differences-between-mpi-and-non-mpi-runs","title":"Differences between MPI and non-MPI runs","text":"<p>MPI jobs can run across multiple nodes at the cost of overhead. This can lead to increased memory usage and decreased performance. The additional processor advantages offered by MPI may be cancelled out by I/O waits to disk. If you request many more processes than can be provided by a single node, use MPI mode. If you choose the MPI type, specify number of processes in the SLURM script by <code>--ntask=8</code> for the example above. SLURM will determine how many nodes and tasks per node are needed. Also,\u00a0memory request in this case should be made on a per CPU basis (by defining <code>--mem-per-cpu</code>).</p> <p>Non-MPI jobs run on a single node with multiple threads/processes. For above Mothur command, you should set up something like</p> <p><code>#SBATCH --nodes=1</code></p> <p><code>#SBATCH --ntasks-per-node=1</code></p> <p><code>#SBATCH --cpus-per-task=8</code></p> <p>in your job submission script. If most of the nodes in the cluster are highly occupied, the job scheduler may have a hard time finding the nodes with availability of your desired number of threads.</p>","tags":["tutorial","Mothur","bioinformatics"]},{"location":"MySQL_configuration/","title":"MySQL configuration for OrthoMCL","text":""},{"location":"MySQL_configuration/#overview","title":"Overview","text":"<p>OrthoMCL is a program that aids in the identification of orthologs. \u00a0The OrthoMCL tool uses NCBI BLAST and the MCL application in conjunction with a relational database (MySQL). \u00a0OrthoMCL version 2.0.2 is available on the HPCC and can be loaded as a module.</p> <p>However, because of the relational database requirement, the HPCC must be contacted in advance to setup a database for your runs. This tutorial briefly describes how to obtain access to the program, and how to use configuration files provided by the HPCC.</p>"},{"location":"MySQL_configuration/#database-access","title":"Database Access","text":"<p>Before beginning your HPCC runs, you will need to complete an\u00a0help request ticket request to request a database and MySQL account for your use of OrthoMCL.\u00a0\u00a0 This information will be provided to you in the form of a configuration file, and you save this to your directory.\u00a0 You can use the filename as a command-line argument to relevant scripts comprising the OrthoMCL application, which tell OrthMCL how to connect to your database.\u00a0</p>"},{"location":"MySQL_configuration/#configuration-file","title":"Configuration File","text":"<p>The following is an example of the configuration file you will receive:</p> <pre><code>dbVendor=mysql\ndbConnectString=dbi:mysql:someUserdb:db-01:3306\ndbLogin=someUser\ndbPassword=somePassword\n# DO NOT CHANGE ANYTHING ABOVE THIS LINE UNLESS YOU KNOW WHAT YOU'RE DOING\nsimilarSequencesTable=SimilarSequences\northologTable=Ortholog\ninParalogTable=InParalog\ncoOrthologTable=CoOrtholog\ninterTaxonMatchView=InterTaxonMatch\npercentMatchCutoff=50\nevalueExponentCutoff=-5\n</code></pre>"},{"location":"MySQL_configuration/#command-example","title":"Command Example","text":"<p>A typical run that would require the database configuration file might look something like the following:</p> <pre><code>orthomclPairs orthomcl.config log_file cleanup=[yes|no|only|all] &lt;startAfter=TAG&gt;\n</code></pre> <p>In the example above, the file \"orthomcl.config\" is the name of the configuration/connection file (provided by the HPCC) that you want to use for your run.</p>"},{"location":"MySQL_configuration/#purging-the-database-between-runs","title":"Purging the Database Between Runs","text":"<p>To facilitate multiple concurrent, or faster consecutive runs, many users ask for more than one database at setup time. \u00a0HPCC will typically be able to provide you with up to four (4) such databases. \u00a0Please specify this in your request.</p> <p>Once your run is completed, you will need to purge the database of its contents prior to beginning new runs using the same database. \u00a0To have this performed, please contact ICER\u00a0 via https://contact.icer.msu.edu, select \"other\" in the form and let us know you need your database purge and the name of the database\u00a0 if you have multiple databases (or just provide the connection string like <code>dbConnectString=dbi:mysql:someUserdb:db-01:3306</code> ).</p> <p>When your work with OrthoMCL is complete, please notify the staff via the ICER contact form so we can purge your databases from the system.</p>"},{"location":"MySQL_configuration/#modifying-the-configuration-file","title":"Modifying the Configuration File","text":"<p>Most users will not need to modify the configuration file provided by the HPCC. \u00a0The most common modification needed will be to change the name of the database to be accessed in those cases where users are provided with access to more than one database.</p> <p>The relevant line to be modified is shown below:</p> <pre><code>dbConnectString=dbi:mysql:someUserdb:db-01:3306 \n</code></pre> <p>In the example above, the database name is \"someUserdb\". \u00a0</p> <p>Let's assume (for example), a user had been issued 4 databases named: someUserdb, someUserdb2, someUserdb3, someUserdb4. \u00a0To perform a run using one of these other databases, we would need to make a copy of the configuration file and change the name in that file, for example:</p> <pre><code>dbConnectString=dbi:mysql:someUserdb2:db-01:3306\n</code></pre> <p>You may then structure the command for each OrthoMCL run to use the configuration file (and related database) desired.</p>"},{"location":"MySQL_configuration/#more-information","title":"More Information","text":"<p>Refer to the OrthoMCL User Manual.</p>"},{"location":"NCBI_Entrez_Direct_tools/","title":"NCBI Entrez Direct tools","text":"<p>Entrez Direct (eDirect) provides access to the NCBI's suite of interconnected databases (publication, sequence, structure, gene, variation, expression, etc.) using Linux command lines. People who need to retrieve sequence and other type of data from NCBI regularly should use the eDirect toolset. To use edirect commands on HPCC, run:</p> <pre><code>ssh dev-intel18\nmodule load Perl/5.28.1\nmodule load edirect\n</code></pre> <p>A full documentation of eDirect is available from https://www.ncbi.nlm.nih.gov/books/NBK179288/</p> <p>A few useful examples can be found in this blog http://bioinformatics.cvr.ac.uk/blog/ncbi-entrez-direct-unix-e-utilities/</p> <p>eDirect cookbook:\u00a0https://github.com/NCBI-Hackathons/EDirectCookbook/blob/master/README.md</p>"},{"location":"Nodes_Available_for_Testing_the_New_OS/","title":"Nodes available for testing the new operating system","text":"<p>Early testing!</p> <p>Please note that the advice and commands on this page are still being tested. Expect the possibility for bugs and errors, and always test for accuracy by comparing against commands you know already work.</p> <p>Check back for updates!</p> <p>This page will continue to be updated with new information. Check back regularly for updates.</p> <p>ICER will progressively transition nodes to the new operating system.</p>","tags":["reference","OS upgrade"]},{"location":"Nodes_Available_for_Testing_the_New_OS/#development-nodes","title":"Development nodes","text":"<p>At the moment, the following development nodes are available for testing access:</p> <ul> <li><code>dev-intel14</code></li> <li><code>dev-intel14-k20</code></li> <li><code>dev-amd20-ubuntu</code></li> </ul> <p>You can access these via SSH like any other development node:</p> <pre><code>ssh user@hpcc.msu.edu  # First SSH into the gateway\nssh dev-intel16-ubuntu  # Then SSH into the desired dev node\n</code></pre>","tags":["reference","OS upgrade"]},{"location":"Nodes_Available_for_Testing_the_New_OS/#slurm-jobs","title":"SLURM jobs","text":"<p>Additionally, a limited number of compute nodes are available with the new operating system. To run jobs on these nodes, include the line <code>#SBATCH --reservation=ubuntu_compute</code> to a job script with any other resource requests, for example:</p> <pre><code>#!/bin/bash --login\n#SBATCH --ntasks=1\n#SBATCH --mem=20M\n#SBATCH --time=01:00:00\n#SBATCH --reservation=ubuntu_compute\n\ncat /etc/os-release  # Should show Ubuntu\n\n# Print resource information\nscontrol show job $SLURM_JOB_ID\njs -j $SLURM_JOB_ID\n</code></pre> <p>At the moment, we have an extremely limited number of nodes available in the following clusters via SLURM:</p> <ul> <li><code>intel14</code> (reserved for OnDemand jobs, which are still a work in progress)</li> <li><code>intel16</code></li> </ul>","tags":["reference","OS upgrade"]},{"location":"OS_Upgrade/","title":"Operating System Upgrade","text":"<p>Early testing!</p> <p>Please note that the advice and commands on this page are still being tested. Expect the possibility for bugs and errors, and always test for accuracy by comparing against commands you know already work.</p> <p>ICER is upgrading the operating system on all development and compute nodes from CentOS 7 to Ubuntu 22.04. This process is expected to be completed in mid-June 2024.</p> <p>As part of this process, all software needs to be rebuilt for the new operating system. This includes all user compiled software, and all software provided by ICER via the module system. ICER will be using this as an opportunity to reevaluate software offerings and organization, and as such, the structure of the module system will change.</p> <p>Check back for updates!</p> <p>This page will continue to be updated with new information. Check back regularly for updates.</p>","tags":["explanation","OS upgrade"]},{"location":"OS_Upgrade/#summary","title":"Summary","text":"<ul> <li>ICER is upgrading the operating system on all nodes from CentOS 7 to Ubuntu 22.04</li> <li>Users will need to change <code>module load</code> commands, possibly updating to different software versions</li> <li>Users will likely need to reinstall/recompile software that is not loaded from the module system (possibly including packages from R, Python, etc) (but should test first)</li> <li>Users may need to reinstall Conda and recreate Conda environments (but should test first)</li> <li>Testing nodes are available</li> <li>Backwards compatibility scripts are available</li> <li>Help is available! Contact us with any problems or questions you have using the subject line \"OS Upgrade 2024\"!</li> </ul> <p>For specific tasks users need to complete to use the new operating system, please see our page on Preparing for the OS Upgrade.</p>","tags":["explanation","OS upgrade"]},{"location":"OS_Upgrade/#why-is-this-happening","title":"Why is this happening?","text":"","tags":["explanation","OS upgrade"]},{"location":"OS_Upgrade/#operating-system-upgrade_1","title":"Operating system upgrade","text":"<p>The HPCC requires operating system updates for similar reasons to a personal computer requiring an operating system upgrade. The current operating system, CentOS 7, was released ten years ago, and is nearing the end of its support. In order to ensure that the HPCC stays secure, operational, and stable, a new, supported operating system will take its place.</p> <p>ICER has chosen to change Linux distributions from CentOS (or any other Red Hat Enterprise Linux derivative) to Ubuntu. This decision involves many factors, but primary amongst them was the ability to continue using ICER's older nodes, increasing the supply of available compute to our users. From a practical perspective, Ubuntu and CentOS will feel very similar to users, and the majority of commands you may be used to using will still continue to work.</p>","tags":["explanation","OS upgrade"]},{"location":"OS_Upgrade/#software-upgrade","title":"Software upgrade","text":"<p>Behind the scenes, changing a computer's operating system also changes the libraries and programs that the software used on a regular basis relies on. This requires reinstallation of nearly every piece of software to ensure that it is compatible with the new system.</p> <p>Historically, ICER has a mix of software from a variety of sources, but primarily uses a tool called EasyBuild to install and manage software. To better manage future installations and upgrades, we are using this as an opportunity to reassess which software we can most effectively support. The end result will be a software stack better aligned with EasyBuild (which stops supporting older pieces of software), with newer versions that can be tracked and updated more easily.</p>","tags":["explanation","OS upgrade"]},{"location":"OS_Upgrade/#estimated-timeline","title":"Estimated timeline","text":"<p>The operating system and software upgrade is happening in stages. The following dates are tentative:</p> By May 17 <p>We plan to transition approximately 25% of the nodes to the new operating system. Most software requested before April 17 should also be installed.</p> By June 17 <p>We plan to transition most of the remainder of the nodes. The remainder of requested and popularly used software should also be installed.</p> <p>Most users should be transitioned to the new operating system. A small portion of nodes will temporarily remain on the CentOS operating system and software stack to help facilitate transition. </p>","tags":["explanation","OS upgrade"]},{"location":"OS_Upgrade_Changelog_and_Known_Issues/","title":"Changelog and Known Issues for OS Upgrade","text":""},{"location":"OS_Upgrade_Changelog_and_Known_Issues/#changelog","title":"Changelog","text":""},{"location":"OS_Upgrade_Changelog_and_Known_Issues/#known-issues","title":"Known issues","text":"<p>Below are issues that ICER is aware of and is taking steps to resolve. If you find an issue that is not on this page, please contact us with the subject line \"OS Upgrade 2024\"</p> <ul> <li><code>OpenMPI/4.1.5-GCCcore-12.3.0</code> has issues when allocating at least 15 tasks to one node, and exactly one task to another. Until we can track down the source of this error, we recommend submitting SLURM scripts specifying <code>--ntasks-per-node</code> instead of just <code>--ntasks</code> so that tasks are spread equally between requested nodes.</li> </ul>"},{"location":"Open_OnDemand/","title":"Open OnDemand","text":"<p>Open OnDemand helps researchers efficiently  utilize the HPCC by providing easy web access with graphical user interfaces from any device. The features include, though are not limited to:</p> <ul> <li>Plugin-Free web experience</li> <li>Easy file management</li> <li>Easy job submission, management, and monitoring </li> <li>Graphical desktop environments and desktop applications;     Jupyter Notebook, RStudio, ...</li> <li>One-click app icons on a desktop to launch your favorite GUI applications </li> <li>Command-line shell access</li> </ul>","tags":["reference","OnDemand"]},{"location":"Open_OnDemand/#connect-to-hpcc-ondemand","title":"Connect to HPCC OnDemand","text":"<p>To connect to HPCC OnDemand, visit\u00a0https://ondemand.hpcc.msu.edu/.  It will first redirect to the CILogon website for authentication.  </p> <p></p> <p>From here, select \"Michigan State University\" as the identity provider and click \"Log On\". This will redirect you to a page where you can log in with your MSU credentials.  </p> <p>Note</p> <p>A valid MSU email address with NetID and password are required to access HPCC OnDemand. This may prohibits the access of external collaborators. Please take this into consideration when you choose tools provided on HPCC OnDemand for your group projects involving external collaborators. </p> <p>After sign in, you will reach the HPCC OnDemand web portal with the following menu options:    </p> <p></p>","tags":["reference","OnDemand"]},{"location":"Open_OnDemand/#files","title":"Files","text":"<p>All user's files in the home, research, scratch, and software file spaces can be accessed.   </p> <p></p> <p>Select a directory then use the <code>File Explorer</code> to download, upload,  view, edit, and move files. You can learn more about the explorer at https://www.osc.edu/resources/online_portals/ondemand/file_transfer_and_management.</p> <p></p> <p>Note</p> <p>The OnDemand portal is best for transferring files less than ~1 GB in size.  For transferring larger files to and from the HPCC,  see Large file transfer (Globus)</p>","tags":["reference","OnDemand"]},{"location":"Open_OnDemand/#jobs","title":"Jobs","text":"<p>Active Jobs: List jobs in the queue; monitor or manage those jobs</p> <p></p> <p>Job Composer: Submit a job script with resource requests and  command lines; create new scripts from a job template, copy a previous job submission,  or select an existing job script from a specified directory. You can learn more about the job composer at https://www.osc.edu/resources/online_portals/ondemand/job_management.</p> <p></p>","tags":["reference","OnDemand"]},{"location":"Open_OnDemand/#interactive-apps","title":"Interactive Apps","text":"<p>Desktops: Request an <code>Interactive Desktop</code>; once it starts, a graphical user interface (GUI) is provided for running GUI based applications</p> <p></p> <p>GUIs: Launch an HPCC provided, GUI based application directly</p> <ul> <li>MATLAB: 2018a-2021a versions are available</li> <li>Stata: 15SE, 15MP, 17SE, 17MP are available</li> <li>Jupyter notebook: By default, Python 3.7.2 will be used; choose      your own Python by selecting 'Launch Jupyter Notebook using     the Anaconda installation in my home directory'</li> <li>RStudio: Various versions are available (3.5.0~ 4.1.2)</li> </ul> <p>Note</p> <p>Please make sure you request enough memory. Otherwise, your interactive  session won't start or your running processes will be terminated prior to completion. </p>","tags":["reference","OnDemand"]},{"location":"Open_OnDemand/#development-nodes","title":"Development Nodes","text":"<p>Start a bash session a specified development node and input commands using the terminal's command line interface. </p> <p></p> <p>Note</p> <p>Currently, there is no remote graphical display capability while in the terminal i.e., no X11 forwarding. For GUI based applications, please use the Interactive Apps feature. </p>","tags":["reference","OnDemand"]},{"location":"Open_OnDemand/#my-interactive-sessions","title":"My Interactive Sessions","text":"<p>All of a user's interactive jobs are displayed, along with a description of the resources currently allocated to each job. User's may then manage aspects of those jobs using the associated action buttons. The Delete button will end the interactive session.</p> <p></p>","tags":["reference","OnDemand"]},{"location":"Open_OnDemand/#debugging-information","title":"Debugging Information","text":"<p>OnDemand jobs produce log files that can help ICER staff fix issues with apps. If you submit a ticket, it helps to provide the <code>output.log</code> file for the OnDemand session with a problem.</p> <p>To access this file, click the Session ID in the Interactive Session card of the session experiencing the issue (red circle below).</p> <p></p> <p>In the file browser that appears, click the checkbox next to <code>output.log</code> (red circle below) in addition to any other files you may be asked for, then click the Download button at the top of the file browser (purple arrow below).</p> <p></p> <p>Please attach the <code>output.log</code> file (and any other files you are asked for) to a ticket to assist ICER staff.</p> <p>We are currently working to add more functionality to the OnDemand interface. Please feel free to contact us if you have any questions or suggestions.</p>","tags":["reference","OnDemand"]},{"location":"Powertools/","title":"Powertools overview","text":"<p>Powertools are useful commands that provide additional functionality or simpler ways to access existing functionality on the HPCC. To view a list of all  available powertools with their documentation, run <code>powertools</code> on a development node. </p> <p>By default, powertools is loaded when users log into a development node.  Users may also run the <code>module list</code> command to check if it is loaded.  If it is not, please use the command:</p> <p><code>$ module load powertools</code></p> <p>to load the module before running powertools commands. Powertools can also be  used in job submission scripts the same as any other commands, if the powertools  module has been loaded.</p> <p>A selection of widely useful powertools are listed below as links to their description pages.</p>"},{"location":"Powertools/#qs","title":"qs","text":"<p>Display job list.</p>"},{"location":"Powertools/#node_status","title":"node_status","text":"<p>Display a list of compute nodes and their properties.</p>"},{"location":"Powertools/#bi","title":"bi","text":"<p>Information of Buy-In Account</p>"},{"location":"Powertools/#js","title":"js","text":"<p>Display job steps and their resource usages.</p>"},{"location":"Powertools/#getexample","title":"getexample","text":"<p>Download user examples.</p>"},{"location":"Powertools_longjob_by_DMTCP/","title":"Powertools longjob by DMTCP","text":"<p>The following are instructions for trying out <code>longjob</code> powertool on HPCC system. First, you start with a basic submission script. For example, consider the following simple submission script:</p> <pre><code>#!/bin/bash -login\n#SBATCH --nodes=1 --ntasks=1 --cpus-per-task=1\n#SBATCH --time=168:00:00\n#SBATCH --mem=2gb\n#SBATCH --constraint=intel16\n#SBATCH --job-name=MyJob0\n#SBATCH --mail-type=FAIL,END\n\nmodule purge\nmodule load GCC/7.3.0-2.30 OpenMPI/3.1.1\n\nsrcdir=${SLURM_SUBMIT_DIR}/bin/\nWORK=/mnt/scratch/${USER}/KineticSN/${SLURM_JOBID}\nmkdir -p ${WORK}\n\n# Copy files to work directory\ncp -r $srcdir/* $WORK/\n\n#Move to the working directory\ncd $WORK\n\n#Run my program\n./SimulationTest -scattering_flag 0 -weak_reaction_flag 0 -outputVisData 100\nret=$?\n\nscontrol show job ${SLURM_JOBID}\n\nexit $ret\n</code></pre> <p>To get <code>longjob</code> to work, the following modifications might need to be made:</p> <ol> <li>Change walltime to be less than 4 hours if you would like to have     more available nodes to your job.</li> <li>Wrap all setup-code that only needs to be run once in an if     statement that checks for the file \"Files_Copied\". This will ensure     that the setup-code only runs the first time the script is run     because in the first time there should be no file with the name     \"Files_Copied\".</li> <li>Add the <code>longjob</code> command before the command in the submission     script that you want to checkpoint.</li> <li> <p>Load the powertools module and turn on aliases. i.e. add the     following lines of code to the script:</p> <pre><code>shopt -s expand_aliases\nmodule load powertools\n</code></pre> </li> <li> <p>Set the following environment variables as appropriate for your job:</p> <ul> <li>JobScript\u00a0 \u2013\u00a0 Name of the job script file which will get     resubmitted. The default is the first submitted job script name.</li> <li>DMTCP_Checkpoint_Time\u00a0 \u2013\u00a0\u00a0Time (in seconds) which DMTCP     needs to work on checkpointing. The default is 5 minutes.</li> <li>DMTCP_CHECKPOINT_INTERVAL\u00a0 \u2013\u00a0 Time (in seconds) between     automatic checkpoints. The default is 4-8 hours. For walltime     less than 4 hours, the default will do checkpointing once at     ${DMTCP_Checkpoint_Time} + 1 minute before the end of     walltime.\u00a0</li> <li>DMTCP_CHECKPOINT_DIR\u00a0\u00a0\u2013\u00a0 Name of the directory to save     checkpoint image and log flies. The default is     ckpt_${SLURM_JOB_NAME}. For job array, the default is     ckpt_${SLURM_JOB_NAME}_${SLURM_ARRAY_TASK_ID}. If two     different jobs use the the same directory to run with the     <code>longjob</code> command, please make sure the environment variables     (or SLURM_JOB_NAME) are set different so their image files are     not saved in the same directory.</li> </ul> </li> </ol> <p>The following is a modified example script with the changes:</p> <pre><code>#!/bin/bash -login\n#SBATCH --nodes=1 --ntasks=1 --cpus-per-task=1\n#SBATCH --time=04:00:00\n#SBATCH --mem=2gb\n#SBATCH --constraint=intel16\n#SBATCH --job-name=MyJob0\n#SBATCH --mail-type=FAIL,END\n\nmodule purge\nmodule load GCC/7.3.0-2.30 OpenMPI/3.1.1\nmodule load powertools\n\n# Change checkpointing environment variables if necessary:\n# export DMTCP_Checkpoint_Time=60                     -- change checkpointing time\n# export DMTCP_CHECKPOINT_INTERVAL=7200               -- change time interval between checkpoints\n# export DMTCP_CHECKPOINT_DIR=ckpt_${SLURM_JOB_NAME}  -- change where to save checkpointing files\n\n# Change to a directory other than ${SLURM_SUBMIT_DIR} if necessary:\n# cd /mnt/scratch/${USER}/WorkPlace\n\nif [ ! -f Files_Copied ]\nthen\n    srcdir=${SLURM_SUBMIT_DIR}/bin/\n    WORK=/mnt/scratch/${USER}/KineticSN/${SLURM_JOBID}\n    mkdir -p ${WORK}\n\n    # Copy files to work directory\n    cp -r $srcdir/* $WORK/\n\n    #Run main simulation program\n    cd $WORK\n    touch Files_Copied \n\nfi\nlongjob ./SimulationTest -scattering_flag 0 -weak_reaction_flag 0 -outputVisData 100\nret=$?\n\nexit $ret\n</code></pre> <p>If everything works as expected, you should be able to submit the above job script and it will resubmit itself until the job completes. Note, this is rough code, not completely tested and does work in all cases. For example, one case that could propose a problem is if the main program gets caught in a loop and never exits. In this case, the code will keep submitting itself indefinitely. \u00a0Note that in addition you can't include output redirection as you'd expect, that is a command like <code>myprogram.py &gt; myoutput.txt</code> \u00a0and <code>longjob myprogram.py &gt; myoutput.py</code> is not the same (the redirection here applies to <code>longjob</code>, not your program). \u00a0  </p> <p>If you have difficulty, please contact us.</p>","tags":["tutorial","checkpointing","powertools"]},{"location":"Preparing_for_the_OS_Upgrade/","title":"Preparing for the Operating System Upgrade","text":"<p>Early testing!</p> <p>Please note that the advice and commands on this page are still being tested. Expect the possibility for bugs and errors, and always test for accuracy by comparing against commands you know already work.</p> <p>Check back for updates!</p> <p>This page will continue to be updated with new information. Check back regularly for updates.</p>","tags":["how-to guide","OS upgrade"]},{"location":"Preparing_for_the_OS_Upgrade/#recommendations-to-users","title":"Recommendations to users","text":"<p>In order to prepare for transition to the upgraded system and stay productive, ICER is asking all users to:</p> <ol> <li>Log into the test development nodes.</li> <li>Compare the modules you load to those already available on the upgraded system<ul> <li>Note: The module names will have changed in the upgraded system, and not all versions will be carried over from the current system</li> <li>Use <code>module spider &lt;module_name&gt;</code> or <code>module avail</code> to find a module you need</li> <li>If a module you need is not available and you have not already requested it, please request it using our form. Please check that all available versions do not work before requesting a different version.</li> </ul> </li> <li>Try running a small test of your existing code on the development nodes, replacing the module names with those available on the upgraded system.</li> <li>Try running a small test of your existing code with a SLURM job.</li> <li>If your code no longer runs as expected it is likely incompatible with the new operating system. If you compile your own code or have built/installed packages in R or Python, rebuild using the modules available on the upgraded system. We recommend doing this in a separate location so your existing code will continue to work.</li> <li>Test your existing codes and scripts with our backwards compatibility helper commands as a backup.</li> </ol> <p>If you run into problems at any of these steps, please contact us using the subject \"OS Upgrade 2024\" with a description of the problem and any files necessary to reproduce it.</p>","tags":["how-to guide","OS upgrade"]},{"location":"Python/","title":"Python packages","text":"<p>It's difficult for the HPCC to host a vast volume of Python packages, and to resolve conflicts between Python versions. Users are encouraged to install and manage packages on their own, through conda or virtual environments.</p>"},{"location":"Python/#using-conda","title":"Using conda","text":"<p>Users install their own version of Python through Anaconda in the home or research space. This gives them full control on their preferred versions of Python and packages. Isolated virtual environments can be created and controlled by conda environments.</p>"},{"location":"Python/#using-virtualenv","title":"Using virtualenv","text":"<p>By default, Python version 3.6.4 (compiled by GCC/6.4.0) is loaded and ready to use after logging into HPCC dev-nodes. Other versions can be found by running <code>module spider Python</code>. Usually, pre-installed packages are available for each Python version. If users need to install new ones, they can install them in the home or research space through virtual environments.</p>"},{"location":"Python_on_HPC/","title":"Python on HPC","text":"<p>Python is popular because it makes a great first impression; i) clean, clear syntax, ii) multi-paradigm, iii) interpreted, iv) duck typing, garbage collection, and most of all, v) instant productivity. It keeps up with users' needs. It has i) flexible, full-featured data structures ii) extensive standard libraries iii) reusable open-source package iv) package management tools v) good unit testing frameworks. </p> <p>In exchange for user-friendliness and ease of use, Python becomes one of the slowest computer languages, primarily because it is an interpreted language, and allows a single thread to run in the interpreter's memory space at once. Python is typically 30 to 300 times slower than C or Fortran. However, Python has a powerful and enthusiastic open-source community which continuously improves the capability of Python.\u00a0</p> <p>On this page, we want to</p> <ul> <li>explain what the MSU HPCC is doing to support Python users.</li> <li>provide guidance to help users improve Python performance on the     HPCC.</li> <li>point out tools that support developers of Python on the HPCC.</li> </ul> <p>We assume that</p> <ul> <li>you know and use Python, or</li> <li>you know and use the HPCC and are curious about using Python in your own     HPCC work.</li> </ul>","tags":["explanation","Python","Jupyter"]},{"location":"Python_on_HPC/#how-to-use-python-on-the-hpcc","title":"How to use Python on the HPCC?","text":"<p>Python applications usually use packages and modules that require specific version of libraries. This means one installed application may conflict with another application due to using the same library but with different versions. It is difficult to meet the requirements of every application by one global Python installation. To resolve this issue,</p> <ul> <li>users can create an isolated virtual environment\u00a0with a particular     version of Python on our system in a self-contained directory of     their home or research space.</li> <li>users can\u00a0install their own version of Python through     Anaconda in their home or research     space. This gives users full control on their preferred versions of     python and packages.</li> </ul> <p>To get started using Python on the HPCC, you have to load a Python module. A few helpful module commands would be <code>module avail Python</code>, <code>module spider Python</code>, and <code>module load Python</code>. More information on our module system can be found here.</p>","tags":["explanation","Python","Jupyter"]},{"location":"Python_on_HPC/#python-with-virtual-environments","title":"Python with virtual environments","text":"<p>More details of how to use virtual environments can be found at this page.</p>","tags":["explanation","Python","Jupyter"]},{"location":"Python_on_HPC/#python-with-conda","title":"Python with Conda","text":"<p>More details of how to use Conda can be found at this page.</p>","tags":["explanation","Python","Jupyter"]},{"location":"Python_on_HPC/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>For Jupyter notebook users, we have the Open OnDemand platform  To connect to HPCC OnDemand, visit\u00a0https://ondemand.hpcc.msu.edu.\u00a0After logging in, choose interactive apps, select Jupyter Notebook, request resources as you need. Your Jupyter Notebook will start when the requested resources are ready.</p> <p></p> <p></p>","tags":["explanation","Python","Jupyter"]},{"location":"Python_on_HPC/#can-my-python-code-be-faster","title":"Can my Python code be faster?","text":"<p>Now, you are ready to use Python on the HPCC. Now, let's learn a few tips to make your Python codes faster.</p>","tags":["explanation","Python","Jupyter"]},{"location":"Python_on_HPC/#vectorization","title":"Vectorization","text":"<p>Vectorization speeds up Python code without using loops. Instead of loops, NumPy can help by minimizing the running time of code efficiently. NumPy offers various operations to be performed over vectors such as the dot product, cross product, and matrix multiplication. See the Numpy array documentation for more information.</p>","tags":["explanation","Python","Jupyter"]},{"location":"Python_on_HPC/#numba","title":"Numba","text":"<p>Numba\u00a0compiles Python codes just in time with a few decorators, without much modification of code.\u00a0In addition, Numba offers automatic parallelization which is very easy to use. You just need to add the one line decorator,\u00a0<code>@njit(parallel=True)</code>. More information can be found here. Numba also supports NVIDIA CUDA. It is easy to use (at least much easier to use than other programming languages). </p>","tags":["explanation","Python","Jupyter"]},{"location":"Python_on_HPC/#use-threaded-libraries","title":"Use Threaded Libraries","text":"<p>Packages like NumPy and SciPy are already built with MPI and multithread support via BLAS/LAPACK, and MKL. In general, it is a plausible guess that most solvers have already been implemented in pure Python. In addition, many major threaded libraries and packages already have binds such as PyTrilinos, Petsc4py, Elemental, and SLEPc. So, don't try to reinvent the wheel. If it is not new, it is probably already implemented for high performance.</p>","tags":["explanation","Python","Jupyter"]},{"location":"Python_on_HPC/#mpi","title":"MPI","text":"<p>Python has a package for MPI, mpi4py.</p> <p>It is</p> <ul> <li>a pythonic wrapping of the system's native MPI.</li> <li>a provider of almost all MPI-1, 2 and common MPI-3 features.</li> <li>very well maintained.</li> <li>distributed with major Python distributions.</li> <li>portable and scalable.</li> <li>dependent only on NumPy, Cython (build only), and MPI libraries.</li> </ul> <p>More information can be found here.</p>","tags":["explanation","Python","Jupyter"]},{"location":"Python_on_HPC/#other-python-resources","title":"Other Python Resources","text":"<p>The following are a few Python resource links.</p> <ul> <li>https://www.python.org/about/gettingstarted/</li> <li>https://wiki.python.org/moin/BeginnersGuide/</li> <li>https://www.codecademy.com/learn/python/</li> <li>https://www.coursera.org/specializations/python/</li> <li>https://software-carpentry.org/lessons/</li> <li>https://pymotw.com/</li> <li>HPCC wiki Python page</li> <li>Python video on youtube</li> <li>https://www.py4e.com/ </li> </ul>","tags":["explanation","Python","Jupyter"]},{"location":"QIIME_2/","title":"QIIME 2","text":"<p>QIIME (Quantitative Insights Into Microbial Ecology) is an open-source bioinformatics pipeline for performing microbiome analysis from raw DNA sequencing data. Since QIIME 1 is no longer supported officially (see announcement at http://qiime.org), it's not installed on the CentOS 7 system of HPCC. The way QIIME 2 is installed and run on the HPCC CentOS 7 is through conda https://docs.qiime2.org/2018.2/install/native/.</p> <p>You may follow our instructions to install anaconda in your home directory.</p> <p>Below is how to install QIIME 2 (version 2018.2) in your home directory via conda:</p>","tags":["tutorial","QIIME 2","bioinformatics"]},{"location":"QIIME_2/#install-qiime-2","title":"Install QIIME 2","text":"<pre><code>export PATH=$PATH:$HOME/anaconda3/bin\nwget https://data.qiime2.org/distro/core/qiime2-2018.2-py35-linux-conda.yml\nconda env create -n qiime2-2018.2 --file qiime2-2018.2-py35-linux-conda.yml\nrm qiime2-2018.2-py35-linux-conda.yml\n\nsource activate qiime2-2018.2\nqiime --help # test if installation is successful\n# all your QIIME commands go here\nsource deactivate\n</code></pre>","tags":["tutorial","QIIME 2","bioinformatics"]},{"location":"QIIME_2/#example-of-analysis","title":"Example of analysis","text":"<p>The example below is from a previous version of the current tutorial.</p> <pre><code>mkdir qiime2-moving-pictures-tutorial\ncd qiime2-moving-pictures-tutorial\nwget -O \"sample-metadata.tsv\" \"https://data.qiime2.org/2018.2/tutorials/moving-pictures/sample_metadata.tsv\"\n\nmkdir emp-single-end-sequences\nwget -O \"emp-single-end-sequences/barcodes.fastq.gz\" \"https://data.qiime2.org/2018.2/tutorials/moving-pictures/emp-single-end-sequences/barcodes.fastq.gz\"\nwget -O \"emp-single-end-sequences/sequences.fastq.gz\" \"https://data.qiime2.org/2018.2/tutorials/moving-pictures/emp-single-end-sequences/sequences.fastq.gz\"\n\nqiime tools import \\\n  --type EMPSingleEndSequences \\\n  --input-path emp-single-end-sequences \\\n  --output-path emp-single-end-sequences.qza\n\nqiime demux emp-single \\\n  --i-seqs emp-single-end-sequences.qza \\\n  --m-barcodes-file sample-metadata.tsv \\\n  --m-barcodes-column BarcodeSequence \\\n  --o-per-sample-sequences demux.qza\n\nqiime demux summarize \\\n  --i-data demux.qza \\\n  --o-visualization demux.qzv\n\nqiime tools view demux.qzv\n</code></pre> <p>A full list of tutorials for QIIME2 can be found here and there is also list of plugins for QIIME2 for handling tasks such as trimming adaptors, demultiplexing, and denoising.</p>","tags":["tutorial","QIIME 2","bioinformatics"]},{"location":"RCentOS7generalinfo/","title":"General information","text":"","tags":["tutorial","R"]},{"location":"RCentOS7generalinfo/#a-quick-start","title":"A quick start","text":"<p>R 4.0.2 has the largest number of libraries installed and is the recommended one to use. To load it, you can run:</p> <p><code>module purge</code> <code>module load GCC/8.3.0 OpenMPI/3.1.4</code> <code>module load R/4.0.2</code></p> <p>Some users may have a local R package directory specified in <code>~/.Renviron</code>; this may create a problem if you load your local packages which were built with an older version of R. Unless you have updated them all, we recommend that you launch R by <code>R --no-environ</code> which suppresses the search of local packages.</p> <p>For other versions of R, run <code>module spider R</code> to get a complete list. </p>","tags":["tutorial","R"]},{"location":"RCentOS7generalinfo/#be-sure-to-specify-version-number","title":"Be sure to specify version number","text":"<p>While it is valid to load R simply by <code>module load R</code>, it may point to a different R version than your desired one. Please be specific about the version, as with any software module.</p>","tags":["tutorial","R"]},{"location":"RCentOS7generalinfo/#rstudio","title":"Rstudio","text":"<p>The best way to launch an Rstudio session is to log into our OnDemand server, which is dedicated to running GUI applications. Here is a short video showing how to request an Rstudio \"job\" from the HPCC cluster after you've logged into OnDemand.</p>","tags":["tutorial","R"]},{"location":"RNA_seq_example/","title":"RNA-seq example","text":"<p>While RNA-seq analysis pipeline is changing, here is an example for demonstration purpose. The pipeline involves <code>tophat</code>, <code>cufflinks</code> and so on. We will provide updates to this page as needed.</p>"},{"location":"R_others/","title":"Some packages and other information","text":"","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#rstan","title":"rstan","text":"<p>The example here follows that in\u00a0RStan Getting Started.\u00a0To test rstan on the HPCC, first load R 3.6.2:</p> <pre><code>module purge  \nmodule load GCC/8.3.0 OpenMPI/3.1.4\u00a0R/3.6.2-X11-20180604\n</code></pre> <p>As of February 2020, the rstan version is 2.19.2.</p> <p>The stan model file \"8schools.stan\" contains:</p> <pre><code>data {\n  int&lt;lower=0&gt; J;         // number of schools \n  real y[J];              // estimated treatment effects\n  real&lt;lower=0&gt; sigma[J]; // standard error of effect estimates \n}\nparameters {\n  real mu;                // population treatment effect\n  real&lt;lower=0&gt; tau;      // standard deviation in treatment effects\n  vector[J] eta;          // unscaled deviation from mu by school\n}\ntransformed parameters {\n  vector[J] theta = mu + tau * eta;        // school treatment effects\n}\nmodel {\n  target += normal_lpdf(eta | 0, 1);       // prior log-density\n  target += normal_lpdf(y | theta, sigma); // log-likelihood\n}\n</code></pre> <p>The R code (\"run.R\") to run stan model contains:</p> <pre><code>library(\"rstan\")\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\nschools_dat &lt;- list(J = 8,\ny = c(28, 8, -3, 7, -1, 1, 18, 12),\nsigma = c(15, 10, 16, 11, 9, 11, 10, 18))\nfit &lt;- stan(file = '8schools.stan', data = schools_dat)\nprint(fit)\npairs(fit, pars = c(\"mu\", \"tau\", \"lp__\"))\nla &lt;- extract(fit, permuted = TRUE) # return a list of arrays\nmu &lt;- la$mu\n</code></pre> <p>To run the model from the command line:</p> <p><code>Rscript run.R</code></p> <p>In addition to the results printed to the stdout, there is an R object file named 8schools.rds generated. This is due to that we've set auto_write to TRUE in run.R. More about the auto_write option:</p> <p>Logical, defaulting to the value of rstan_options(\"auto_write\"), indicating whether to write the object to the hard disk using saveRDS. Although this argument is FALSE by default, we recommend calling rstan_options(\"auto_write\" = TRUE) in order to avoid unnecessary recompilations. If file is supplied and its dirname is writable, then the object will be written to that same directory, substituting a .rds extension for the .stan extension. Otherwise, the object will be written to the tempdir.</p>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#rjags","title":"rjags","text":"<p>To use {rjags}, first load R and JAGS\u00a0from a dev-node (e.g. dev-amd20) as follows:</p>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#loading-r-and-jags","title":"Loading R and JAGS","text":"<pre><code>module purge\nmodule load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2\nmodule load JAGS/4.3.0\n</code></pre> <p>Next, we will run a short example of data analysis using rjags. This example comes from\u00a0this tutorial which presents many Bayesian models using this package.</p> <p>To invoke R from the command line: <code>R --vanilla</code></p> <p>Then, in the R console, you can run the following codes (for detailed explanation refer to the tutorial mentioned above):</p>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#sample-r-code-using-rjags-commands","title":"Sample R code using {rjags} commands","text":"<pre><code>library(rjags)\ndata &lt;- read.csv(\"data1.csv\")\nN &lt;- length(data$y)\ndat &lt;- list(\"N\" = N, \"y\" = data$y, \"V\" = data$V)\ninits &lt;- list( d = 0.0 )\n\njags.m &lt;- jags.model(file = \"aspirinFE.txt\", data=dat, inits=inits, n.chains=1, n.adapt=500)\nparams &lt;- c(\"d\", \"OR\")\nsamps &lt;- coda.samples(jags.m, params, n.iter=10000)\nsummary(window(samps, start=5001))\nplot(samps)\n</code></pre> <p>where the two input files, <code>data1.csv</code> and <code>aspirinFE.txt</code>, need to be located in the working directory. The content of the two files is below.</p> data1.csv<pre><code>N,y,V\n1,0.3289011,0.0388957\n2,0.3845458,0.0411673\n3,0.2195622,0.0204915\n4,0.2222206,0.0647646\n5,0.2254672,0.0351996\n6,-0.1246363,0.0096167\n7,0.1109658,0.0015062\n</code></pre> aspirinFE.txt<pre><code>model {\n\n    for ( i in 1:N ) {\n\n        P[i] &lt;- 1/V[i]\n        y[i] ~ dnorm(d, P[i])\n    }\n\n    ### Define the priors\n    d ~ dnorm(0, 0.00001)\n\n    ### Transform the ln(OR) to OR\n    OR &lt;- exp(d)\n}\n</code></pre> <p>A screen shot of the entire run including the output figures is attached here. </p>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#r2openbugs","title":"R2OpenBUGS","text":"<p>R package R2OpenBUGS is available on the HPCC.</p> <p>OpenBUGS is a software package that performs Bayesian inference Using Gibbs Sampling. The latest version of OpenBUGS on the HPCC is 3.2.3.\u00a0R2OpenBUGS is an R package that allows users to run OpenBUGS from R.</p> <p>To load R and OpenBUGS, run:</p> <pre><code>module purge  \nmodule load\u00a0GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2  \nmodule load OpenBUGS\n</code></pre> <p>Then, in your R session, use <code>library(R2OpenBUGS)</code> to load this package. You can execute the following testing R code to see if it works for you.</p>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#testing-r2openbugs","title":"Testing R2OpenBUGS","text":"<pre><code>library(R2OpenBUGS)\n\n# Define the model \nBUGSmodel &lt;- function() {\n\n    for (j in 1:N) {\n        Y[j] ~ dnorm(mu[j], tau)\n        mu[j] &lt;- alpha + beta * (x[j] - xbar)\n    }\n\n    # Priors\n    alpha ~ dnorm(0, 0.001)\n    beta ~ dnorm(0, 0.001)\n    tau ~ dgamma(0.001, 0.001)\n    sigma &lt;- 1/sqrt(tau)\n}\n\n# Data\nData &lt;- list(Y = c(1, 3, 3, 3, 5), x = c(1, 2, 3, 4, 5), N = 5, xbar = 3)\n\n# Initial values for the MCMC\nInits &lt;- function() {\n    list(alpha = 1, beta = 1, tau = 1)\n}\n\n# Run BUGS \nout &lt;- bugs(data = Data, inits = Inits, parameters.to.save = c(\"alpha\", \"beta\", \"sigma\"), model.file = BUGSmodel, n.chains = 1, n.iter = 10000)\n\n# Examine the BUGS output\nout\n\n# Inference for Bugs model at \"/tmp/RtmphywZxh/model77555ea534aa.txt\",\n# Current: 1 chains, each with 10000 iterations (first 5000 discarded)\n# Cumulative: n.sims = 5000 iterations saved\n#          mean  sd 2.5%  25%  50%  75% 97.5%\n# alpha     3.0 0.6  1.9  2.7  3.0  3.3   4.1\n# beta      0.8 0.4  0.1  0.6  0.8  1.0   1.6\n# sigma     1.0 0.7  0.4  0.6  0.8  1.2   2.8\n# deviance 13.0 3.7  8.8 10.2 12.0 14.6  22.9\n#\n# DIC info (using the rule, pD = Dbar-Dhat)\n# pD = 3.9 and DIC = 16.9\n# DIC is an estimate of expected predictive error (lower deviance is better).\n</code></pre>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#r-interface-to-tensorflowkeras","title":"R interface to TensorFlow/Keras","text":"<p>After you've installed TF in your conda environment, load R 4.1.2 and set up a few environment variables:</p> <pre><code>module purge; module load GCC/11.2.0  OpenMPI/4.1.1 R/4.1.2\nexport CONDA_PREFIX=/mnt/home/user123/anaconda3/envs/tf_gpu_Feb2023\nexport LD_LIBRARY_PATH=$CONDA_PREFIX/lib/:$CONDA_PREFIX/lib/python3.9/site-packages/tensorrt:$LD_LIBRARY_PATH\nexport XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib # to fix \"Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice.\"\n</code></pre> <p>Above, you need to change <code>CONDA_PREFIX</code> to your own conda env path (that is, the directory where you have installed Anaconda).</p> <p>Then, run the following R code to test if R/TF works as expected.</p> <pre><code>library(tensorflow)\nlibrary(keras)\nlibrary(tfdatasets)\n\nuse_python(\"/mnt/home/user123/anaconda3/envs/tf_gpu_Feb2023/bin\")\nuse_condaenv(\"/mnt/home/user123/anaconda3/envs/tf_gpu_Feb2023\")\n\n# simple test\ntf$config$list_physical_devices(\"GPU\")\n\n# model training\n\n# loading the keras inbuilt mnist dataset\ndata&lt;-dataset_mnist()\n\n#separating train and test file\ntrain_x&lt;-data$train$x\ntrain_y&lt;-data$train$y\ntest_x&lt;-data$test$x\ntest_y&lt;-data$test$y\n\nrm(data)\n\n# converting a 2D array into a 1D array for feeding into the MLP and normalising the matrix\ntrain_x &lt;- array(train_x, dim = c(dim(train_x)[1], prod(dim(train_x)[-1]))) / 255\ntest_x &lt;- array(test_x, dim = c(dim(test_x)[1], prod(dim(test_x)[-1]))) / 255\n\n# converting the target variable to once hot encoded vectors using keras inbuilt function\ntrain_y&lt;-to_categorical(train_y,10)\ntest_y&lt;-to_categorical(test_y,10)\n\n# defining a keras sequential model\nmodel &lt;- keras_model_sequential()\n\n# defining the model with 1 input layer[784 neurons], 1 hidden layer[784 neurons] with dropout rate 0.4 and 1 output layer[10 neurons]\n# i.e number of digits from 0 to 9\n\nmodel %&gt;% \nlayer_dense(units = 784, input_shape = 784) %&gt;% \nlayer_dropout(rate=0.4)%&gt;%\nlayer_activation(activation = 'relu') %&gt;% \nlayer_dense(units = 10) %&gt;% \nlayer_activation(activation = 'softmax')\n\n# compiling the defined model with metric = accuracy and optimiser as adam.\nmodel %&gt;% compile(\nloss = 'categorical_crossentropy',\noptimizer = 'adam',\nmetrics = c('accuracy')\n)\n\n# fitting the model on the training dataset\nmodel %&gt;% fit(train_x, train_y, epochs = 20, batch_size = 128)\n\n# evaluating model on the cross validation dataset\nloss_and_metrics &lt;- model %&gt;% evaluate(test_x, test_y, batch_size = 128)\n</code></pre>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#r-351-with-intel-mkl","title":"R 3.5.1 with Intel MKL","text":"<p>Intel MKL can accelerate R's speed in linear algebra calculations (such as cross-product, matrix decomposition, inverse computation, linear regression and etc.) by providing BLAS with higher performance.  On the HPCC, only 3.5.1 has been built by linking to Intel MKL.</p>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#loading-r","title":"Loading R","text":"","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#loading-r-351-built-w-openblas","title":"Loading R 3.5.1 built w/ OpenBLAS","text":"<pre><code>module purge\nmodule load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604\n</code></pre>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#loading-r-351-built-w-mkl","title":"Loading R 3.5.1 built w/ MKL","text":"<pre><code>module purge\nmodule load R-Core/3.5.1-intel-mkl\n</code></pre> <p>You could double check the BLAS/LAPACK libraries linked by running <code>sessionInfo()</code> in R.</p>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#benchmarking","title":"Benchmarking","text":"<p>We have a simple R code,\u00a0<code>crossprod.R</code>,\u00a0for testing the computation time.\u00a0The code is below, where the function <code>crossprod</code>\u00a0can run in a multi-threaded mode implemented by OpenMP.</p> crossprod.R<pre><code>set.seed(1)\nm &lt;- 5000\nn &lt;- 20000\nA &lt;- matrix(runif (m*n),m,n)\nsystem.time(B &lt;- crossprod(A))\n</code></pre> <p>Now, open an interactive SLURM job session by requesting 4 cores:</p>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#benchmarking-openblas-vs-mkl-multi-threads","title":"Benchmarking OpenBLAS vs. MKL (multi-threads)","text":"<pre><code>salloc --time=2:00:00 --cpus-per-task=4 --mem=50G --constraint=intel18\n\n# After you are allocated a compute node, do the following.\n\n# 1) Run R/OpenBLAS\nmodule purge\nmodule load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604\nRscript --vanilla crossprod.R &amp;\n# Time output:\n# user     system  elapsed \n# 50.036   1.574   14.156\n\n\n# 2) Run R/MKL\nmodule purge\nmodule load R-Core/3.5.1-intel-mkl\nRscript --vanilla crossprod.R &amp;\n# Time output:\n# user     system  elapsed \n# 28.484   1.664   8.737\n</code></pre> <p>Above, the boost in speed is clear from using MKL as compared with OpenBLAS.</p> <p>Even if we use a single thread (by requesting one core), MKL still shows some advantage.</p>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#benchmarking-openblas-vs-mkl-single-thread","title":"Benchmarking OpenBLAS vs. MKL (single-thread)","text":"<pre><code>salloc --time=2:00:00 --cpus-per-task=1 --mem=50G --constraint=intel18\n\n# After you are allocated a compute node, do the following.\n\n# 1) Run R/OpenBLAS\nmodule purge\nmodule load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604\nRscript --vanilla crossprod.R &amp;\n# Time output:\n# user     system  elapsed \n# 47.763   0.598   49.287\n\n\n# 2) Run R/MKL\nmodule purge\nmodule load R-Core/3.5.1-intel-mkl\nRscript --vanilla crossprod.R &amp;\n# Time output:\n# user     system  elapsed \n# 25.846   0.641   27.006\n</code></pre>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#notes","title":"Notes","text":"<p>When loading R, the OpenMP environment variable\u00a0<code>OMP_NUM_THREADS</code> is left unset.\u00a0This means that when running R code directly on a dev-node, all CPUs on that node will be used by the internal multithreading library compiled into R. This is discouraged since the node will be overloaded and your job may even fail. Therefore, please set <code>OMP_NUM_THREADS</code> to a proper value before running the R code. For example,</p> <p><code>$ OMP_NUM_THREADS=4</code></p> <p><code>$ Rscript --vanilla crossprod.R</code></p> <p>On the other hand, when the code is run on a compute node allocated by SLURM, you don\u2019t need to set OMP_NUM_THREADS as R would automatically detect CPUs available for use (which should have been requested in your salloc command or sbatch script).</p>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_workshop_tutorial/","title":"R workshop tutorial","text":""},{"location":"R_workshop_tutorial/#preparation","title":"Preparation","text":"<ol> <li>Basic knowledge of R\u00a0language, Linux, and the HPCC environment.</li> <li>Login<ol> <li><code>ssh -XY YourAccount@hpcc.msu.edu</code></li> <li><code>ssh\u00a0dev-amd20</code></li> </ol> </li> <li>We will be using R 4.0.2. To load     it:\u00a0<code>module purge; module load\u00a0GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2</code></li> <li>Copy files that we are using in this workshop to your home     directory: <code>cp -r /mnt/research/common-data/workshops/R-workshop .</code></li> </ol>"},{"location":"R_workshop_tutorial/#r-startup","title":"R startup","text":"<p>When an R session starts, it looks for and loads two hidden configuration files, <code>.Renviron</code> and <code>.Rprofile</code>.</p> <ul> <li><code>.Renviron</code>: contains environment variables to be set in R sessions</li> <li><code>.Rprofile</code>: contains R commands to be run in R sessions</li> </ul> <p>The following search order is applied: your current working directory, your home directory, and the system-wide <code>R_HOME/etc/</code> (you can use R command <code>R.home()</code> to check path of <code>R_HOME</code>). Below are examples of the two files which have been placed in our R workshop directory. You need to use <code>ls -a</code> to list them since they are \"hidden\" files.</p> <p><code>.Rprofile</code> (an example)</p> <pre><code>cat(\"Sourcing .Rprofile from the R-workshop directory.\\n\")\n\n# To avoid setting the CRAN mirror each time you run install.packages\nlocal({\n  options(repos = \"https://repo.miserver.it.umich.edu/cran/\")\n})\n\noptions(width=100) # max number of columns when printing vectors/matrices/arrays\n\n.First &lt;- function() cat(\"##### R session begins for\", paste(Sys.getenv(\"USER\"),\".\", sep=\"\"), \"You are currently in\", getwd(), \"#####\\n\\n\")\n.Last &lt;- function()  cat(\"\\n##### R session ends for\", paste(Sys.getenv(\"USER\"),\".\", sep=\"\"),  \"You are currently in\", getwd(), \"#####\\n\\n\")\n</code></pre> <p><code>.Renviron</code> (an example)</p> <pre><code>R_DEFAULT_PACKAGES=\"datasets,utils,grDevices,graphics,stats,methods\"\nR_LIBS_USER=/mnt/home/longnany/Rlibs\n</code></pre> <p>Let's run a short Rscript command to see what we get:</p> <pre><code>$ Rscript -e 'date()'\n</code></pre> <p>Notes:</p> <ul> <li>Personalizing these two files can reduce code portability.</li> <li> <p>If you don't want R or Rscript to read any <code>.Renviron</code> or <code>.Rprofile</code>     files when starting an R session, use option <code>--vanilla</code>.  </p> <p>A caveat:\u00a0if you explicitly export an R environment variable, such as\u00a0 <code>export R_LIBS_USER=~/Rlibs</code>, then adding\u00a0<code>--vanilla</code>\u00a0will not ignore its value. See below the result.</p> <pre><code>$ Rscript --vanilla -e '.libPaths()' # .libPaths() is used to see the directories where R searches for libraries\n[1] \"/opt/software/R/4.0.2-foss-2019b/lib64/R/library\"\n$ export R_LIBS_USER=~/Rlibs\n$ Rscript --vanilla -e '.libPaths()'\n[1] \"/mnt/home/longnany/Rlibs\"\n[2] \"/opt/software/R/4.0.2-foss-2019b/lib64/R/library\"\n</code></pre> </li> </ul>"},{"location":"R_workshop_tutorial/#rscript-one-liner","title":"Rscript one-liner","text":"<p>The general format of <code>Rscript</code>:</p> <pre><code>Rscript [options] [-e expression1 -e expression2 ... | source file] [args]\n</code></pre> <ul> <li> <p><code>options</code>: can be multiple; all beginning with <code>--</code>     (e.g.,\u00a0<code>--vanilla</code> as mentioned above). To learn about all the     options, run <code>Rscript --help</code> on the command line.</p> </li> <li> <p><code>expression1, expression2 ...</code>: can be one or multiple. They are R     commands.</p> </li> <li> <p><code>source file</code>: R source code.</p> </li> <li> <p><code>args</code>: arguments to be passed.</p> </li> </ul> <p>You may have both expressions and source file present in your Rscript line. Here are a few one-liner examples:</p> <p>Rscript one-liner examples</p> <pre><code># Ex1: simple loop\n$ Rscript -e 'for (i in 1:5) print(paste(\"g\", i))'\n\n\n# Ex2: print time\n$ Rscript -e 'date()'\n\n\n# Ex3: quick math (calculating quotient and remainder)\n$ Rscript -e '128 %/% 11' -e '128 %% 11'\n\n\n# Ex4: get help for command \"paste\"\n$ Rscript -e 'help(paste)'\n\n\n# Ex5: used in conjunction with pipe.\n#   Generate three sets of random Normal variables with different means (sd all being one); means are given in file test.dat.\n$ cat &gt; test.dat # ctrl+D to go back when done typing\n1\n10\n20\n$ cat test.dat | Rscript -e 'input_con  &lt;- file(\"stdin\"); open(input_con); while (length(oneLine &lt;- readLines(con = input_con, n = 1, warn = FALSE)) &gt; 0) {print(rnorm(5,mean=as.numeric(oneLine)))};close(input_con)'\n</code></pre>"},{"location":"R_workshop_tutorial/#using-rscript-with-source-code","title":"Using Rscript with source code","text":"<p>a. simple usage:</p> <p>Instead of using <code>'-e your_commands</code>', we now put R commands in a source file and run it with <code>Rscript</code>. Below is an R script file and we can run <code>Rscript multiplots.R</code> to get a PDF file <code>multiplots.pdf</code>. To view it, run <code>evince multiplots.pdf</code>. Or, you can go to our OnDemand File Browser to open the file in your web browser.</p> <p><code>multiplots.R</code>: a very simple example of making 4 plots on the same page</p> <pre><code>pdf(\"multiplots.pdf\")\npar(mfrow=c(2,2))\nfor (i in 1:4) plot(1:10, 1:10, type=\"b\", xlab=bquote(X[.(i)]), ylab=bquote(Y[.(i)]), main=bquote(\"Multiple plots: #\" ~ .(i)))\ndev.off()\n</code></pre> <p>b. passing command line arguments:</p> <p>We can also pass arguments to our R script, as shown in the example below.</p> <p><code>args-1.R</code></p> <pre><code>args &lt;- commandArgs(trailingOnly = TRUE)\n\nnargs &lt;- length(args)\nfor (i in 1:nargs) {\n    cat(\"Arg\", i, \":\", args[i], \"\\n\")\n}\n\n\ncat(\"Generating\",as.numeric(args[nargs-1]), \"normal variables with mean =\", as.numeric(args[nargs]), \"and sd = 1 \\n\")\nrnorm(as.numeric(args[nargs-1]), mean=as.numeric(args[nargs]))\n</code></pre> <p>Running script <code>args-1.R</code>:</p> <pre><code>$ Rscript args-1.R 5 3\nSourcing .Rprofile from the 11092017-R-Workshop directory.\n##### R session begins for longnany. You are currently in /mnt/home/longnany/Documents/11092017-R-Workshop #####\n\nArg 1 : 5\nArg 2 : 3\nGenerating 5 normal variables with mean = 3 and sd = 1\n[1] 2.707162 3.677923 3.192272 2.531973 3.699060\n\n##### R session ends for longnany. You are currently in /mnt/home/longnany/Documents/11092017-R-Workshop #####\n</code></pre> <p>c. processing command line arguments with {<code>getopt</code>}:</p> <p><code>args-2.R</code></p> <pre><code>require(\"getopt\", quietly=TRUE)\n\nspec = matrix(c(\n    \"Number\", \"n\", 1, \"integer\",\n    \"Mean\", \"m\", 1, \"double\"\n), byrow=TRUE, ncol=4) # cf. https://cran.r-project.org/web/packages/getopt/getopt.pdf\n\nopt = getopt(spec);\n\nif (is.null(opt$Number)) {\n    n &lt;- 5\n} else {\n    n &lt;- opt$Number\n}\n\nif (is.null(opt$Mean)) {\n    m &lt;- 3\n} else {\n    m &lt;- opt$Mean\n} \n\ncat(\"Generating\", n, \"normal variables with mean =\", m, \"and sd = 1 \\n\")\nrnorm(n=n, mean=m)\n</code></pre> <p>Running the script <code>args-2.R</code>:</p> <pre><code># Use long flag names\n$ Rscript --vanilla args-2.R --Number 10 --Mean -2\nGenerating 10 normal variables with mean = -2 and sd = 1\n [1] -0.4776278 -1.7759145 -0.9977682 -2.6452126 -3.4050587 -2.2358362\n [7] -1.2696362 -1.6213633 -2.7013074 -1.9271954\n\n# Use short flag names\n$ Rscript --vanilla args-2.R -n 10 -m -2\nGenerating 10 normal variables with mean = -2 and sd = 1\n [1] -2.2241837 -1.6704711  0.1481244  0.2072124 -1.0385386 -1.5194874\n [7] -2.6744478 -2.4683039 -0.7962113 -1.1901021\n\n# No arguments provided so defaults are used\n$ Rscript --vanilla args-2.R\nGenerating 5 normal variables with mean = 3 and sd = 1\n[1] 3.951492 4.255879 4.485044 2.727223 3.039532\n</code></pre>"},{"location":"R_workshop_tutorial/#submitting-parallel-jobs-to-the-cluster-using-doparallel-single-node-multiple-cores","title":"Submitting parallel jobs to the cluster using <code>doParallel</code>: single node, multiple cores","text":"<p>To submit a single-node job, we recommend the package <code>doParallel</code>.  The <code>doParallel</code> package is a \"parallel backend\" for the <code>foreach</code> package, making it possible to execute foreach loops in parallel.</p> <p>Note</p> <p><code>doParallel</code> supports two functionalities: multicore and snow. The most important difference between them is that multicore can only run tasks on a single node (computer) whereas snow can execute tasks on different nodes in a cluster. This leads to different commands for registering the parallel backend. In our example here, we are interested in using the multicore-like parallelism, since we are trying to use the many cores on a single compute node in the cluster. To learn more about their differences, you can refer to this discussion.</p> <p>Example R code <code>R_doParallel_singlenode.R</code>: run 10,000 bootstrap iterations of fitting a logistic regression model</p> <pre><code>library(doParallel)\n\n# Registering a parallel backend, using the \"multicore\" functionality\nregisterDoParallel(cores=as.numeric(Sys.getenv(\"SLURM_CPUS_ON_NODE\")[1]))\n\nx &lt;- iris[which(iris[,5] != \"setosa\"), c(1,5)]\ntrials &lt;- 10000\nptime &lt;- system.time({\n    r &lt;- foreach(icount(trials), .combine=cbind) %dopar% {\n        ind &lt;- sample(100, 100, replace=TRUE)\n        result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n        coefficients(result1)\n    }\n})[3]\n\ncat(\"Time elapsed:\", ptime, \"\\n\")\ncat(\"Currently registered backend:\", getDoParName(), \"\\n\")\ncat(\"Number of workers used:\", getDoParWorkers(), \"\\n\")\nprint(str(r)) # column-binded result\n</code></pre> <p>Now, submit the job to the HPCC through the following SLURM script <code>submit-doParallel.sbatch</code>:</p> <pre><code>#!/bin/bash\n\n# Job name:\n#SBATCH --job-name=doParallel_test\n#\n# Number of nodes needed:\n#SBATCH --nodes=1\n#\n# Tasks per node:\n#SBATCH --ntasks-per-node=1\n#\n# Processors per task:\n#SBATCH --cpus-per-task=4\n#\n# Memory per node:\n#SBATCH --mem=500M\n#\n# Wall time (e.g. \"minutes\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\"):\n#SBATCH --time=3:00:00\n#\n# Standard out and error:\n#SBATCH --output=%x-%j.SLURMout\n\nmodule purge\nmodule load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2\n\nRscript --vanilla R_doParallel_singlenode.R &gt; R_doParallel_singlenode.Rout\n</code></pre> <p>Submission command: <code>sbatch --constraint=\"[intel16|intel18|amd20|amd22]\" submit-doParallel.sbatch</code></p> <p>The output file <code>R_doParallel_singlenode.Rout</code> \u00a0should look like:</p> <pre><code>Time elapsed: 8.946\nCurrently registered backend: doParallelMC\nNumber of workers used: 4\n num [1:2, 1:10000] -14.6 2.26 -11.86 1.91 -7.75 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:2] \"(Intercept)\" \"x[ind, 1]\"\n  ..$ : chr [1:10000] \"result.1\" \"result.2\" \"result.3\" \"result.4\" ...\nNULL\n</code></pre>"},{"location":"R_workshop_tutorial/#submitting-parallel-jobs-to-the-cluster-using-pbdmpi-multiple-nodes","title":"Submitting parallel jobs to the cluster using\u00a0<code>pbdMPI</code>: multiple nodes","text":"<p>MPI stands for Message Passing Interface, and is the standard for managing multi-node communication. The R package <code>Rmpi</code> is an implementation of it for R. Because of its complicated usage, here we will use another R package, <code>pbdMPI</code>,  which greatly simplifies the use of MPI from R.</p> <p><code>pbdMPI</code> is a more recent R MPI package developed under the pbdR project, which simplifies MPI interaction and thus reduces the traffics of node-to-node communication. It works in Single Program/Multiple Data (SPMD) mode, which is an important distinction as compared with <code>Rmpi</code>.</p> <p>As an illustration, we consider the problem of computing the log likelihood of data following a 2-dimensional Multi-Variate Normal (MVN) distribution. The example below applies Cholesky decomposition on the 2-by-2 covariance matrix (code line 25 below), solves a system of linear equations (line 30), followed by some matrix/vector operation (line 31). Line 30 and 31 are where MPI plays a vital role via distributed computing.</p> <p>Below is a graphic illustration of solving a system of linear equations by part so as to be able to distribute the task. To learn more about the underlying mechanism, you can take a look at this note.</p> <p></p> <p><code>MVN.R</code>: MPI in SPMD mode</p> <pre><code># Load pbdMPI and initialize the communicator\nlibrary(pbdMPI, quiet = TRUE)\ninit()\n\n# Check processes\ncomm.cat(\"All processes start...\\n\\n\")\nmsg &lt;- sprintf(\"I am rank %d on host %s of %d processes\\n\", comm.rank(), Sys.info()[\"nodename\"], comm.size())\ncomm.cat(msg, all.rank=TRUE, quiet=TRUE) # quiet=T tells each rank not to \"announce\" itself when it's printing\n\n\nset.seed(1234)\nN &lt;- 100\np &lt;- 2\nX &lt;- matrix(rnorm(N * p), ncol = p)\nmu &lt;- c(0.1, 0.2)\nSigma &lt;- matrix(c(0.9, 0.1, 0.1, 0.9), ncol = p)\n\n# Load data partially by processors\nid.get &lt;- get.jid(N)\nX.spmd &lt;- matrix(X[id.get, ], ncol = p)\ncomm.cat(\"\\nPrint out the matrix on each process/rank:\\n\\n\", quiet=TRUE)\ncomm.print(X.spmd, all.rank=TRUE, quiet=TRUE)\n\n# Cholesky decomposition\nU &lt;- chol(Sigma) # U'U = Sigma\nlogdet &lt;- sum(log(abs(diag(U))))\n\n# Call R's backsolve function for each chunk of the data matrix X (i.e. B.spmd)\nB.spmd &lt;- t(X.spmd) - mu\nA.spmd &lt;- backsolve(U, B.spmd, upper.tri = TRUE, transpose = TRUE) # U'A = B\ndistval.spmd &lt;- colSums(A.spmd * A.spmd)\n\n# Use sum as the reduction operation\nsum.distval &lt;- allreduce(sum(distval.spmd), op = \"sum\")\ntotal.logL &lt;- -0.5 * (N * (p * log(2 * pi) + logdet * 2) + sum.distval)\n\n# Output\ncomm.cat(\"\\nFinal log-likelihood:\\n\\n\", quiet=TRUE)\ncomm.print(total.logL, quiet=TRUE)\n\nfinalize()\n</code></pre> <p>SLURM submission script: <code>submit-pbdMPI.sbatch</code>:</p> <pre><code>#!/bin/bash\n\n# Job name:\n#SBATCH --job-name=pbdMPI_test\n#\n# Number of MPI tasks:\n#SBATCH --ntasks=20\n#\n# Processors per task:\n#SBATCH --cpus-per-task=1\n#\n# Memory:\n#SBATCH --mem-per-cpu=800M\n#\n# Wall clock limit (e.g. \"minutes\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\"):\n#SBATCH --time=30\n#\n# Standard out and error:\n#SBATCH --output=%x-%j.SLURMout\n\necho \"SLURM_NTASKS: $SLURM_NTASKS\"\n\nmodule purge\nmodule load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2\n\n# Suppress warnings about forks and missing CUDA libraries\nexport OMPI_MCA_mpi_warn_on_fork=0\nexport OMPI_MCA_mpi_cuda_support=0\n\nmpirun -n $SLURM_NTASKS Rscript --vanilla MVN.R &gt; MVN.Rout\n</code></pre> <p>Now, submit the job by\u00a0<code>sbatch --constraint=\"[intel16|intel18|amd20|amd22]\" submit-pbdMPI.sbatch</code>. When finished, <code>MVN.Rout</code> should contain the following information:</p> <pre><code>COMM.RANK = 0\nAll processes start...\n\nI am rank 0 on host lac-419 of 20 processes\nI am rank 1 on host lac-420 of 20 processes\n... ...\nI am rank 18 on host lac-421 of 20 processes\nI am rank 19 on host lac-421 of 20 processes\n\nPrint out the matrix on each process/rank:\n\n           [,1]       [,2]\n[1,] -0.1850210 -0.2771503\n[2,] -0.8596753 -0.1081122\n[3,] -1.0927853  0.8690750\n[4,] -0.5831948  0.8032846\n[5,]  1.0796870  0.2354514\n            [,1]       [,2]\n[1,]  0.05274001 -0.8410549\n[2,] -0.92568393  0.9461704\n[3,]  1.01632804  0.6875080\n[4,]  0.34271061  0.4905065\n[5,]  0.84192956 -1.6685933\n... ...\n            [,1]       [,2]\n[1,] -0.55673659 -0.1229023\n[2,] -0.03156318 -0.8501178\n[3,] -1.28832627 -1.3115801\n[4,] -0.47546826 -0.4856559\n[5,]  0.81134183  0.7499220\n            [,1]       [,2]\n[1,] -0.95620195  0.6560605\n[2,]  0.04671396  0.6093924\n[3,]  0.18986742  0.2077641\n[4,]  0.73281327 -1.0452661\n[5,]  2.27968859  1.0611428\n\nFinal log-likelihood:\n\n[1] -283.2159\n</code></pre> <p>You can download more R pbdMPI examples from here.</p>"},{"location":"Rclone_-_rsync_for_cloud_storage/","title":"Rclone - rsync for cloud storage","text":"<p>Rclone can be used to copy files from/to their Microsoft OneDrive or Google Drive cloud storage to/from HPCC disk space. This tool can also be used to mount a user's cloud storage to their HPCC disk so that the storage on cloud could be used as extended disk space.</p> <p>Rclone is installed on HPCC system wide. To use it, users should first\u00a0 load the software module into their environment using command: </p> <pre><code>module load Rclone\u00a0\n</code></pre> <p>For more details of using rclone, users can visit Rclone web site at\u00a0https://rclone.org/.</p> <p>To start using Rclone, users need to run the following command to configure it:</p> <pre><code>rclone config\n</code></pre> <p>The instructions for this command could be found at https://rclone.org/commands/rclone_config/.</p> <p>Specifically, to configure for Google Drive, see\u00a0https://rclone.org/drive/, and to configure for Microsoft Onedrive, see\u00a0https://rclone.org/onedrive/ for instructions. The specific details of how to start using this software on HPCC could be found in the document\u00a0Rclone.pdf</p> <p>After successfully configuring the software, users should be able to use \"rclone\" command to copy or mount the cloud storage to HPCC. There are many rclone sub-commands that can be used to handle file transfers and manage files on HPCC and cloud storage. To get help, use \"rclone --help\" as shown below:</p> <pre><code>[hpc@dev-intel16-k80 ~]$ module load Rclone\n[hpc@dev-intel16-k80 ~]$ rclone --help\n\nRclone syncs files to and from cloud storage providers as well as\nmounting them, listing them in lots of different ways.\n\nSee the home page (https://rclone.org/) for installation, usage,\ndocumentation, changelog and configuration walkthroughs.\n\nUsage:\n  rclone [flags]\n  rclone [command]\n\nAvailable Commands:\n  about           Get quota information from the remote.\n  authorize       Remote authorization.\n  cachestats      Print cache stats for a remote\n  cat             Concatenates any files and sends them to stdout.\n  check           Checks the files in the source and destination match.\n  cleanup         Clean up the remote if possible\n  config          Enter an interactive configuration session.\n  copy            Copy files from source to dest, skipping already copied\n  copyto          Copy files from source to dest, skipping already copied\n  copyurl         Copy url content to dest.\n  cryptcheck      Cryptcheck checks the integrity of a crypted remote.\n  cryptdecode     Cryptdecode returns unencrypted file names.\n  dbhashsum       Produces a Dropbox hash file for all the objects in the path.\n  dedupe          Interactively find duplicate files and delete/rename them.\n  delete          Remove the contents of path.\n  deletefile      Remove a single file from remote.\n  genautocomplete Output completion script for a given shell.\n  gendocs         Output markdown docs for rclone to the directory supplied.\n  hashsum         Produces an hashsum file for all the objects in the path.\n  help            Show help for rclone commands, flags and backends.\n  link            Generate public link to file/folder.\n  listremotes     List all the remotes in the config file.\n  ls              List the objects in the path with size and path.\n  lsd             List all directories/containers/buckets in the path.\n  lsf             List directories and objects in remote:path formatted for parsing\n  lsjson          List directories and objects in the path in JSON format.\n  lsl             List the objects in path with modification time, size and path.\n  md5sum          Produces an md5sum file for all the objects in the path.\n  mkdir           Make the path if it does not already exist.\n  mount           Mount the remote as file system on a mountpoint.\n  move            Move files from source to dest.\n  moveto          Move file or directory from source to dest.\n  ncdu            Explore a remote with a text based user interface.\n  obscure         Obscure password for use in the rclone.conf\n  purge           Remove the path and all of its contents.\n  rc              Run a command against a running rclone.\n  rcat            Copies standard input to file on remote.\n  rcd             Run rclone listening to remote control commands only.\n  rmdir           Remove the path if empty.\n  rmdirs          Remove empty directories under the path.\n  serve           Serve a remote over a protocol.\n  settier         Changes storage class/tier of objects in remote.\n  sha1sum         Produces an sha1sum file for all the objects in the path.\n  size            Prints the total size and number of objects in remote:path.\n  sync            Make source and dest identical, modifying destination only.\n  touch           Create new file or change file modification time.\n  tree            List the contents of the remote in a tree like fashion.\n  version         Show the version number.\n\nUse \"rclone [command] --help\" for more information about a command.\nUse \"rclone help flags\" for to see the global flags.\nUse \"rclone help backends\" for a list of supported services.\n[hpc@dev-intel16-k80 ~]$ \n</code></pre> <p>The tool \"cloudSync\" was developed to help user to synchronize the files between their cloud storages. It is accessible through \"powertools\" which should automatically loaded upon logging into HPCC, but can be manually loaded with 'ml load powertools' if need be. Users are welcome to try it  and report any problems to us via contact form\u00a0here.</p> <p>Following are a few examples of running rclone commands after successfully having configured the cloud storage. Assume that the cloud storage is configured as\u00a0 the name \"MyOneDrive\".\u00a0</p>","tags":["how-to guide","rsync","cloud","rclone"]},{"location":"Rclone_-_rsync_for_cloud_storage/#1-see-current-remote-storage","title":"(1) See current remote storage","text":"<p>We can check the current configuration of rclone using 'rclone config'. As is shown below, we can see that there are currently two remote cloud storage configured:\u00a0  \"MyOneDrive\" and \"googledoc\"\u00a0</p> <pre><code>[user@dev-intel18 ~]$ rclone config\nCurrent remotes:\n\nName                 Type\n====                 ====\nMyOneDrive           onedrive\ngoogledoc            drive\n\ne) Edit existing remote\nn) New remote\nd) Delete remote\nr) Rename remote\nc) Copy remote\ns) Set configuration password\nq) Quit config\ne/n/d/r/c/s/q&gt; q\n\n[user@dev-intel18 ~]$\n</code></pre>","tags":["how-to guide","rsync","cloud","rclone"]},{"location":"Rclone_-_rsync_for_cloud_storage/#2-check-the-remote-storage-information","title":"(2) Check the remote storage information","text":"<p>We can see the remote storage usage and quota using \"rclone about\" command.</p> <pre><code>[user@dev-intel16-k80 ~]$ rclone about MyOneDrive:\nTotal:   5T\nUsed:    450.999M\nFree:    4.998T\nTrashed: 404.576k\n</code></pre>","tags":["how-to guide","rsync","cloud","rclone"]},{"location":"Rclone_-_rsync_for_cloud_storage/#3-list-the-contents-of-the-cloud-storage","title":"(3) List the contents of the cloud storage","text":"<pre><code>[user@dev-intel18 ~]$ rclone lsd MyOneDrive:\n          -1 2018-02-02 08:57:54         0 Attachments\n          -1 2019-08-27 15:43:33         1 IMAGES\n          -1 2019-08-22 15:50:10        42 Matlab\n          -1 2019-02-26 17:12:01        16 Microsoft Teams Chat Files\n          -1 2018-08-24 08:56:32         1 Notebooks\n</code></pre>","tags":["how-to guide","rsync","cloud","rclone"]},{"location":"Rclone_-_rsync_for_cloud_storage/#4-copy-files-on-hpcc-to-remote-cloud","title":"(4) Copy files on HPCC to remote cloud:","text":"<pre><code>[user@dev-intel18 ~]$ rclone copy Project MyOneDrive:Project   # copy the content of directory \"Project\" to remote cloud storage\n\n[user@dev-intel18 ~]$ rclone lsd MyOneDrive:               # view the contents of cloud storage to confirm the copy\n          -1 2018-02-02 08:57:54         0 Attachments\n          -1 2019-08-27 15:43:33         1 IMPACT\n          -1 2019-08-22 15:50:10        42 Matlab\n          -1 2019-02-26 17:12:01        16 Microsoft Teams Chat Files\n          -1 2018-08-24 08:56:32         1 Notebooks\n          -1 2020-04-27 15:43:25         2 Project\n[user@dev-intel18 ~]$ rclone lsd MyOneDrive:Project\n          -1 2020-04-27 15:44:39         1 GPAW\n          -1 2020-04-27 15:43:26         3 MATLAB\n</code></pre>","tags":["how-to guide","rsync","cloud","rclone"]},{"location":"Rclone_-_rsync_for_cloud_storage/#5-copy-files-on-cloud-storage-to-hpcc","title":"(5) Copy files on cloud storage to HPCC:","text":"<pre><code>[user@dev-intel18 Project]$ ls                                 # current content of Project directory before copy\nGPAW  MATLAB\n[user@dev-intel18 Project]$ rclone copy MyOneDrive:IMPACT ./   # copy the content of IMPACT in cloud to current directory         \n[user@dev-intel18 Project]$ ls                                 # confirm that the copy is done\nGPAW  impact_run  MATLAB\n</code></pre> <p>Note</p> <p>Although \"rclone copy\" is similar as unix commands rsync and cp, when using it, users should be aware of the differences and know the details of its behavior.\u00a0</p> <p>(1) \"rclone copy\" does not\u00a0transfer unchanged files, testing by size and modification time or MD5SUM. In this sense, it is similar as linux command rsync;</p> <p>(2) When running \"<code>rclone copy source:sourcepath dest:destpath</code>\", if\u00a0<code>source:sourcepath</code>\u00a0is a directory, <code>dest:destpath</code> should also be a directory. \u00a0It does not copy the directory <code>source:sourcepath</code>, instead, it will copy the content of the directory <code>source:sourcepath</code>\u00a0to the destination <code>dest:destpath</code>. If\u00a0<code>dest:destpath</code>\u00a0does not exist, it will be created and the content of <code>source:sourcepath</code>\u00a0will be stored in it.</p> <p>(3) \"rclone copyto\" is a very similar rclone command to \"rclone copy\". The only difference is that it can\u00a0be used to upload single files to files other than their current name. When running \"<code>rclone copyto source:sourcepath dest:destpath</code>\", if\u00a0<code>source:sourcepath</code>\u00a0is a file, <code>dest:destpath</code> could be a new file name. If\u00a0<code>source:sourcepath</code>\u00a0is a directory, it would be the same as using \"<code>rclone copy\".</code></p>","tags":["how-to guide","rsync","cloud","rclone"]},{"location":"Rclone_-_rsync_for_cloud_storage/#6-checks-the-files-in-the-source-and-destination-match","title":"(6)\u00a0Checks the files in the source and destination match.","text":"<pre><code>[user@dev-intel18 Project]$ rclone check impact_run MyOneDrive:IMPACT/impact_run   # check if it is matched both sides\n2020/04/27 16:19:01 NOTICE: One drive root 'IMPACT/impact_run': 0 differences found\n2020/04/27 16:19:01 NOTICE: One drive root 'IMPACT/impact_run': 21 matching files\n</code></pre> <p>Note</p> <p>For archiving your files to your cloud storage, if the connection between HPCC and your cloud storage is not stable, we would NOT recommend using \"rclone move\" because it may loss the data during the transfer. Instead, we recommend using \"rclone copy\" to successfully copy the files over and run \"rclone check\" to check if files are identical. After that, it is safe to delete local copy of the files.</p> <p>Note</p> <p>When using \"rclone mount\" command to mount your cloud storage to HPCC, there are two things users should be careful:</p> <p>(1) When running rclone mount, the process runs NOT as the user, instead, it runs as a \"root\" of the cloud storage. Therefore, user may see the error message like \"mount helper error: fusermount: failed to open mountpoint for reading: Permission denied\". User could use /tmp space for mount point because that space is accessible for all users. Users should be very careful to open the permission to others for the purpose of using rclone mount.\u00a0</p> <p>(2) The \"rclone mount\" users should unmount it after use using \"fusermount -u \\&lt;endpoint_dir&gt;\". Note that sometimes the endpoint is not unmounted from some nodes due to timeout or some reason, you may see the message like \"Transport endpoint is not connected\" when accessing the endpoint directory on the node. Just manually unmount it again should resolve the issue.\u00a0</p> <p>Note</p> <p>When using \"rclone config\" command to configure your cloud storage on HPCC, the command will\u00a0guide you through an interactive setup process. At the step of auto config, after you chose \"y\", it will start authentication. You will see something like:</p> <pre><code>If your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth\n\nLog in and authorize rclone for access\nWaiting for code...\n</code></pre> <p>At this time, a Firefox browser should be opened. If you did not get the browser window, check if you used -X option to allow X11 forwarding when you run ssh. \u00a0You may follow the instructions at Connect to HPCC System\u00a0to get the display right.</p> <p>It will take a few minutes to get the browser open and connected. Please be patient. If the browser window is open but does not open the authentication page, you could manually input the link provided by the \"rclone config\" command to the firefox browser's url address box to connect to the site. DO NOT use the link on your personal computer's browser. The authentication have to use the browser on HPCC development node. \u00a0</p>","tags":["how-to guide","rsync","cloud","rclone"]},{"location":"Regular_Expressions/","title":"Regular Expressions","text":"<p>A regular expression is a powerful tool to match patterns. With this tool, you can validate text input, search/replace text within a file, batch rename files, test for patterns within strings etc.</p> <p>There are two types of regular expressions: the basic regular expressions (BRE), and the extended regular expressions (ERE). Most utilities (including <code>vi</code>, <code>sed</code>, and <code>grep</code>)\u00a0use the basic regular expression. <code>awk</code> and <code>egrep</code> use the extended expression.</p> <p>There are three parts to a regular expression: anchors, character sets, and modifiers. Anchors\u00a0are used to specify the position of the pattern in relation to a line of text. Character sets match one or more characters in a single position. Modifiers specify how many times the previous character set is repeated.</p>","tags":["reference","command line"]},{"location":"Regular_Expressions/#anchor-characters-and","title":"Anchor characters <code>^</code> and <code>$</code>","text":"<p>The character <code>^</code> is the starting anchor, and the character <code>$</code> is the ending anchor. The regular expression <code>^A</code> will match all lines that start with a capital A. The expression <code>A$</code> will match all lines that end with the capital A.</p> <p>The anchor characters works only if they are located in a proper location. Otherwise, \u00a0they no longer act as anchors. For example, <code>^</code> is only an anchor if it is the first character in a regular expression and <code>$</code> is only an anchor if it is the last character. The expression <code>$1</code> and <code>1^</code> do not have an anchor. If you want to match a <code>^</code> at the beginning of the line, or a <code>$</code> at the end of a line, you must escape the special characters with a backslash.</p> pattern Matches <code>^A</code> A at the beginning of a line <code>A$</code> A at the end of a line <code>A^</code> A^ anywhere on a line <code>$A</code> $A anywhere on a line <code>^^</code> ^ at the begining of a line <code>$$</code> $ at the end of a line","tags":["reference","command line"]},{"location":"Regular_Expressions/#matching-a-character-with-character-sets","title":"Matching a character with character sets","text":"<p>The regular expression <code>the</code> has three characters: <code>t</code>, <code>h</code>, and <code>e</code>. It will match any line with the string \"the\" inside it. However, it will also match the word \"there\" or \"them\". To prevent this, put spaces before and after the pattern as <code>the</code>. You can combine the string with an anchor such as <code>^HPCC</code>.</p>","tags":["reference","command line"]},{"location":"Regular_Expressions/#specifying-a-range-of-characters-with","title":"Specifying a range of characters with <code>[ ]</code>","text":"<p>If you want to match specific characters, you can use the square brackets to identify the exact characters you are searching for. The pattern that will match any line of text that contains exactly one number is <code>^[0123456789]$</code></p> <p>You can use the hyphen between two characters to specify a range. For example <code>^[0-9]$</code> is identical to <code>^[0123456789]$</code>.</p> <p>You can intermix explicit characters with character ranges. This pattern will match a single character that is a letter, number, or underscore: <code>[A-Za-z0-9_]</code>.</p>","tags":["reference","command line"]},{"location":"Regular_Expressions/#specifying-exceptions-in-character-sets-with","title":"Specifying exceptions in character sets with <code>[^ ]</code>","text":"<p><code>[^]</code> matches a single character that is not contained within the brackets.</p> <p>For example, <code>[^abc]</code> matches any character other than \"a\", \"b\", or \"c\". <code>[^a-z]</code> matches any single character that is not a lowercase letter from \"a\" to \"z\". Likewise, literal characters and ranges can be mixed.\u00a0To match all characters except vowels use <code>[^aeiou]</code>.</p> Regular Expression Matches <code>[]</code> The characters \"[]\" <code>[0]</code> The character \"0\" <code>[0-9]</code> Any number <code>[^0-9]</code> Any character other than a number <code>[-0-9]</code> Any number or a \"-\" <code>[0-9-]</code> Any number or a \"-\" <code>[^-0-9]</code> Any character except a number or a \"-\" <code>[]0-9]</code> Any number or a \"]\" <code>[0-9]]</code> Any number followed by a \"]\" <code>[0-9-z]</code> Any number,\u00a0or any character between \"9\" and \"z\". <code>[0-9\\-a]]</code> Any number, a \"-\", a \"a\", or a \"]\"","tags":["reference","command line"]},{"location":"Regular_Expressions/#matching-anything-with-the-wildcard-character","title":"Matching anything with the wildcard character <code>.</code>","text":"<p>A dot <code>.</code> is a special meta-character. It will match any character, except the end-of-line character.</p> <p>For example, the pattern that will match a line with a single characters is <code>^.$</code>, and a line with two characters is <code>^..$</code>.\u00a0You can use <code>...\\.</code> to match three (wildcard) characters, and escape the final wildcard meta-character to match the period instead.\u00a0</p>","tags":["reference","command line"]},{"location":"Regular_Expressions/#repeating-character-sets-with","title":"Repeating character sets with <code>*</code>","text":"<p>The <code>*</code> character matches the preceding element zero or more times.</p> <p>For example, <code>ab*c</code> matches \"ac\", \"abc\", \"abbbc\", etc. <code>[xyz]*</code> matches \"\", \"x\", \"y\", \"z\", \"zx\", \"zyx\", \"xyzzy\", and so on. </p> <p>Using parentheses will create an element that can be repeated with <code>*</code>. For example, <code>(ab)*</code> matches \"\", \"ab\", \"abab\", \"ababab\", and so on.</p>","tags":["reference","command line"]},{"location":"Regular_Expressions/#matching-a-specific-number-of-sets-with-and","title":"Matching a specific number of sets with <code>\\{</code> and <code>\\}</code>","text":"<p>You cannot specify a maximum number of sets with the <code>*</code> modifier. There is a special pattern you can use to specify the minimum and maximum number of repeats. This is done by putting those two numbers between <code>\\{</code> and <code>\\}</code>.</p> <p><code>\\{m, n\\}</code> matches the preceding element at least m and not more than n times. </p> <p>For example, <code>a\\{3,5\\}</code> matches only \"aaa\", \"aaaa\", and \"aaaaa\". Another example is\u00a0<code>[a-z]\\{4,8\\}</code> which\u00a0matches 4, 5, 6, 7 or 8 lower case letters.</p>","tags":["reference","command line"]},{"location":"Regular_Expressions/#more-examples","title":"More examples","text":"Regular expression Matches <code>.og</code> any three-character string ending with \"og\", including \"dog\", \"fog\", and \"hog\". <code>[df]og</code> \"dog\" and \"fog\". <code>[^d]og</code> all strings matched by <code>.og</code> except \"dog\". <code>[^df]og</code> all strings matched by <code>.og</code> other than \"dog\" and \"fog\". <code>^[df]og</code> \"dog\" and \"fog\", but only at the beginning of the string or line. <code>[df]og$</code> \"dog\" and \"fog\", but only at the end of the string or line. <code>\\[.\\]</code> any single character surrounded by \"[\" and \"]\" since the brackets are escaped, for example: \"[a]\" and \"[b]\". <code>b.*</code> b followed by zero or more characters, for example: \"b\" and \"boy\" and \"bowl\".","tags":["reference","command line"]},{"location":"Regular_Expressions/#extended-regular-expressions","title":"Extended regular expressions","text":"<p>The command line utilities <code>egrep</code> and <code>awk</code> use the extended regular expressions. In extended extensions, the backslash before some special characters is no longer required.</p> <p>For example, <code>\\{...\\}</code> becomes <code>{...}</code> and <code>\\(...\\)</code> becomes <code>(...)</code>.</p> <p>Examples:</p> <ul> <li><code>[hc]+at</code> matches with \"hat\", \"cat\", \"hhat\", \"chat\", \"hcat\", \"ccchat\" etc</li> <li><code>[hc]?at</code> matches \"hat\", \"cat\" and \"at\"</li> <li><code>([cC]at)|([dD]og)</code> matches \"cat\", \"Cat\", \"dog\" and \"Dog\"</li> <li>The characters <code>(</code>, <code>)</code>, <code>[</code>, <code>]</code>, <code>.</code>, <code>*</code>, <code>?</code>, <code>+</code>, <code>|</code>, <code>^</code>, and <code>$</code> are   special symbols and have to be escaped with a backslash symbol in order to be   treated as literal characters. For example:</li> <li><code>a\\.(\\(|\\))</code> matches the string \"a.)\" or \"a.(\"</li> </ul> <p>Modern regular expression tools allow a quantifier to be specified as non-greedy (i.e., match the fewest number of times), by putting a question mark after the quantifier. For example, in the string \"[a] [bb]\", <code>\\[.*?\\]</code> will match \"[a]\" since it matches the wildcard the fewest number of times.</p>","tags":["reference","command line"]},{"location":"Regular_Expressions/#comparison","title":"Comparison","text":"BRE ERE Matches <code>\\( \\)</code> <code>( )</code> a marked subexpression. The string matched within the parentheses can be recalled later. <code>\\+</code> <code>+</code> the preceding element one or more times. <code>\\?</code> <code>?</code> the preceding element one or zero times. <code>\\|</code> <code>|</code> the preceding element or the following element. <code>\\{m, n\\}</code> <code>{m, n}</code> the preceding element at least m and not more than n times. <code>\\{m\\}</code> <code>{m}</code> the preceding element exactly m times. <code>\\{m,\\}</code> <code>{m,}</code> the preceding element at least m times. <code>\\{,n\\}</code> <code>{,n}</code> the preceding element not more than n times.","tags":["reference","command line"]},{"location":"Regular_Expressions/#examples","title":"Examples","text":"BRE ERE Matched results <code>\\(ab\\)*</code> <code>(ab)*</code> \"\", \"ab\", \"abab\", \"ababab\" etc. <code>ab\\+c</code> <code>ab+c</code> \"abc\", \"abbbc\", etc, but not \"ac\". <code>[xyz]|+</code> <code>xyz+</code> \"x\", \"y\", \"z\", \"zx\", \"zyx\", \"xyzzy\", etc. <code>\\(ab\\)</code> <code>(ab)+</code> \"ab\", \"abab\", \"ababab\" etc. <code>ab\\?c</code> <code>ab?c</code> \"ac\" or \"abc\". <code>\\(ab\\)\\?</code> <code>(ab)?</code> \"\" or \"ab\". <code>abc\\|def</code> <code>abc|def</code> \"abc\" or \"def\". <code>a\\{3,5\\}</code> <code>a{3,5}</code> \"aaa\", \"aaaa\", and \"aaaaa\". <code>ba\\{,2\\}b</code> <code>ba{,2}b</code> \"bb\", \"bab\", \"baab\".","tags":["reference","command line"]},{"location":"Regular_Expressions/#posix-character-sets","title":"POSIX character sets","text":"<p>POSIX has added newer and more convenient ways to search for character sets. For example, you can use\u00a0<code>[:upper:]</code> instead of\u00a0<code>[A-Z]</code>. In fact, <code>[A-Z]</code> can be different on different systems based on the <code>LC_COLLATE</code> value. For further discussion, check here. On the HPCC at MSU, the default of\u00a0<code>[A-Z]</code> is\u00a0\u00a0a, A, b, B, c, C, ....y, Y, z, Z, which is standard collations (en_US).\u00a0</p> <p>You can use\u00a0<code>[[:upper:]]</code> instead of <code>[:upper:]</code>, and you can mix the old style and POSIX styles, such as\u00a0<code>[1-9[:upper:]]</code>.</p>","tags":["reference","command line"]},{"location":"Regular_Expressions/#listing","title":"Listing","text":"Expression matches <code>[:alnum:]</code> Alphanumeric <code>[:alpha:]</code> Alphabetic <code>[:blank:]</code> Whitespace, tabs, etc <code>[:cntrl:]</code> Control character <code>[:digit:]</code> digit <code>[:graph:]</code> Printable and visible characters <code>[:lower:]</code> Lower case character <code>[:print:]</code> Printable character <code>[:punct:]</code> Punctuation <code>[:space:]</code> Whitespace <code>[:upper:]</code> Upper case character <code>[:xdigit:]</code> Extended digit","tags":["reference","command line"]},{"location":"Requesting_GPUs/","title":"Requesting GPUs","text":"<p>Jobs that will utilize ICER's GPU resources must request these devices through SLURM. This page will cover the nuances of GPU resource requests and how those requests relate to CPU and memory requests.</p>","tags":["reference","GPU"]},{"location":"Requesting_GPUs/#basics-of-gpu-requests","title":"Basics of GPU Requests","text":"<p>GPUs may be requested using the <code>--gpus</code> (or <code>-G</code>) option within a batch script or on the command line when using <code>sbatch</code> or <code>salloc</code>.  At minimum, users must specify the number of GPUs they want to request. Due to the various generations of GPUs available on the HPCC, ICER strongly suggests that users also specify the type of GPU. This is because your software may be compiled for a specific GPU architecture. </p> <p>Software that can run on GPUs will still need a CPU to manage the GPU. Unless you know that your software can take advantage of multiple CPUs while also using one or more GPUs, you should only request one CPU for this purpose. Additionally, be sure your software can utilize multiple GPUs at the same time before requesting more than one GPU.</p> <p>For example, the following requests one V100 GPU that will be managed by a single CPU: <pre><code>#SBATCH --ntasks=1\n#SBATCH --gpus=v100:1\n</code></pre> This GPUs will be hosted on the same node as the single CPU. Notice the colon <code>:</code> that separates the type and number of GPUs. For more on the available types of GPUs, see our page on GPU resources.</p>","tags":["reference","GPU"]},{"location":"Requesting_GPUs/#gpus-in-ondemand","title":"GPUs in OnDemand","text":"<p>GPUs can also be selected for Open OnDemand apps. Look for the \"Advanced Options\" checkbox. Once selected, you should see options for \"Node type\" and \"Number of GPUs\". You must set the node type if you wish to select a particular GPU model; for example, \"amd20\" or \"intel18\" for the V100 GPUs. Reference the GPU resources page to determine what type of node you should request for each type of GPU. SLURM will be unable to schedule your job if you request more GPUs than are present on that type of node.</p>","tags":["reference","GPU"]},{"location":"Requesting_GPUs/#memory-gpus","title":"Memory &amp; GPUs","text":"<p>GPU's have their own memory (sometimes referred to as Video Random Access Memory or VRAM) that is separate from the RAM used by the CPU. The name VRAM follows from the origin of GPUs as dedicated graphical rendering hardware and does not imply any restrictions on the type of data a GPU can process.</p> <p>Each of the GPUs available on the HPCC have varying amounts of VRAM as seen in our hardware resource tables. Requesting a GPU through SLURM will automatically give you access to all of its VRAM. This is in contrast to CPU RAM, which you must explicitly request through SLURM.</p> <p>For example, the SLURM option <code>--mem-per-gpu</code> will request a certain amount of CPU memory per GPU requested. The following settings will request a total of 6GB of CPU RAM, 3GB for each of the two GPUs requested: <pre><code>#SBATCH --ntasks=1       # number of CPUs\n#SBATCH --gpus=v100:2    # number of GPUs\n#SBATCH --mem-per-gpu=3G # memory for CPUs\n</code></pre> Please note that <code>--mem</code> (total memory) and <code>--mem-per-cpu</code> are mutually exclusive with <code>--mem-per-gpu</code>.</p>","tags":["reference","GPU"]},{"location":"Requesting_GPUs/#utilizing-multiple-gpus","title":"Utilizing Multiple GPUs","text":"<p>Some software is able to utilize multiple GPUs. These GPUs may be managed by one or more CPUs depending on how the software was written. Make sure you understand your software's capabilities and limits before requesting multiple GPUs and/or CPUs at the same time; otherwise, these resources will remain idle but still count against your yearly usage limits.</p>","tags":["reference","GPU"]},{"location":"Requesting_GPUs/#one-cpu-for-multiple-gpus","title":"One CPU for Multiple GPUs","text":"<p>Most software that can utilize multiple GPUs will still use a single CPU to manage them. This means that the total number of GPUs your software can utilize is restricted by the number of GPUs on one node. See our table on GPU Resources for a reference.</p> <p>This arrangement can be visualized with the following diagram. The user has requested a single CPU and four GPUs, shown in blue. The single CPU communicates instructions and data with all of the allocated GPUs. Since the hypothetical node in this diagram only has four GPUs, the user can only use a maximum of four GPUs at a time on this type of node.</p> <p></p> <p>Requesting this kind of configuration is straightforward: <pre><code>#SBATCH --ntasks=1\n#SBATCH --gpus=v100:4\n</code></pre></p>","tags":["reference","GPU"]},{"location":"Requesting_GPUs/#multiple-cpus-for-multiple-gpus","title":"Multiple CPUs for Multiple GPUs","text":"<p>It's possible for multiple CPUs to manage a set of GPUs when using multiple processes. The most common way to achieve this is by using MPI. Each MPI process can manage one or more GPUs while also communicating with other processes. With the addition of MPI, the software is no longer restricted by the number of GPUs on a node. </p> <p>One possible configuration for this is represented in the diagram below. In this diagram, each node has one CPU managing two GPUs. The two CPUs are able to communicate with each other via MPI (gray arrows).</p> <p></p> <p>The best way to request this kind of configuration is to use the <code>--gpus-per-node</code> option: <pre><code>#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=1\n#SBATCH --gpus-per-node=v100:2\n</code></pre></p> <p>Another option is to have each MPI process manage only one GPU, as diagrammed below. Each of the four CPUs manages a GPU of the same color. These CPUs are also able to communicate with each other via MPI (gray arrows).</p> <p></p> <p>The optimal way to request this configuration is to use the <code>--gpus-per-task</code> option: <pre><code>#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=2\n#SBATCH --gpus-per-task=v100:1\n</code></pre> This will automatically bind one GPU to each of the four total tasks (CPUs) requested.</p>","tags":["reference","GPU"]},{"location":"Research_Space/","title":"Research Space","text":"<p>A research group's central directory, or research space, is a shared directory established by a MSU\u00a0Principal Investigator (PI) for use by the members of the PI's research group.</p> <p>To create a research space the PI must submit a Research Request form. The initial limit on storage is 50GB and the initial limit on the number of files contained in a research space is 1,000,000 files. A PI may request an increase in storage of up to 1TB of space at no cost by submitting a Quota Increase Request form or an increase beyond 1TB or 1 million files for an annual fee by submitting a Large Quota Increase Request form. Use the <code>quota</code> command to check a research group's current space and file quotas.</p> <pre><code>$ quota\nResearch Groups: Space    Space   Space     Space     Files     Files     Files     Files        \n                 Quota    Used    Available % Used    Quota     Used      Available % Used\n-----------------------------------------------------------------------------------------------\n&lt;groupname&gt;      4096G    3733G   363G      91%       2097152   432525    1664627   21%\n</code></pre> <p>The group's research space is associated with an assigned group name and is located at <code>/mnt/research/&lt;groupname&gt;</code> by default. It is accessible to all users who have been added to the group by the PI. Members of the group must set their user file mode creation mask and file permissions correctly such that the other group members have the appropriate access to the shared files in the research space. See the section Using a research space for more details.  </p> <p>All research space files are periodically, automatically copied to an offsite disaster recovery servers, except those files that a group has opted to store in a specially requested <code>nodr</code> space.\u00a0To request data from this system, please submit a help ticket containing the file paths and the period, i.e. the time frame, from which the files should be restored.  </p>","tags":["explanation","quota","files"]},{"location":"Research_Space/#using-a-research-space","title":"Using a research space","text":"<p>To configure a research space such that all group members have the appropriate access to the files and directories contained within, it is important to read and follow the instructions below.</p> <p>1. Ensure that all directories created in a research space have the group ownership set to the <code>&lt;groupname&gt;</code>  and the set-group-ID (<code>setgid</code>) bit enabled. </p> <p>By default, the group's research space <code>/mnt/research/&lt;groupname&gt;</code> is set with the correct group ownership and <code>setgid</code> bit. This setting makes newly created sub-directories and files within inherit the group ownership of the parent directory rather than the primary group of the individual user. To check the group ownership and <code>setgid</code> bit of a directory within the research space use the <code>ls -ld &lt;path/to/directory&gt;</code> command:</p> <pre><code>$ ls -ld /mnt/research/&lt;groupname&gt;/&lt;subdirectory&gt;\ndrwxrws--- 9 &lt;username&gt; &lt;groupowner&gt; 8192 Jul 22 08:38 /mnt/research/&lt;groupname&gt;/&lt;subdirectory&gt;\n</code></pre> <p>The <code>setgid</code> bit is indicated by the <code>s</code> in place of the group's executable permissions. See the page File Permissions on HPCC for more details. </p> <p>If the access permissions of a sub-directory are not set correctly, the group may encounter a <code>Disk quota exceeded</code> error when creating or copying files. See the section Quotas on a research space for more details. Use the following commands to ensure all directories and files in the group's research space have the proper settings:</p> <pre><code>$ find /mnt/research/&lt;groupname&gt;/ -not -group &lt;groupowner&gt; -print0 | xargs -0 chgrp &lt;groupowner&gt;\n$ find /mnt/research/&lt;groupname&gt;/ -type d -print0 | xargs -0 chmod g+s\n</code></pre> <p>2. Do not use the <code>mv</code> or <code>cp -p</code> commands to transfer files into the group's research space directories. </p> <p>Both <code>mv &lt;filename&gt;</code> and <code>cp -p &lt;filename&gt;</code> may preserve an undesired group ownership attribute even when transferred into a research space directory with ownership and permissions configured correctly. You should use <code>cp &lt;filename&gt;</code> without the <code>-p</code> option.</p> <p>3. Use the <code>rsync --chmod=Dg+s</code> command to transfer files from a local machine to the HPCC research space.</p> <p>For example, use the command</p> <pre><code>$ rsync -avz testdir --chmod=Dg+s &lt;username&gt;@rsync.hpcc.msu.edu:/mnt/research/&lt;groupname&gt;/\n</code></pre> <p>to transfer a directory <code>testdir</code> from your local machine to the HPCC research space  <code>/mnt/research/&lt;groupname&gt;</code>. This will automatically configure the transferred directory with the <code>setgid</code> bit.</p> <p>4. During a bash session set the user file mode creation mask to <code>0007</code> or any lower value. </p> <p>Use the  <code>umask</code> command to set the file mode creation mask, e.g. </p> <pre><code>$ umask 0007\n</code></pre> <p>For the duration of the session, files and directories created by the user are now readable, writable and executable for all members of the research group. Alternatively, a user may run the following Powertools  command one time to add the line\u00a0<code>umask 0002</code> to the\u00a0user's\u00a0<code>.bashrc</code>\u00a0file:</p> <pre><code>$ module load powertools\n$ umask_in_bash                \n</code></pre> <p>5. Ensure that each group member's currently active group ID is set to the research group of interest.</p> <p>Each time a group member logs in to the HPCC that user's group ID is set to the primary group ID assigned by default when a user's account was created. However, a user may be added to other research groups at the request of that group's PI. After a login, the user may then toggle between group memberships using the  <code>newgrp &lt;groupname&gt;</code>  command when needed. For more information, refer to the  Change Primary Group  page.\u00a0A user may request that the primary group ID be changed from the default setting by submitting a request via a  help ticket.</p>","tags":["explanation","quota","files"]},{"location":"Research_Space/#quotas-on-a-research-space","title":"Quotas on a research space","text":"<p>The space and file quotas on an research space are calculated by matching the group ownership settings of files stored on the HPCC to that of the research space group name. Hence, a user may not create files larger than 8 MB in a specified research space with a group ownership attribute different from that of the research space. Attempting to do so will likely result in a  <code>Disk quota exceeded</code>  error even though use of the  <code>quota</code>  command indicates that the reseach space quotas have not been exceeded. To resolve the <code>Disk quota exceeded</code> error in the absence of actual quota violations, users should follow the instructions in the section Using a research space  to ensure that:</p> <ul> <li> <p>The directory into which the files will be transfered has the same group ownership as the research space and the set-group-ID bit</p> </li> <li> <p>If the file already exists, its group ownership has been changed to that of the research space</p> </li> <li> <p>If the file is to be created, the primary group of the user creating the file is set to that of the research space</p> </li> </ul>","tags":["explanation","quota","files"]},{"location":"Running-multiple-jobs-sequentially_34963786.html/","title":"Running multiple jobs sequentially","text":"<p>Create a python script (python_script.py)with the following code.</p> <p>python_script.py </p> <pre><code>print(\"Hello, World!\")\n</code></pre> <p>Create an R script (r_script.R)with the following code.</p> <p>r_script.R </p> <pre><code>z=rnorm(10000,mean=10,sd=2)\nmean(z)\nsd(z)\npdf(file=\"r_histogram.pdf\")\nhist(z,freq=FALSE,nclass=100)\n</code></pre> <p>Make a copy of hello.sb and name it multi_seq.sb</p> <p>Edit multi_seq.sb to run the tasks, python_script.py and r_script.r :</p> <ul> <li>Request resources</li> <li>Load the required modules,\u00a0</li> <li>sequentially run the two scripts</li> </ul> <p>Submit job to compute node</p> <p>Answer </p> <pre><code>#Make a copy of hello.sb and name it multi_seq.sb\ncp hello.sb multi_seq.sb\n\n#Using your favorite editor(nano, vi, emacs, gedit, etc.), \n#make the following edits to the multi_seq.sb script to run the tasks, python_script.py and r_script.r simultaneously:\n\ngedit multi_seq.sb\n\n#Request resources: Typically you'll need to request the largest number of nodes needed for each task. Since each task above only uses one node and one core there is no change to the number of nodes or cores.\n\n#Load the required modules \nmodule load R/3.5.0-X11-20180131\nmodule load python\n\n#sequentially run the two scripts\npython3 python_script.py\nRscript r_script.R\n\n#Submit job to compute node: Execute the following at the command line\nsbatch multi_seq.sb\n</code></pre>"},{"location":"Running_Gaussian_by_Command_Lines/","title":"Running Gaussian by Command Lines","text":"<p>Here we are going to do an geometry optimization calculation on a simple molecule Formamide (HCONH<sub>2</sub>). Users can use the Gaussian input file <code>g16.com</code> :</p> <p>g16.com</p> <pre><code>%NProcShared=4\n%Mem=5GB\n%NoSave\n%chk=g16.chk\n# opt freq b3lyp/cc-pvdz\n\nTitle Card: Single Molecule Formamide\n\n0 1\n C                  3.89594917   -4.10509404   -0.06119675\n O                  5.13960552   -4.14687675    0.12626969\n H                  3.41099598   -3.16562892   -0.22589552\n N                  3.10941607   -5.34695260   -0.05391726\n H                  2.79423275   -5.53644967    0.87600227\n H                  2.31994463   -5.24505745   -0.65918762                                                      \n</code></pre> <p>In the file, the <code>%</code> lines (Link 0 section) specify the system resources. <code>%NprocShared</code> gives how many CPUs to use in a node and <code>%Mem</code> indicates how much memory to use. If any file specified before the <code>%NoSave</code> line, it will not be saved once Gaussian finishes the calculation normally.\u00a0 <code>%chk</code> specify a check point file name to save and <code>#</code> line (Route section) specify the methods of Gaussian calculations. You can give this Gaussian input a title name in Title Card section. After that, use\u00a0Molecule Specification section to assign the coordinates of the atoms with the charge and spin multiplicity of the system in the first line. Please make sure there is at least one empty line in the end of file.</p> <p>By logging into HPCC and ssh to a dev node, users can find Gaussian program installed in HPCC with the module commands:</p> <pre><code>$ module spider Gaussian\n\n----------------------------\n  Gaussian:\n----------------------------\n Versions:\n        Gaussian/g16_AVX\n        Gaussian/g16-AVX2\n        Gaussian/g16\n\n----------------------------\n  For detailed information about a specific \"Gaussian\" module (including how to load the modules) use the module's full name.\n  For example:\n\n     $ module spider Gaussian/g16\n----------------------------\n</code></pre> <p>Please load Gaussian module with g16 version:</p> <pre><code>$ module load Gaussian/g16\n$ which g16\n/opt/software/Gaussian/g16-AVX/g16/g16\n</code></pre> <p>and the Gaussian command \"g16\" can be used. Simply run the command by giving the input and output file names:</p> <pre><code>$ g16 &lt; g16.com &gt; g16.log\n</code></pre> <p>and it starts to calculate the system on the node. It will take about 2 minutes to complete the calculation. After it is finished, you can check the output file <code>g16.log</code>. All dev nodes have 2-hour CPU limit. Please restrict your resource usage on them.</p> <p>Users can also use <code>GaussView</code> to create their molecular system and do calculations. To use GaussView, just run the command <code>gview</code> after loading the Gaussian module.</p>"},{"location":"SLURM_Check_Modify_and_Cancel_a_Job_by_scontrol_scancel_Command/","title":"SLURM Check, Modify and Cancel a Job using the scontrol &amp; scancel commands","text":""},{"location":"SLURM_Check_Modify_and_Cancel_a_Job_by_scontrol_scancel_Command/#scontrol-command","title":"scontrol command","text":"<p>Besides the brief listing of every job using the <code>squeue</code> command, a user can also see the detailed information of each job. Run the SLURM command <code>scontrol show</code> with a job ID:</p> <pre><code>$ scontrol show job 8929\nJobId=8929 JobName=test\n   UserId=nobody(804293) GroupId=helpdesk(2103) MCS_label=N/A\n   Priority=404 Nice=0 Account=classres QOS=normal\n   JobState=PENDING Reason=Resources Dependency=(null)\n   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   RunTime=00:00:00 TimeLimit=00:01:00 TimeMin=N/A\n   SubmitTime=2018-08-01T14:33:04 EligibleTime=2018-08-01T14:33:04\n   StartTime=Unknown EndTime=Unknown Deadline=N/A\n   PreemptTime=None SuspendTime=None SecsPreSuspend=0\n   LastSchedEval=2018-08-03T12:38:48\n   Partition=general-short-14,general-short-16,general-short-18,general-long-14,general-long-16,general-long-18,classres-14,classres-16 AllocNode:Sid=dev-intel18:4996\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=(null)\n   NumNodes=80-80 NumCPUs=160 NumTasks=80 CPUs/Task=2 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=40,mem=80G,node=40,gres/gpu=40\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   MinCPUsNode=2 MinMemoryNode=2G MinTmpDiskNode=0\n   Features=intel14 DelayBoot=00:00:00\n   Gres=(null) Reservation=(null)\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/mnt/home/changc81/GetExample/helloMPI/test\n   WorkDir=/mnt/home/changc81/GetExample/helloMPI\n   Comment=stdout=/mnt/home/changc81/GetExample/helloMPI/slurm-8929.out\n   StdErr=/mnt/home/changc81/GetExample/helloMPI/slurm-8929.out\n   StdIn=/dev/null\n   StdOut=/mnt/home/changc81/GetExample/helloMPI/slurm-8929.out\n   Power=\n</code></pre> <p>You can check if the information is right for the job. If the job has not started to run and you would like change any specification, you can hold the job first using the <code>scontrol hold</code> command:</p> <pre><code>$ scontrol hold 8929\n$ squeue -l -u $USER\nFri Aug  3 12:26:57 2018\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n              8929 general-s     test   nobody  PENDING       0:00      1:00     80 (JobHeldUser)\n</code></pre> <p>where you can see from the results of the <code>squeue</code> command, the job is pending due to the user's hold. You can choose the information you want to change in <code>scontrol show</code> results. Put them in the <code>scontrol update</code> command and modify the information after the <code>=</code> symbol. For example, the command line</p> <pre><code>$ scontrol update job 8929  NumNodes=2-2 NumTasks=2 Features=intel16\n</code></pre> <p>will change the resource request of the job 8929 from 80 nodes and 80 tasks with intel14 nodes to 2 nodes and 2 tasks with intel16 nodes. After the update, you can use the <code>scontrol show</code> command again to verify the job setting. Once you are done with the update work, you can release the job hold by command <code>scontrol release</code>:</p> <pre><code>$ scontrol release 8929\n$ squeue -l -u $USER\nFri Aug  3 13:18:10 2018\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n              8929 general-s     test   nobody  RUNNING       0:07      1:00      2 lac-[386-387]\n</code></pre> <p>The job is now running due to the change of the resource request by the command <code>scontrol update</code>. Again, we can check the running job using the command <code>scontrol show</code>:</p> <pre><code>$ scontrol show job 8929\nJobId=8929 JobName=test\n   UserId=changc81(804793) GroupId=helpdesk(2103) MCS_label=N/A\n   Priority=379 Nice=0 Account=classres QOS=normal\n   JobState=RUNNING Reason=None Dependency=(null)\n   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   RunTime=00:00:08 TimeLimit=00:01:00 TimeMin=N/A\n   SubmitTime=2018-08-01T14:33:04 EligibleTime=2018-08-01T14:33:04\n   StartTime=2018-08-03T13:18:03 EndTime=2018-08-03T13:18:11 Deadline=N/A\n   PreemptTime=None SuspendTime=None SecsPreSuspend=0\n   LastSchedEval=2018-08-03T13:18:03\n   Partition=general-long-16 AllocNode:Sid=dev-intel18:4996\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=lac-[386-387]\n   BatchHost=lac-386\n   NumNodes=2 NumCPUs=4 NumTasks=2 CPUs/Task=2 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=4,mem=4G,node=2,billing=4\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   MinCPUsNode=2 MinMemoryNode=2G MinTmpDiskNode=0\n   Features=intel16 DelayBoot=00:00:00\n   Gres=(null) Reservation=(null)\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/mnt/home/changc81/GetExample/helloMPI/test\n   WorkDir=/mnt/home/changc81/GetExample/helloMPI\n   Comment=stdout=/mnt/home/changc81/GetExample/helloMPI/slurm-8929.out\n   StdErr=/mnt/home/changc81/GetExample/helloMPI/slurm-8929.out\n   StdIn=/dev/null\n   StdOut=/mnt/home/changc81/GetExample/helloMPI/slurm-8929.out\n   Power=\n</code></pre> <p>For complete usage information about the <code>scontrol</code> command, please refer to https://slurm.schedmd.com/scontrol.html at the SLURM web site.</p>"},{"location":"SLURM_Check_Modify_and_Cancel_a_Job_by_scontrol_scancel_Command/#scancel-command","title":"scancel command","text":"<p>If at any moment before the job complete, you would like to remove the job, you can use the <code>scancel</code> command to cancel a job. For example, the command</p> <pre><code>$ scancel 8929\n</code></pre> <p>will cancel job 8929. For a complete usage information about the <code>scancel</code> command, please refer to https://slurm.schedmd.com/scancel.html at the SLURM web site.</p>"},{"location":"SLURM_Queueing_and_Partitions/","title":"SLURM Queueing and Partitions","text":"","tags":["reference","buyin","partitions"]},{"location":"SLURM_Queueing_and_Partitions/#partitions","title":"Partitions","text":"<p>SLURM partitions are separate queues for submitted jobs. Each partition has a different set of constraints that controls which jobs should be in that queue.</p> <p>In most cases, you will not need to specify a partition when submitting a job. Given a specified (or default) account, SLURM's job submit plugin control will automatically send your job to the correct partition. For information regarding changing your account when submitting and ensuring access to buy-in nodes, please see the page on Buy-In and Account Management</p> <p>The jobs in each partition are evaluated by the scheduler to determine the order in which they are run. See How jobs are scheduled for more information.</p>","tags":["reference","buyin","partitions"]},{"location":"SLURM_Queueing_and_Partitions/#types-of-partitions","title":"Types of Partitions","text":"Name Purpose general-short This partition includes all nodes--buy-in and non-buy-in--and runs jobs that request a wall time of four hours or less. Jobs in this partition are considered for scheduling after jobs in buy-in partitions and the general-long partition. To prevent these jobs from being continuously bumped by general-long/buy-in jobs, they are also submitted to general-long. general-long This partition includes non-buy-in nodes and allows jobs to run for up to seven days. Jobs with a requested wall time of four hours or less are also submitted to the general-short partition. general-long-bigmem This partition includes non-buy-in nodes with more memory and CPU cores than most nodes and allows jobs to run for up to seven days. Jobs requesting more than 256GB or 40 CPUs per node are automatically submitted to this partition. This partition ensures large jobs get priority access to large nodes over jobs that can run elsewhere.\u00a0Jobs with a requested wall time of four hours or less are also submitted to the general-short partition. general-long-gpu This partition contains non-buy-in nodes with GPUs and allows jobs to run for up to seven days. Jobs requesting GPUs are automatically submitted to the partition. This partition ensures jobs requesting GPUs get priority access to nodes with GPUs over jobs not requesting GPUs. Jobs with a requested wall time of four hours or less are also submitted to the general-short partition. Buyin Partition (names vary) A partition is created for each buy-in account. Each buy-in partition includes all non-buy-in nodes, allowing buy-in jobs to span buy-in and non-buy-in nodes. These jobs get equal consideration for scheduling on non-buy-in nodes as jobs in general-long. When jobs submitted to these partitions request a wall time of four hours or less, they are also submitted to the general-short, enabling them to use other available buy-in nodes and ensuring they are scheduled as fast as possible.","tags":["reference","buyin","partitions"]},{"location":"SLURM_Queueing_and_Partitions/#how-jobs-are-assigned-to-queues","title":"How jobs are assigned to queues","text":"<p>Depending on whether you are using the <code>general</code> account or a buy-in specific account, your job will be submitted to each of the following queues depending on the wall time, memory, and GPUs requested.</p> Default Account Wall Time&lt;=4 Hours Wall Time&lt;=4 Hours and\u00a0&gt;=256GB or 40CPU Wall Time&lt;=4 Hours and\u00a0GPUs Reqested Wall Time&gt;4 Hours\u00a0 Wall Time&gt;4 Hours and\u00a0&gt;=256GB or 40CPU Wall Time&gt;4 Hours and\u00a0GPUs general general-long, general-short general-long-bigmem, general-short general-long-gpu, general-short general-long general-long-bigmem general-long-gpu <code>&lt;buyin&gt;</code> <code>&lt;buyin&gt;</code>, general-long, general-short <code>&lt;buyin&gt;</code>, general-long-bigmem, general-short <code>&lt;buyin&gt;</code>, general-long-gpu, general-short <code>&lt;buyin&gt;</code>, general-long <code>&lt;buyin&gt;</code>, general-long-bigmem <code>&lt;buyin&gt;</code>, general-long-gpu <p>where <code>&lt;buyin&gt;</code> is the name of buyin account.</p>","tags":["reference","buyin","partitions"]},{"location":"SLURM_commands/","title":"SLURM Commands","text":"<p>SLURM uses command-line commands to control jobs and clusters as well as show detailed information about jobs. The table below presents the most frequently used commands on HPCC. A complete list can be found at the SLURM documentation page. Please also see this SLURM cheatsheet.</p> Command Example Usage Description srun <code>srun my_program --arg foo</code> Run parallel jobs; often used within job scripts. salloc <code>salloc -c 2 --time=1:00:00</code> Request an interactive job on a compute node. sbatch <code>sbatch my_job.sb</code> Used to submit batch jobs to the SLURM scheduler. squeue <code>squeue -u user123</code> View information about jobs - pending or running - in the queue. scancel <code>scancel 123456789</code> Used to cancel jobs or job steps that are under the control of SLURM. sacct <code>sacct -o \"JobID,AllocCPUS,State\"</code> Display accounting data for all jobs (and job steps) - running, pending, or ended - stored in the SLURM database. sprio <code>sprio -j 123456789</code> View the factors considered for a job's scheduling priority. sinfo <code>sinfo -p general-long-gpu</code> View status information SLURM nodes and partitions.","tags":["reference","slurm"]},{"location":"SLURM_overview/","title":"What is SLURM?","text":"<p>Most of the HPCC's resources are in the form of compute nodes. These compute nodes are accessed by running batch jobs.</p> <p>The HPCC uses SLURM (Simple Linux Utility for Resource Management) to manage compute node resources.  SLURM is an open-source, fault-tolerant, and highly scalable scheduling system. It has been employed by a large number of national and international computing centers. </p> <p>Users can submit batch jobs to the SLURM scheduler from the command line. Job requests include a specification of desired resources as well as the commands necessary for your program(s) to run. These job requests can either be in a job script or entered on the command line.</p>","tags":["explanation","slurm"]},{"location":"SLURM_resource_request_guide/","title":"SLURM resource request guide","text":"<p>This guide will help you identify ways you can improve your SLURM job submission  scripts to request the appropriate resources for your jobs. This will help your jobs queue efficiently.</p> <p>Note</p> <p>Walltime is the time taken measured by a \"clock on the wall\", i.e. it is the time taken for your code to run, and the time you will request in a SLURM script. CPU time is the amount of time used per CPU, so if 5 CPUs are used for the entire walltime, the CPU time will be 5 times the walltime.</p>","tags":["how-to guide","slurm","queue"]},{"location":"SLURM_resource_request_guide/#measure-your-resource-requirements","title":"Measure your resource requirements","text":"<p>The first step to obtain the correct resource requirements is to understand your  code's resource usage. There are many ways to do this, so this guide will not be exhaustive. </p>","tags":["how-to guide","slurm","queue"]},{"location":"SLURM_resource_request_guide/#code-documentation","title":"Code documentation","text":"<p>If your code has documentation, this should be the first place to go. The code documentation or descriptive journal article may have guidelines for resource  requirements such as memory usage, walltime, and number of CPU cores. If the  code is currently maintained or you have a support agreement with the software  producer, you can contact the developers to ask them about their experience with HPCC resource requirements.</p>","tags":["how-to guide","slurm","queue"]},{"location":"SLURM_resource_request_guide/#local-testing","title":"Local testing","text":"<p>If your code can be run on a local computer such as your laptop, you can easily  estimate the required walltime by timing a run of the code. You may also be able  to estimate the required CPU cores and memory using your computer's resource  monitor (Task Manager for Windows, Activity Monitor on Mac). For timing on Linux systems,  you can use the <code>time</code> command. This is run as <code>time &lt;your process name&gt;</code> and will return 3 time meaurements: <code>real</code>, <code>user</code> and <code>sys</code>. <code>real</code> is the equivalent of walltime, while <code>user</code> and <code>sys</code> are CPU time measurements. See this Wikipedia article for more information: https://en.wikipedia.org/wiki/Time_%28Unix%29.</p>","tags":["how-to guide","slurm","queue"]},{"location":"SLURM_resource_request_guide/#development-node-testing","title":"Development node testing","text":"<p>Note</p> <p>Development nodes have a maximum CPU time of 2 hours for processes. They are also used by other users which will affect CPU and GPU usage estimates.</p> <p>Our development nodes are a potentially useful place to investigate your code's  resource requirements for short jobs (&lt; 2 CPU hours). Note that each additional  CPU you use reduces your total allowed process time. Testing is best done when the dev node reports low usage.</p> <p>You can use the Linux tool <code>top</code> to measure memory and CPU usage. Some  development nodes (<code>dev-intel16-k80</code> and <code>dev-amd20-v100</code>) have  access to GPUs to help you determine GPU resource requirements.  As mentioned in Local testing you can use the <code>time</code> command  to measure walltime and CPU time, though this may be inaccurate in some cases.</p>","tags":["how-to guide","slurm","queue"]},{"location":"SLURM_resource_request_guide/#basic-slurm-run","title":"Basic SLURM run","text":"<p>Ideally you will have been able to estimate your resource requirements using  documentation or a local computer before this step. Then you  can use these estimates for your SLURM run. If not, you will need to use a  permissive resource request with a large amount of memory and walltime so that  you can measure your code's needs. You should expect the queue time for this test job to be long. For faster queuing of test jobs, request walltimes of less than 4 hours.</p> <p>After the run completes (or all the walltime is used), you can determine the  approximate resource requirements of your code by inspecting the amount of time taken. For more in-depth statistics, see  <code>seff</code> and <code>reportseff</code> below.</p> <p>For jobs that you expect to take longer than 4 hours, you will need to  understand your code's scaling. Scaling is how your code changes its run  time as more CPUs or GPUs are used to run the code. To measure scaling, you can run your code a few times with increasing resource requests each time and  measure how long it takes. Then you can fit a simple linear or exponential  function to these points and approximate intermediate requests, or extrapolate  larger resource requests. </p>","tags":["how-to guide","slurm","queue"]},{"location":"SLURM_resource_request_guide/#seff-and-reportseff","title":"<code>seff</code> and <code>reportseff</code>","text":"<p><code>seff</code> and <code>reportseff</code> are useful tools for investigating your resource request efficiency. They will provide statistics for individual jobs (<code>seff &lt;job id&gt;</code>)  or a report of multiple jobs (<code>reportseff -u &lt;user name&gt;</code>). <code>seff</code> statistics  list the used and requested resources as well as a percentage efficiency.  <code>reportseff</code> statistics include:</p> <ul> <li>the time efficiency of the job (TimeEff), which is the percentage use of the requested walltime;</li> <li>the CPU efficiency of the job (CPUEff), which is the percentage use of the  requested CPU cores;</li> <li>the memory efficiency of the job (MemEff), which is the percentage use of the requested memory.</li> </ul> <p>You can use these tools to get a quick measurement of your resource request usage.</p> <p>Note</p> <p><code>reportseff</code> can sometimes generate malformed output if the number of lines overflow the terminal window, in such cases pipe the ouput of 'reportseff' through more: <code>reportseff &lt;options&gt; | more</code>.</p> <p>The <code>reportseff</code> developers can be reached at  https://github.com/troycomi/reportseff.</p>","tags":["how-to guide","slurm","queue"]},{"location":"SSH_Key-Based_Authentication/","title":"SSH key-based authentication","text":"<p>While the most common way of login to the HPCC is by using the username/password pair, a more secure authentication method is the use of SSH keys. Although setting up your keys is a little more complex, it is a one-time investment. The HPCC provides key-based authentication as an option, in addition to the usual password-based login. </p> <p>Note</p> <p>Starting in October 2022, login to our rsync gateway (<code>rsync.hpcc.msu.edu</code>)  will accept SSH keys as the ONLY authentication method. Username/password won't work.</p>","tags":["how-to guide","ssh"]},{"location":"SSH_Key-Based_Authentication/#what-are-ssh-keypairs","title":"What are SSH keypairs?","text":"<p>An SSH keypair consists of a private key and a public key. Your private key is a secret key just like your password which you should not share with anyone. On the other hand, your public key can be made publicly available in the same way that your name can be made public. The public key is stored on the server you attempt to log into (that is, the HPCC), while the private key is stored on your own computer. When a user attempts to log in, an encryption process starts on the HPCC side, using the public key. With your private key, your computer will be able to decrypt the encrypted message sent from the HPCC. When everything matches up, your login is approved.</p>","tags":["how-to guide","ssh"]},{"location":"SSH_Key-Based_Authentication/#via-command-line-tools-linux-macos-mobaxterm-terminal-on-windows","title":"Via command line tools (Linux, MacOS, MobaXterm terminal on Windows)","text":"<p>SSH tool suites usually provide a utility for generating these keypairs. On a Mac and Linux, you can run the command <code>ssh-keygen</code> using the built in terminal Terminal. </p> <p>On a Windows computer, you will first need to download MobaXterm to generate SSH key pairs. See the Intro to MobaXterm for more on getting started. Once you have installed MobaXerm, run the program and choose the \"Start local terminal\" option.</p> <p></p>","tags":["how-to guide","ssh"]},{"location":"SSH_Key-Based_Authentication/#generating-ssh-keypairs-on-command-line","title":"Generating SSH keypairs on command line","text":"<p>To generate a keypair from command line (e.g., after opening a terminal on Mac, Linux or through MobaXterm on Windows), run</p> <p><code>ssh-keygen -t rsa -v 4096</code></p> <p>You will be given an option for protecting your private key with a passphrase. Please do this, as it will prevent your private key from being used by a malicious individual if it is ever stolen.</p> <p>After you have set a passphrase and the keys has generated, you will find the key's files in the <code>.ssh</code> directory under your home directory. By default, <code>id_rsa</code> is the private key file and <code>id_rsa.pub</code> the public key file.</p>","tags":["how-to guide","ssh"]},{"location":"SSH_Key-Based_Authentication/#uploading-your-public-key-to-the-hpcc","title":"Uploading your public key to the HPCC","text":"<p>In order to login to HPCC with key-based authentication from your local computer, you will need to add your public key to the <code>~/.ssh/authorized_keys</code> file.  The following steps will guide you through this process:</p> <ol> <li> <p>Log on to HPCC gateway <code>gateway.hpcc.msu.edu</code> or start an interactive ondemand session to edit within a linux GUI environment</p> </li> <li> <p>On the HPCC, make sure you have a directory named <code>.ssh</code> under your home directory. If not, create one by running <code>mkdir ~/.ssh</code> in the terminal</p> </li> <li> <p>Upload your public key <code>id_rsa.pub</code> from your computer to <code>gateway.hpcc.msu.edu</code>. There are multiple ways to do so, as given here.</p> </li> <li> <p>Append the public key file to another file <code>~/.ssh/authorized_keys</code>. In order to do so, assuming that the pub key file has been copied to your home directory from Step  3, you can run the following command</p> <p><code>cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</code></p> </li> <li> <p>Set correct permissions by running </p> <p><code>chmod 700 ~/.ssh</code> </p> <p><code>chmod 600 ~/.ssh/authorized_keys</code></p> </li> </ol> <p>Warning</p> <p>Only copy key files with the <code>.pub</code> extension. Key files without this extension are private keys, which should never be shared!</p>","tags":["how-to guide","ssh"]},{"location":"SSH_Key-Based_Authentication/#via-gui-tools-in-mobaxterm","title":"Via GUI Tools in MobaXTerm","text":"<p>We recommend Windows users use MobaXTerm to generate and manage their key pairs. If you are Windows user who is not comfortable with the command line, the following instructions will guide you through using Graphical User Interface (GUI) tools built into MobaXterm which should work similar to other Windows applicaitons. Please follows these instructions carefully as each step is important to ensure your keys are created using the proper format for use with HPCC.</p>","tags":["how-to guide","ssh"]},{"location":"SSH_Key-Based_Authentication/#generating-ssh-keypairs-using-mobakeygen","title":"Generating SSH keypairs using MobaKeyGen","text":"<p>In MobaXTerm, click Tools -&gt; MobaKeyGen (SSH Key Generator) in the menu bar.  You will see the following window:</p> <p></p> <p>Click Generate and follow the commands on the screen. The Parameters should be set to RSA, Number of bits 4096. The <code>Key comment</code> field can be used to enter a description of the key e.g. \"My home PC\". As above, we recommend adding a key passphrase.</p> <p></p> <p>Save the private key to your computer by going to the Conversions menu at the top of the screen, and selecting \"Export OpenSSH key\". Save the resulting file in <code>C:\\Users\\&lt;Account_Name&gt;\\.ssh\\id_rsa</code> where <code>&lt;Account_Name&gt;</code> is your Windows account name. Create the <code>.ssh</code> directory if it doesn't already exist.</p>","tags":["how-to guide","ssh"]},{"location":"SSH_Key-Based_Authentication/#uploading-your-public-key-to-the-hpcc_1","title":"Uploading your public key to the HPCC:","text":"<ol> <li>Copy the Public key from the MobaKeyGen window. Make sure you copy all of the characters. </li> </ol> <p>Note</p> <p>If you use \"Save public key\", the file will not be in the correct format. </p> <p></p> <ol> <li> <p>Log on to the HPCC gateway <code>gateway.hpcc.msu.edu</code> or start an interactive Open OnDemand session</p> </li> <li> <p>On the HPCC, make sure you have a directory named <code>.ssh</code> under your home directory. If not, create one by running <code>mkdir ~/.ssh</code> on the HPCC or using the GUI file browser in the applications menu of an interactive OnDemand session.</p> </li> <li> <p>Using an editor on the HPCC, such as <code>nano</code> or the editors in the OnDemand GUI session, open or create the <code>~/.ssh/authorized_keys</code> file and paste the copied public key. Save the file.</p> </li> <li> <p>Set correct permissions by running these commands in the HPCC terminal or using the GUI file browser of an OnDemand session</p> <p><code>chmod 700 ~/.ssh</code> </p> <p><code>chmod 600 ~/.ssh/authorized_keys</code></p> </li> </ol>","tags":["how-to guide","ssh"]},{"location":"SSH_Key-Based_Authentication/#adding-your-private-key-to-mobaxterms-ssh-connection","title":"Adding your private key to MobaXterm's SSH connection","text":"<p>In the MobaXTerm window, in the User sessions pane, right click the link you use to connect to the hpcc (usually something like <code>hpcc.msu.edu (&lt;netid&gt;)</code>), and select \"Edit session\". In the \"Advanced SSH settings\" pane, click \"Use private key\", and add the path to the key file you saved in <code>C:\\Users\\&lt;Account_Name&gt;\\.ssh\\id_rsa</code>. Click \"OK\" to save.</p> <p>When you use this session in the future, you should no longer be prompted for a password.</p> <p></p> <p>Note</p> <p>If you use MobaXTerm for access, make sure you keep MobaXTerm up to date.  We recommend using MobaXTerm to generate keys for use with MobaXTerm on Windows.</p> <p>Note</p> <p>If you are having trouble connecting with an SFTP session in MobaXterm to rsync.hpcc.msu.edu, make sure you  check the \"Use private key\" box and point towards the private key (id_rsa) file which should be in either: - C:\\Users\\\\AppData\\Roaming\\MobaXterm\\home.ssh if you used command line or - C:\\Users\\.ssh if you use the the GUI tool <p></p>","tags":["how-to guide","ssh"]},{"location":"Scavenger_Queue/","title":"Scavenger Queue","text":"<p>The scavenger queue allows users to run preemptible jobs on idle cores. Jobs in the scavenger queue may be interrupted if resources are required for other non-scavenger jobs.</p> <p>With few exceptions, each researcher using the HPCC is limited in the number of jobs or cores they can run at one time. Annually, non-buyin users are limited in the total number of CPU and GPU hours they can use. These limits do not apply to jobs submitted to the scavenger queue.</p> <p>Jobs in the scavenger queue can start on resources that would otherwise be left idle, improving research throughput. Similar to jobs submitted to the general-long queue, these jobs can request up to a 7-day wall time. The default behavior for interrupted jobs is to be re-queued, but users can opt for cancellation if it is more conducive to their workflow.</p> <p>Note</p> <p>We recommend that only users who can checkpoint and restart or have a workflow implemented that can manage jobs being canceled or requeued use this queue.</p>","tags":["how-to guide","slurm","partitions"]},{"location":"Scavenger_Queue/#usage","title":"Usage","text":"<p>To use the scavenger queue, add the following line to your job script: <pre><code>#SBATCH --qos=scavenger\n</code></pre></p> <p>To prevent your job from requeuing automatically if interrupted, add the following line to your job script: <pre><code>#SBATCH --no-requeue\n</code></pre></p> <p>The scavenger queue is not affected by the amount of wall time requested in your job script, e.g. 24 hours wall time is treated with the same scavenger queue priority as 4 hours wall time.</p> <p>Scavenger queue jobs will be automatically assigned to the <code>scavenger</code> account, regardless of the <code>-A</code> setting in your job script.</p>","tags":["how-to guide","slurm","partitions"]},{"location":"Scavenger_Queue/#scheduling","title":"Scheduling","text":"<p>The scavenger queue runs using the backfill scheduler (see How Jobs are Scheduled). Job scheduling may take on the order of minutes to occur, depending on the current load on the HPCC. Scavenger queue jobs run for a minimum of 1 minute before they can be preempted, but typically scavenger queue jobs run for approximately 1 hour before preemption.</p>","tags":["how-to guide","slurm","partitions"]},{"location":"Science_DMZ/","title":"High Speed Research Network","text":"<p>MSU's High Speed Research Network (formerly the Science Demilitarized\u00a0Zone) is a portion of the network designed to optimize high-performance for research applications. This Research Network enables researchers to disseminate terabytes or even petabytes of specialized data more easily\u00a0and\u00a0at speeds of 10 to 100 gigabits per second to other institutions and cloud providers. This ability to share data immeasurably increases its value, as the insights extrapolated from it by additional researchers have the potential to change society in significant and meaningful ways.\u00a0</p> <p>The\u00a0High Speed Research Network offers increased network speeds and reliability, broadly enhancing MSU\u2019s research and education cyberinfrastructure. All campus network users will benefit from the high-speed network connections that will be used for sharing data already stored on ICER's HPCC and on the NSF-funded OSIRIS storage infrastructure.\u00a0</p> <p>The High Speed Research Network also eliminates obstacles for better access to valuable data. By sharing resources and working together, researchers are better positioned to collaboratively find solutions to our biggest problems. This project also lays the foundation for a new relationship between MSU IT and the Office of Research and Innovation, strengthening collaboration and strategic planning as MSU develops cyberinfrastructure capabilities to enhance scientific research support.\u00a0\u00a0</p>"},{"location":"Science_DMZ/#using-the-high-speed-research-network","title":"Using the High Speed Research Network","text":"<p>Globus is the recommended method to transfer big data files as it automatically leverages the High Speed Research Network. For a general overview of Globus and information on setting up a Globus account, see Transferring data with Globus For walk-through training on using Globus, please self-enroll in ICER's DMZ Globus Training D2L course.</p>"},{"location":"Scratch_File_Systems/","title":"Scratch Space","text":"<p>Each user is provided with a working directory know as scratch space. This space is intended for intensive input/output (I/O) operations i.e., heavy reading and writing of data, involving very large files and/or a very large number of small files. Research groups may also request a scratch space. Unlike the home space and research\u00a0space, the scratch space is not intended for long-term storage and cannot be accessed from a gateway node, with the exception of the rsync gateway used for file transfer. </p> <p>Data stored in a user's scratch space is not backed-up, and files in a scratch space with no I/O operations for 45 days will be automatically deleted to ensure the space is available to all users.  The limit on storage is 50TB and the initial limit on the number of files contained in a scratch space is 1,000,000 files. </p> <p>A user's scratch space is available at <code>/mnt/scratch/$USER</code>, or use the bash environmental variable <code>$SCRATCH</code>. Use the <code>quota</code> command to check a user's current space and file quotas.</p> <pre><code>$ quota\n\nTemporary Filesystems:\n---------------------------------------------------------------------------------------------------------------------------------------\n\n/mnt/scratch (/mnt/gs21)        Space Quota  Space Used   Space Free   Space % Used Filess Quota Files Used   Files Free   Files % Used\n                                51200G       0G           51200G       0%           1048576      1            1048575      0%       \n</code></pre>","tags":["explanation","quota","files"]},{"location":"Scratch_File_Systems/#using-a-scratch-space","title":"Using a Scratch Space","text":"<p>Scratch Space can sustain high data transfer rates and is a good  choice for data files used in running parallel on multiple nodes with intensive I/O requirements. Jobs of this type will run much faster with  data accessed from a scratch space and users should follow the procedure  below for best practice:    </p> <ol> <li>Configure the job script and/or the main program for scratch space I/O using the path <code>/mnt/scratch/$USER</code> or the variable <code>$SCRATCH</code></li> <li>Copy input data from the home space or research space to scratch space; to maintain data integrity keep the original data files in the home space or research space**</li> <li>Schedule the job and confirm successful completion of the I/O operations</li> <li>Move the resulting output data back to either the home space or research space</li> <li>Delete that data from the scratch space</li> </ol>","tags":["explanation","quota","files"]},{"location":"Scratch_File_Systems/#time-limits-on-scratch-space","title":"Time Limits on Scratch Space","text":"<p>Files in a scratch space with no I/O operations for 45 days will be automatically deleted to ensure the space is available to all users. To find files in a scratch space approaching the 45 day limit run the following command:</p> <p><code>find $SCRATCH -type f -ctime +40</code></p> <p>Here the <code>+40</code> argument specifies files with no I/O for more than 40 days. Users may set this argument to any number of days desired, up to the 45 day limit. This time is measured based on the \"change time\" of the file, so files extracted from archives will not be deleted until the 45 day limit is reached, after they have been extracted.</p>","tags":["explanation","quota","files"]},{"location":"Scratch_File_Systems/#multiple-scratch-spaces","title":"Multiple Scratch Spaces","text":"<p>You may have multiple directories in scratch, such as one associated with your home directory (/mnt/scratch/$USER) and one or more associatedithed with research spaces (/mnt/scratch/). Research scratch directory, like regular research directories are shared with other memebers of your reserearch group and intended to facilitate collaboration with groups. However, like the scratch space under your username, research scratch spaces is not backed-up, and files will be automatically deleted after 45 days. <p>Importantly, the limits on your scratch space useage applies to all scratch spaces. In other words, you have a maximum of 50TB and  1,000,000 files in scratch, regardless of which scratch directory those files are in.</p>","tags":["explanation","quota","files"]},{"location":"Sensitive_Data_on_the_HPCC/","title":"Sensitive Data on the HPCC","text":"<p>Data that are subject to external security requirements are considered sensitive data. These requirements may be imposed by regulations or contractual obligations. In either case, users that intend to store sensitive data on HPCC systems must work with the HPCC to ensure data security.  </p> <p>Examples of sensitive data include:</p> <ul> <li>Personally Identifiable Information (PII)</li> <li>Protected Health Information (PHI)</li> <li>Controlled Unclassified Information (CUI)</li> <li>Data covered under the Heath Insurance Portability and     Accountability Act (HIPAA)</li> <li>Data covered under the Federal Information Security Management Act     (FISMA)</li> <li>Data covered under the Federal Education Rights and Privacy Act     (FERPA)</li> <li>Data with security requirements set by the MSU Institutional Review     Board (IRB)</li> <li>Controlled access data from the NIH Database of Genotypes and     Phenotypes (dbGaP)</li> </ul> <p>Information about the HPCC's Sensitive Data Policy is available to users with MSU credentials.</p> <p>MSU users may also visit https://data-storage-finder.tech.msu.edu/ for more information about data storage choices on campus (NOTE: This page is only accessible from campus or using Campus VPN).</p>","tags":["reference","files","sensitive data"]},{"location":"Show_Job_Steps_by_sacct_and_srun_Commands/","title":"Showing job steps","text":"<p>SLURM provides commands to show the execution information of each command line in a job script. This can be helpful for debugging and testing. In order to get  such information, the wrapper command <code>srun</code> needs to be used. Let's take a look at the following job script:</p> <pre><code>#!/bin/bash\n\n#SBATCH -N 4 -n 4 -c 2\n#SBATCH --time=00:05:00\n#SBATCH --mem=1G\n\nmodule purge; module load GCC/6.4.0-2.28 OpenMPI/2.1.2\nmodule list\n\nmpicc mpi-hello.c -o hello.exe\n\necho; echo \"====== mpirun hello.exe ======\"\nmpirun hello.exe                                            #0 Step\n\necho; echo \"====== srun hello.exe ======\"\nsrun hello.exe                                              #1 Step\n\necho; echo \"====== srun -n 8 -c 1 hello.exe ======\"\nsrun -n 8 -c 1 hello.exe                                    #2 Step\n\necho; echo \"====== srun  ======\"\nsrun NoSuchCommand                                          #3 Step\n\necho; echo \"====== mpirun  ======\"\nmpirun NoSuchCommand                                        #4 Step\n\necho; echo \"====== scontrol show job $SLURM_JOB_ID ======\"\nsrun -N 1 -n 1 -c 1 scontrol show job $SLURM_JOB_ID         #5 Step\n</code></pre> <p>Although there are many command lines, only 6 of them are executed with either mpirun or srun wrapper and marked with the comments (from step 0 to 5) in the end. SLURM can record each of the 6 executions as a job step. Once the job is submitted by sbatch command and starts running, you can use sacct command to check the steps:</p> <pre><code>$ sacct -j 10732\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n10732              test general-l+   classres          8  COMPLETED      0:0\n10732.batch       batch              classres          2  COMPLETED      0:0\n10732.extern     extern              classres          8  COMPLETED      0:0\n10732.0           orted              classres          6  COMPLETED      0:0\n10732.1       hello.exe              classres          8  COMPLETED      0:0\n10732.2       hello.exe              classres          8  COMPLETED      0:0\n10732.3      NoSuchCom+              classres          8     FAILED      2:0\n10732.4           orted              classres          6  COMPLETED      0:0\n10732.5        scontrol              classres          1  COMPLETED      0:0\n</code></pre> <p>where the Job has ID 10732 and the 6 steps are shown from JobID 10732.0 to 10732.5.</p> <p>We can also use a powertools command js to see more detailed information (such as memory usage and a list of used nodes) about the steps:</p> <pre><code>$ js -j 10732 -C5\n\nSLURM Job ID: 10732\n===============================================================================================================================\n          JobID |               10732 |         10732.batch |        10732.extern |             10732.0 |             10732.1 |\n        JobName |                test |               batch |              extern |               orted |           hello.exe |\n           User |            changc81 |                     |                     |                     |                     |\n       NodeList |       lac-[380-383] |             lac-380 |       lac-[380-383] |       lac-[381-383] |       lac-[380-383] |\n         NNodes |                   4 |                   1 |                   4 |                   3 |                   4 |\n         NTasks |                     |                   1 |                   4 |                   3 |                   4 |\n          NCPUS |                   8 |                   2 |                   8 |                   6 |                   8 |\n         ReqMem |                 1Gn |                 1Gn |                 1Gn |                 1Gn |                 1Gn |\n      Timelimit |            00:05:00 |                     |                     |                     |                     |\n        Elapsed |            00:00:16 |            00:00:16 |            00:00:16 |            00:00:02 |            00:00:01 |\n      SystemCPU |           00:03.283 |           00:00.562 |           00:00.001 |           00:00.646 |           00:00.572 |\n        UserCPU |           00:02.119 |           00:00.753 |           00:00.003 |           00:00.396 |           00:00.281 |\n       TotalCPU |           00:05.403 |           00:01.316 |           00:00.005 |           00:01.042 |           00:00.853 |\n     AveCPULoad |            0.337687 |             0.08225 |           0.0003125 |               0.521 |               0.853 |\n         MaxRSS |                     |              10409K |                120K |                861K |                863K |\n      MaxVMSize |                     |             652100K |             173968K |             324440K |             324436K |\n          Start | 2018-08-06T13:22:44 | 2018-08-06T13:22:44 | 2018-08-06T13:22:44 | 2018-08-06T13:22:54 | 2018-08-06T13:22:57 |\n            End | 2018-08-06T13:23:00 | 2018-08-06T13:23:00 | 2018-08-06T13:23:00 | 2018-08-06T13:22:56 | 2018-08-06T13:22:58 |\n       ExitCode |                 0:0 |                 0:0 |                 0:0 |                 0:0 |                 0:0 |\n          State |           COMPLETED |           COMPLETED |           COMPLETED |           COMPLETED |           COMPLETED |\n===============================================================================================================================\n          JobID |             10732.2 |             10732.3 |             10732.4 |             10732.5 |\n        JobName |           hello.exe |       NoSuchCommand |               orted |            scontrol |\n           User |                     |                     |                     |                     |\n       NodeList |       lac-[380-383] |       lac-[380-383] |       lac-[381-383] |             lac-380 |\n         NNodes |                   4 |                   4 |                   3 |                   1 |\n         NTasks |                   8 |                   4 |                   3 |                   1 |\n          NCPUS |                   8 |                   8 |                   6 |                   1 |\n         ReqMem |                 1Gn |                 1Gn |                 1Gn |                 1Gn |\n      Timelimit |                     |                     |                     |                     |\n        Elapsed |            00:00:01 |            00:00:00 |            00:00:01 |            00:00:00 |\n      SystemCPU |           00:01.141 |           00:00.051 |           00:00.289 |           00:00.017 |\n        UserCPU |           00:00.521 |           00:00.031 |           00:00.096 |           00:00.035 |\n       TotalCPU |           00:01.663 |           00:00.083 |           00:00.385 |           00:00.053 |\n     AveCPULoad |               1.663 |                     |               0.385 |                     |\n         MaxRSS |              34812K |                865K |                865K |                840K |\n      MaxVMSize |             324436K |             324436K |             324440K |             324436K |\n          Start | 2018-08-06T13:22:58 | 2018-08-06T13:22:59 | 2018-08-06T13:22:59 | 2018-08-06T13:23:00 |\n            End | 2018-08-06T13:22:59 | 2018-08-06T13:22:59 | 2018-08-06T13:23:00 | 2018-08-06T13:23:00 |\n       ExitCode |                 0:0 |                 2:0 |                 0:0 |                 0:0 |\n          State |           COMPLETED |              FAILED |           COMPLETED |           COMPLETED |\n=========================================================================================================\n</code></pre> <p>From the results above, we can see the executions by mpirun are different from srun. First of all, for mpirun, the JobName only show \"orted\" no matter what commands are used in the steps 10732.0 and 10732.4. However, srun shows the correct commands in all of the steps (10732.1, 10732.2, 10732.3 and 10732.5). Secondly, mpirun results show only 3 tasks with 6 CPUs are used but srun results correctly show 4 tasks with 8 CPUs in step 10732.1, 8 tasks with 8 CPUs in step 10732.2 and 1 task with 1 CPU in 10732.5 step. Finally, both steps 10732.3 and 10732.4 ran the same command NoSuchCommand where there is no such file or directory and should cause an error execution. However, mpirun wrapper still consider it is complete without error. Only srun wrapper get the FAIL state with an exit code 2.</p> <p>From the job output in the following results, we see no difference between the outputs of the step 10732.0 (mpirun hello.exe) and the step 10732.1\u00a0(srun hello.exe). SLURM seems to get a good sacct information with srun but not with mpirun. If you wish to use the step information, do not forget to put srun in the command lines.</p> <pre><code>Currently Loaded Modules:\n  1) GCCcore/6.4.0   2) binutils/2.28   3) GCC/6.4.0-2.28   4) OpenMPI/2.1.1\n\n\n====== mpirun hello.exe ======\nHello From: lac-380 I am the recieving processor 1 of 4\nHello From: lac-381              I am processor 2 of 4\nHello From: lac-382              I am processor 3 of 4\nHello From: lac-383              I am processor 4 of 4\n\n====== srun hello.exe ======\nHello From: lac-380 I am the recieving processor 1 of 4\nHello From: lac-381              I am processor 2 of 4\nHello From: lac-382              I am processor 3 of 4\nHello From: lac-383              I am processor 4 of 4\n\n====== srun -n 8 -c 1 hello.exe ======\nHello From: lac-380 I am the recieving processor 1 of 8\nHello From: lac-380              I am processor 2 of 8\nHello From: lac-381              I am processor 3 of 8\nHello From: lac-381              I am processor 4 of 8\nHello From: lac-382              I am processor 5 of 8\nHello From: lac-382              I am processor 6 of 8\nHello From: lac-383              I am processor 7 of 8\nHello From: lac-383              I am processor 8 of 8\n\n====== srun  ======\nslurmstepd: error: execve(): NoSuchCommand: No such file or directory\nslurmstepd: error: execve(): NoSuchCommand: No such file or directory\nslurmstepd: error: execve(): NoSuchCommand: No such file or directory\nsrun: error: lac-381: task 1: Exited with exit code 2\nsrun: error: lac-383: task 3: Exited with exit code 2\nsrun: error: lac-382: task 2: Exited with exit code 2\nslurmstepd: error: execve(): NoSuchCommand: No such file or directory\nsrun: error: lac-380: task 0: Exited with exit code 2\n\n====== mpirun  ======\n--------------------------------------------------------------------------\nmpirun was unable to find the specified executable file, and therefore\ndid not launch the job.  This error was first reported for process\nrank 0; it may have occurred for other processes as well.\n\nNOTE: A common cause for this error is misspelling a mpirun command\n      line parameter option (remember that mpirun interprets the first\n      unrecognized command line token as the executable).\n\nNode:       lac-380\nExecutable: NoSuchCommand\n--------------------------------------------------------------------------\n\n====== scontrol show job 10732 ======\nJobId=10732 JobName=test\n   UserId=changc81(804793) GroupId=helpdesk(2103) MCS_label=N/A\n   Priority=103 Nice=0 Account=classres QOS=normal\n   JobState=RUNNING Reason=None Dependency=(null)\n   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   RunTime=00:00:18 TimeLimit=00:05:00 TimeMin=N/A\n   SubmitTime=2018-08-06T13:22:43 EligibleTime=2018-08-06T13:22:43\n   StartTime=2018-08-06T13:22:44 EndTime=2018-08-06T13:27:44 Deadline=N/A\n   PreemptTime=None SuspendTime=None SecsPreSuspend=0\n   LastSchedEval=2018-08-06T13:22:44\n   Partition=general-long-16 AllocNode:Sid=lac-249:5133\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=lac-[380-383]\n   BatchHost=lac-380\n   NumNodes=4 NumCPUs=8 NumTasks=4 CPUs/Task=2 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=8,mem=4G,node=4,billing=8\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   MinCPUsNode=2 MinMemoryNode=1G MinTmpDiskNode=0\n   Features=(null) DelayBoot=00:00:00\n   Gres=(null) Reservation=(null)\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/mnt/home/changc81/GetExample/helloMPI/test\n   WorkDir=/mnt/home/changc81/GetExample/helloMPI\n   Comment=stdout=/mnt/home/changc81/GetExample/helloMPI/slurm-10732.out\n   StdErr=/mnt/home/changc81/GetExample/helloMPI/slurm-10732.out\n   StdIn=/dev/null\n   StdOut=/mnt/home/changc81/GetExample/helloMPI/slurm-10732.out\n   Power=\n</code></pre> <p>For a complete instruction of sacct command, please refer to the SLURM web site.</p>","tags":["tutorial","slurm","srun"]},{"location":"Singularity_Advanced_Topics/","title":"Singularity Advanced Topics","text":"","tags":["explanation","containers","Singularity","Docker"]},{"location":"Singularity_Advanced_Topics/#building-containers","title":"Building containers","text":"","tags":["explanation","containers","Singularity","Docker"]},{"location":"Singularity_Advanced_Topics/#building-writable-containers","title":"Building writable containers","text":"<p>By default, the containers you build in Singularity are read-only. Any changes you make are not saved. This usually is not a problem if the container you use has everything you need, since you can save files in your working directory or home directory and they will persist outside of the containers.</p> <p>However, if your container is missing something that doesn't make sense to include in your home directory that you would like to persist between runs of the container (like another piece of software), you can build a container in a writable directory, called a sandbox.</p> <p>Tip</p> <p>An alternative way to have a writable (portion of a) filesystem in Singularity is an overlay. Overlays are files that act like a storage drive you can \"plug in\" to your container rather than encompassing the entire root filesystem. Since overlays are a viewed as a single file, they are great for \"tricking\" the HPCC into allowing you to use more files than your quota allows. For more information, including powertools to help you get started and examples installing conda, see the Lab Notebook on Singularity Overlays.</p> <p>You can create a sandbox using the <code>singularity build</code> command with the <code>--sandbox</code> option. As arguments, use a directory name for the location of the sandbox and an image you want to start with (which can either be a URI or a file):</p> <pre><code>$ singularity build --sandbox alpine/  docker://alpine\n</code></pre> <p>If you look inside the directory, it looks like the full file system for the container</p> <pre><code>$ ls alpine\nbin  environment  home  media  opt   root  sbin         srv  tmp  var\ndev  etc          lib   mnt    proc  run   singularity  sys  usr\n</code></pre> <p>To run this image with any Singularity command, pass the directory as the image name:</p> <pre><code>singularity shell alpine/\n</code></pre> <p>However in order to make changes that will persist, you need to use the <code>--writable</code> option. Let's try to install some software:</p> <pre><code>$ singularity shell --writable alpine/\nSingularity&gt; python\n/bin/sh: python: command not found\nSingularity&gt; apk add --update py3-pip\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.17/main/x86_64/APKINDEX.tar.gz\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.17/community/x86_64/APKINDEX.tar.gz\n(1/19) Installing libbz2 (1.0.8-r4)\n...\n(19/19) Installing py3-pip (22.3.1-r1)\nSingularity&gt; python\nPython 3.10.11 (main, Apr  6 2023, 01:16:54) [GCC 12.2.1 20220924] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\n</code></pre> <p>If we use the <code>alpine/</code> sandbox again, we'll still have access to Python! The sandbox can be packaged back up into an image file by again using the <code>singularity build</code>:</p> <pre><code>$ singularity build alpine_with_python.sif alpine/\nINFO:    Starting build...\nINFO:    Creating SIF file...\nINFO:    Build complete: alpine_with_python.sif\n$ singularity exec alpine_with_python.sif python --version\nPython 3.10.11\n</code></pre> <p>However, this method for building new containers does not leave a record of how you changed the base image. For better reproducibility, you should use a Singularity definition file as described in the next section.</p>","tags":["explanation","containers","Singularity","Docker"]},{"location":"Singularity_Advanced_Topics/#building-containers-from-scratch-with-singularity-definition-files","title":"Building containers from scratch with Singularity definition files","text":"<p>Warning</p> <p>Building Singularity files from definition files requires super user permissions. You will need to install Singularity on your local computer to run these steps.</p> <p>Alternatively, you might prefer building a Docker container and using it in Singularity as discussed below.</p> <p>To build containers, Singularity uses a Singularity definition file which is similar to a Dockerfile in Docker. We will walk through building a Singularity definition file that creates a comparable image to the one in our Docker tutorial.</p> <p>We will set up our working directory the same way:</p> <pre><code>$ cd ~\n$ mkdir my_first_Singularity_image\n$ cd my_first_Singularity_image\n</code></pre> <p>and create the python script <code>hello.py</code>:</p> hello.py<pre><code>print(\"Hello world!\")\nprint(\"This is my 1st Singularity image!\")\n</code></pre> <p>Now, create the file <code>Singularity</code> with the content below:</p> Singularity<pre><code>Bootstrap: docker\nFrom: alpine\n\n# copy files required to run\n%files \n    hello.py /usr/src/my_app/\n\n# install python and pip\n%post\n    apk add --update py3-pip\n\n# run the application\n%runscript\n    python3 /usr/src/my_app/hello.py\n</code></pre> <p>Now, let's learn the meaning of each section.</p> <p>The first section means that we will use Alpine Linux as a base image. In fact, the <code>Bootstrap</code> line tells Singularity that we are using the Docker alpine image hosted on Docker Hub. Other options for the <code>Bootstrap</code> line include <code>library</code> for images in Sylab's container library and <code>shub</code> for images on the (archived) Singularity hub.</p> <pre><code>Bootstrap: docker\nFrom: alpine\n</code></pre> <p>The <code>%files</code> section tells Singularity which files in our local directory we want to copy to the container and where we should move them. In this case, we are copying our Python script into <code>/usr/src/my_app/</code> in the container.</p> <pre><code>%files \n    hello.py /usr/src/my_app/\n</code></pre> <p>The <code>%post</code> section is used to install <code>pip</code> using the Alpine Package Keeper (<code>apk</code>).</p> <pre><code>%post\n    apk add --update py3-pip\n</code></pre> <p>Finally, the <code>%runscript</code> section tells the container what command to run when the container is invoked through <code>singularity run</code>.</p> <pre><code>%runscript\n    python3 /usr/src/my_app/hello.py\n</code></pre> <p>We can now build the image using the <code>singularity build</code> command. Don't forget that you'll need super user permission!</p> <pre><code>$ sudo singularity build my_first_image.sif Singularity\n</code></pre> <p>This will create the <code>my_first_image.sif</code> file that you can now run.</p> <pre><code>$ singularity run my_first_image.sif\nHello world!\nThis is my 1st Singularity image!\n</code></pre> <p>You can now use this singularity image file anywhere you like, including the HPCC.</p>","tags":["explanation","containers","Singularity","Docker"]},{"location":"Singularity_Advanced_Topics/#migrating-from-docker-to-singularity","title":"Migrating from Docker to Singularity","text":"<p>For more information regarding Docker support in Singularity, please see the official documentation.</p>","tags":["explanation","containers","Singularity","Docker"]},{"location":"Singularity_Advanced_Topics/#direct-comparision","title":"Direct comparision","text":"Topic Docker Singularity Installation Local computer only HPCC and local computer Privileges Requires super user privileges Only requires super user privileges for building images Compatibility Docker images Docker and Singularity images Images Cached and managed by Docker Available as <code>.sif</code> files (can also be cached and managed by Singularity) File sharing Manually specifying bind mounts (e.g., <code>-v</code> option) Automatically binds useful directories (<code>$HOME</code>, <code>$PWD</code>, etc.); others can be specified via <code>--bind</code> option and through overlay files Build file Dockerfile Singularity definition file Downloading images <code>docker pull &lt;container&gt;</code> <code>singularity pull &lt;uri-prefix&gt;://&lt;container&gt;</code> Running <code>docker run &lt;container&gt;</code> <code>singularity run &lt;container&gt;.sif</code> Running command <code>docker run &lt;container&gt; &lt;command&gt;</code> <code>singularity exec &lt;container&gt;.sif &lt;command&gt;</code> Interactive shell <code>docker -it &lt;container&gt; sh</code> <code>singularity shell &lt;container&gt;.sif</code>","tags":["explanation","containers","Singularity","Docker"]},{"location":"Singularity_Advanced_Topics/#converting-from-docker-images-to-singularity-images","title":"Converting from Docker images to Singularity images","text":"<p>There are a few ways to use Docker images with Singularity. If the image is publicly available on Docker Hub, it is as easy as using the <code>singularity pull</code> command with a Docker URI. See the example in the Singularity introduction. If you are installing from a private repository on Docker Hub, use the <code>--docker-login</code> flag with <code>singularity pull</code> to authenticate with Docker.</p> <p>If the Docker image is only available locally (e.g., you are testing local builds and don't want to push to a repository), you have two options. First, you can build a Singularity image directly from a cached Docker image:</p> <pre><code>sudo singularity build &lt;singularity-image-filename&gt;.sif docker-daemon://&lt;docker-image-name&gt;\n</code></pre> <p>Note that this requires Singularity and Docker to be installed on the same system, and requires super user permissions.</p> <p>The second option is to first archive the Docker image into a <code>tar</code> file, then use this to build the Singularity image:</p> <pre><code>docker save &lt;docker-image-name&gt; -o docker_image.tar\nsingularity build &lt;singularity-image-filename&gt;.sif docker-archive://docker_image.tar\n</code></pre> <p>Here you could perform the <code>docker save</code> on your local machine, move the <code>docker_image.tar</code> file to the HPCC, and then run the <code>singularity build</code> step on the HPCC since it does not require super user privileges.</p>","tags":["explanation","containers","Singularity","Docker"]},{"location":"Singularity_Advanced_Topics/#a-note-on-permissions","title":"A note on permissions","text":"<p>Singularity automatically mounts many system directories in your container, including <code>$HOME</code> and <code>$PWD</code>. When you enter a shell in a Singularity container, you will be in the same directory you started from. You are also logged in as the same user inside the Singularity container as you are on the host when you start the container.</p> <p>In contrast, a Docker shell usually starts in <code>/</code> as the <code>root</code> user (or some other user).  Thus, you may have different permissions in a Docker container that is run in Singularity. This can cause problems if a Docker container expects you to be able to write to directories that your HPCC user will not have access to (like <code>/root</code>).</p> <p>In these cases, you may have to modify the Dockerfile used to create the Docker image so that anything you need to access is stored in a location accessible to your user.</p>","tags":["explanation","containers","Singularity","Docker"]},{"location":"Singularity_Advanced_Topics/#using-singularity-with-mpi-and-gpus","title":"Using Singularity with MPI and GPUs","text":"<p>If you are running a container that uses MPI, you must use <code>srun -n $SLURM_NTASKS -c $SLURM_CPUS_PER_TASK</code> before the <code>singularity</code> command to make the command aware of all resources allotted. See a template script below.</p> singularity_mpi.sbatch<pre><code>#!/bin/bash\n\n# Job name:\n#SBATCH --job-name=singularity-test\n#\n# Number of MPI tasks needed for use case:\n#SBATCH --ntasks=18\n#\n# Number of nodes to split the tasks across:\n#SBATCH --nodes=2\n#\n# Processors per task:\n#SBATCH --cpus-per-task=4\n#\n# Memory per CPU\n#SBATCH --mem-per-cpu=1G\n#\n# Wall clock limit:\n#SBATCH --time=30\n#\n# Standard out and error:\n#SBATCH --output=%x-%j.SLURMout\n\ncd &lt;directory containing the singularity image file (.sif)&gt;\nsrun -n $SLURM_NTASKS -c $SLURM_CPUS_PER_TASK singularity exec &lt;singularity-file&gt;.sif &lt;commands&gt;\n</code></pre> <p>To run a container that takes advantage of GPU resources, you can use the <code>--nv</code> flag on any <code>run</code>, <code>exec</code>, or <code>shell</code> singularity commands. Otherwise, use the standard <code>sbatch</code> setup for running any GPU job. An example script that pulls a TensorFlow container and displays the available GPUs is shown below.</p> singularity_gpu.sbatch<pre><code>#!/bin/bash\n\n# Job name:\n#SBATCH --job-name=singularity-test\n#\n# Request GPU:\n#SBATCH --gpus=v100:1\n#\n# Memory per CPU\n#SBATCH --mem-per-cpu=20G\n#\n# Wall clock limit:\n#SBATCH --time=30\n#\n# Standard out and error:\n#SBATCH --output=%x-%j.SLURMout\n\nsingularity pull docker://tensorflow/tensorflow:latest-gpu\nsingularity exec --nv tensorflow_latest-gpu.sif python -c \"from tensorflow.python.client import device_lib; print(device_lib.list_local_devices())\"\n</code></pre>","tags":["explanation","containers","Singularity","Docker"]},{"location":"Singularity_Advanced_Topics/#cached-images","title":"Cached images","text":"<p>When you use Docker image without pulling it first, it appears that no Singularity image file was created:</p> <pre><code>$ mkdir new_dir\n$ cd new_dir\n$ singularity exec docker://centos uname -a\nGetting image source signatures\nCopying blob a1d0c7532777 done  \n...\nLinux dev-amd20 3.10.0-1160.80.1.el7.x86_64 #1 SMP Tue Nov 8 15:48:59 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux\n$ ls\n$\n</code></pre> <p>In fact, Singularity stores these files and the files used to create them in a cache:</p> <pre><code>$ singularity cache list\nThere are 9 container file(s) using 4.06 GiB and 68 oci blob file(s) using 4.18 GiB of space\nTotal space used: 8.24 GiB\n</code></pre> <p>As you can see, the files stored here can build up quickly. You can clean this cache using</p> <pre><code>$ singularity cache clean\n</code></pre> <p>Everything in the cache can be safely removed, and will just be redownloaded if needed again.</p> <p>By default, Singularity uses <code>~/.singularity/cache</code> to store these files. If you want to use another directory (e.g., your scratch space), you can use the <code>SINGULARITY_CACHEDIR</code> environment variable. Singularity also uses a temporary directory (<code>/tmp</code> by default) that you might also want to change using the <code>SINGULARITY_TEMPDIR</code> environment variable. For example:</p> <pre><code>$ mkdir -p $SCRATCH/singularity_tmp\n$ mkdir -p $SCRATCH/singularity_scratch\n$ SINGULARITY_CACHEDIR=$SCRATCH/singularity_scratch SINGULARITY_TMPDIR=$SCRATCH/singularity_tmp singularity --debug pull --name ubuntu-tmpdir.sif docker://ubuntu\n</code></pre> <p>Using the <code>--debug</code> flag shows a lot of information, but at the end we see these lines:</p> <pre><code>VERBOSE [U=919141,P=3605]  Full()    Build complete: /mnt/gs21/scratch/&lt;user&gt;/singularity_scratch/cache/oci-tmp/tmp_011111517\nDEBUG   [U=919141,P=3605]  cleanUp() Cleaning up \"/mnt/gs21/scratch/&lt;user&gt;/singularity_tmp/build-temp-030629912/rootfs\" and \"/mnt/gs21/scratch/&lt;user&gt;/singularity_tmp/bundle-temp-545905815\"\n</code></pre> <p>verifying that the scratch directories were used.</p>","tags":["explanation","containers","Singularity","Docker"]},{"location":"Singularity_Introduction/","title":"Singularity Introduction","text":"<p>Note</p> <p>This tutorial is adapted from the Container Camp Tutorial produced and copyrighted by CyVerse.</p> <p>Singularity allows users to run software inside of containers. Another popular container system is Docker, which is interoperable with Singularity. Singularity provides many features that make it well suited for HPC applications, and therefore, it is the container system that is installed on our HPCC.</p>","tags":["tutorial","containers","Singularity"]},{"location":"Singularity_Introduction/#installation","title":"Installation","text":"<p>To install Singularity on your local machine, please see the installation instructions in the documentation. Singularity can only be run natively on Linux, but if you need to run it locally on a Windows or Mac computer, you can use a virtual machine provided by the creators of Singularity. See these alternative instructions for details.</p> <p>Note</p> <p>As of May 2023, the version of Singularity on the MSU HPCC is currently 3.11. For any questions that this tutorial or the Advanced Topics page do not answer, please consult the official documentation for this version. All Singularity commands are built into the system such as <code>singularity shell</code> and <code>singularity exec</code>, which means you can invoke these commands directly from the command line.</p>","tags":["tutorial","containers","Singularity"]},{"location":"Singularity_Introduction/#check-installation","title":"Check installation","text":"<p>If you would like to check your installation of Singularity on your local machine or the installation on the HPCC, you can run</p> <pre><code>$ singularity pull shub://vsoch/hello-world\nINFO:    Downloading shub image\n59.75 MiB / 59.75 MiB      [========================================================================================] 100.00% 10.46 MiB/s 5s\n\n$ singularity run hello-world_latest.sif\nRaawwWWWWWRRRR!! Avocado!\n</code></pre> <p>In the above example, we used the Singularity Hub \"unique resource identifier,\" or URI, \"shub://\" which tells the software to run an image from Singularity Hub. To get help, you can use the <code>--help</code> flag which gives a general overview of Singularity options and subcommands as follows:</p> <pre><code>$ singularity --help\n\nLinux container platform optimized for High Performance Computing (HPC) and\nEnterprise Performance Computing (EPC)\n\nUsage:\n  singularity [global options...]\n\nDescription:\n  Singularity containers provide an application virtualization layer enabling\n  mobility of compute via both application and environment portability. With\n  Singularity one is capable of building a root file system that runs on any\n  other Linux system where Singularity is installed.\n\nOptions:\n  -d, --debug     print debugging information (highest verbosity)\n  -h, --help      help for singularity\n  --nocolor   print without color output (default False)\n  -q, --quiet     suppress normal output\n  -s, --silent    only print errors\n  -v, --verbose   print additional information\n      --version   version for singularity\n ...\n</code></pre> <p>You can use the <code>help</code> command if you want to see the information about subcommands. For example, to see the <code>pull</code> command help,</p> <pre><code>$ singularity help pull\n\nPull an image from a URI\n\nUsage:\nsingularity pull [pull options...] [output file] &lt;URI&gt;\n\nDescription:\nThe 'pull' command allows you to download or build a container from a given\nURI. Supported URIs include:\n\nlibrary: Pull an image from the currently configured library\n    library://user/collection/container[:tag]\n\ndocker: Pull an image from Docker Hub\n    docker://user/image:tag\n\nshub: Pull an image from Singularity Hub\n    shub://user/image:tag\n\noras: Pull a SIF image from a supporting OCI registry\n    oras://registry/namespace/image:tag\n\nhttp, https: Pull an image using the http(s?) protocol\n    https://library.sylabs.io/v1/imagefile/library/default/alpine:latest\n...\n</code></pre>","tags":["tutorial","containers","Singularity"]},{"location":"Singularity_Introduction/#downloading-pre-built-images","title":"Downloading pre-built images","text":"<p>We already downloaded a pre-built image \"hello-world\" from shub, one of the  registries, using pull command. This is the easiest way to use Singularity.</p> <p>You can use the <code>pull</code> command to download pre-built images from a number of Container Registries. Here we\u2019ll be focusing on the Singularity Hub or Docker Hub. The following are some of container registries.</p> <ul> <li>library - images hosted on Sylabs Cloud</li> <li>shub - images hosted on the (archived) Singularity Hub</li> <li>docker - images hosted on Docker Hub</li> </ul>","tags":["tutorial","containers","Singularity"]},{"location":"Singularity_Introduction/#pulling-an-images-from-sylabs-cloud-library","title":"Pulling an images from Sylabs cloud library","text":"<p>In this example, I will pull a base Alpine container from Sylabs cloud:</p> <pre><code>$ singularity pull library://sylabsed/linux/alpine\nINFO:    Downloading library image\n 2.08 MiB / 2.08 MiB [===========================================================================================] 100.00% 4.74 MiB/s 0s\n</code></pre> <p>You can rename the container using the <code>--name</code> flag:</p> <pre><code>$ singularity pull --name my_alpine.sif library://sylabsed/linux/alpine\nINFO:    Downloading library image\n 2.08 MiB / 2.08 MiB [===========================================================================================] 100.00% 9.65 MiB/s 0s\n</code></pre> <p>The above example will save the image in the current directory as <code>my_alpine.sif</code></p>","tags":["tutorial","containers","Singularity"]},{"location":"Singularity_Introduction/#pulling-an-image-from-docker-hub","title":"Pulling an image from Docker hub","text":"<p>Many programs are available as Docker containers pre-built, and many of those are available on Docker Hub. For more details on using Docker containers with Singularity, see our section on Migrating from Docker to Singularity.</p> <p>Here is a quick example of pulling an Alpine Docker container for use with Singularity.</p> <pre><code>$ singularity pull docker://alpine\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nGetting image source signatures\nCopying blob df20fa9351a1 done\nCopying config 0f5f445df8 done\nWriting manifest to image destination\nStoring signatures\n2020/08/20 15:53:52  info unpack layer: sha256:df20fa9351a15782c64e6dddb2d4a6f50bf6d3688060a34c4014b0d9a752eb4c\nINFO:    Creating SIF file...\nINFO:    Build complete: alpine_latest.sif\n</code></pre>","tags":["tutorial","containers","Singularity"]},{"location":"Singularity_Introduction/#interacting-with-images","title":"Interacting with images","text":"<p>You can interact with images via the <code>shell</code>, <code>exec</code>, and <code>run</code> commands. To learn how to interact with images, let's first pull an image <code>lolcow_latest.sif</code> from the library.</p> <pre><code>singularity pull library://sylabsed/examples/lolcow\n</code></pre>","tags":["tutorial","containers","Singularity"]},{"location":"Singularity_Introduction/#shell","title":"<code>shell</code>","text":"<p>The <code>shell</code> command allows you to spawn a new shell within your container and interact with it as if it is a virtual machine.</p> <pre><code>$ singularity shell lolcow_latest.sif\nSingularity&gt;    \n</code></pre> <p>The change in prompt indicates that you have entered the container. Once inside of a container, you are the same user as you are on the host system.</p> <pre><code>Singularity&gt; whoami\nchoiyj\n</code></pre> <p>You will also have access to a few directories that you can access outside the container, most notably, the directory you ran the container from and your home directory. Try running <code>pwd</code> and <code>ls ~</code> inside the container to verify this.</p> <p>Anything you write to these directories will stay around after you are done with the container. This is a significant difference from Docker, where the default is to close off the files in the container and you need to use \"bind mounts\" to manually connect file spaces.</p> <p>To exit from a container, type <code>exit</code>.</p> <pre><code>Singularity&gt; exit\n$ \n</code></pre>","tags":["tutorial","containers","Singularity"]},{"location":"Singularity_Introduction/#exec","title":"<code>exec</code>","text":"<p>The <code>exec</code> command allows you to execute a custom command within a container by specifying the image file. For instance, to execute the <code>cowsay</code> program within the <code>lolcow_latest.sif</code> container:</p> <pre><code>$ singularity exec lolcow_latest.sif cowsay container camp rocks\n______________________\n&lt; container camp rocks &gt;\n ----------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre> <p>You can also use <code>shell</code> command to run the program in the container.</p> <pre><code>$ singularity shell lolcow_latest.sif\nSingularity&gt; cowsay container camp rocks\n ______________________\n&lt; container camp rocks &gt;\n ----------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre>","tags":["tutorial","containers","Singularity"]},{"location":"Singularity_Introduction/#run","title":"<code>run</code>","text":"<p>Singularity containers contain runscripts. These are predefined scripts which define the actions of a container when user runs it. The runscript can be performed with the <code>run</code> command, or simply by calling the container as though it were an executable.</p> <pre><code>$ singularity run lolcow_latest.sif\n _________________________________________\n/ You're ugly and your mother dresses you \\\n\\ funny.                                  /\n -----------------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre>","tags":["tutorial","containers","Singularity"]},{"location":"Singularity_Introduction/#submitting-a-singularity-job","title":"Submitting a Singularity job","text":"<p>In general, running Singularity commands is the same as running any kind of program when you prepare your SLURM script.</p> <p>In this case, you put your Singularity commands (<code>singularity exec &lt;image&gt;.sif &lt;command&gt;</code>) right after the <code>sbatch</code> directive lines you use to specify your job resources. If the program needs to use multiple threads/cores on a node, say 8, you would request 8 cores using <code>#SBATCH --cpus-per-task=8</code> as you would do with any other <code>sbatch</code> script.</p> <p>For situations where you are using MPI within the container (e.g., you would like to split your code over multiple nodes) or would like to use GPU resources, please see Using Singularity with MPI and GPUs on the Advanced Topics page.</p>","tags":["tutorial","containers","Singularity"]},{"location":"Slurm_Environment_Variables/","title":"SLURM Environment Variables","text":"<p>The SLURM controller will set variables in the environment of the batch script. Below is a list of SLURM variables that you may use within your job script. Some variables are only defined if their corresponding options were invoked, such as those pertaining to job arrays or task and CPU configurations. See the curated list of job specifications for more on these options.</p> SLURM Variables Description SLURM_ARRAY_TASK_COUNT Total number of tasks in a job array SLURM_ARRAY_TASK_ID Job array ID (index) number SLURM_ARRAY_TASK_MAX Job array's maximum ID (index) number SLURM_ARRAY_TASK_MIN Job array's minimum ID (index) number SLURM_ARRAY_TASK_STEP Job array's index step size SLURM_ARRAY_JOB_ID Job array's master job ID number SLURM_CLUSTER_NAME Name of the cluster on which the job is executing SLURM_CPUS_ON_NODE Number of CPUS on the allocated node SLURM_CPUS_PER_TASK Number of cpus requested per task. Only set if the <code>--cpus-per-task</code> option is specified. SLURM_JOB_ACCOUNT Account name associated of the job allocation SLURM_JOBID, SLURM_JOB_ID The ID of the job allocation SLURM_JOB_CPUS_PER_NODE Count of processors available to the job on this node. SLURM_JOB_DEPENDENCY Set to value of the --dependency option SLURM_JOB_NAME Name of the job SLURM_NODELIST, SLURM_JOB_NODELIST List of nodes allocated to the job SLURM_NNODES, SLURM_JOB_NUM_NODES Total number of different nodes in the job's resource allocation SLURM_MEM_PER_NODE Takes the value of <code>--mem</code> if this option was specified. SLURM_MEM_PER_CPU Takes the value of <code>--mem-per-cpu</code> if this option was specified. SLURM_NTASKS, SLURM_NPROCS Same as <code>-n</code> or <code>--ntasks</code> if either of these options was specified. SLURM_NTASKS_PER_NODE Number of tasks requested per node. Only set if the <code>--ntasks-per-node</code> option is specified. SLURM_NTASKS_PER_SOCKET Number of tasks requested per socket. Only set if the <code>--ntasks-per-socket</code> option is specified. SLURM_SUBMIT_DIR The directory from which sbatch was invoked SLURM_SUBMIT_HOST The hostname of the computer from which <code>sbatch</code> was invoked SLURM_TASK_PID The process ID of the task being started SLURMD_NODENAME Name of the node running the job script SLURM_JOB_GPUS GPU IDs allocated to the job (if any).","tags":["reference","slurm","job script"]},{"location":"Slurm_Environment_Variables/#setting-environment-variables-in-slurm-jobs","title":"Setting Environment Variables in SLURM Jobs","text":"<p>You may also set your own variables for use in your SLURM jobs.    One way is to set them inside the script itself, but that requires modifying the script. </p> <p>It is possible to pass variables into a SLURM job when you submit the job using the <code>--export</code> flag.  For example to pass the value of the variables <code>REPS</code> and <code>X</code> into the job script named <code>jobs.sb</code> you can use:</p> <pre><code>sbatch --export=REPS=500,X='test' jobs.sb\n</code></pre> <p>These are then available in your jobs as <code>$REPS</code> and <code>$X</code></p>","tags":["reference","slurm","job script"]},{"location":"Slurm_Environment_Variables/#using-variables-to-set-slurm-job-name-and-output-files","title":"Using variables to set SLURM job name and output files","text":"<p>SLURM does not support using arbitrary variables in the <code>#SBATCH</code> lines within a job script; for example, <code>#SBATCH -N=$REPS</code> will not replace <code>$REPS</code> with the variable's value.</p> <p>For specifying filenames (such as the SLURM log/output file), a limited number of pre-defined variables are available. These include <code>%j</code>, which references the job ID. For example, you can use <code>#SBATCH --output=analysis-%j.out</code> to set a custom output filename that includes the job number. The full list of these variables is available in the <code>sbatch</code> documentation</p> <p>Job options specified from the command line have precedence over values defined in the job script and you can set certain SLURM variables in the command line.  For example, you could set the job name and output/error files from the <code>sbatch</code> command line:</p> <pre><code>RUNTYPE='test'\nRUNNUMBER=5\nsbatch --job-name=$RUNTYPE.$RUNNUMBER.run \\\n--output=$RUNTYPE.$RUNUMBER.txt \\\n--export=A=$A,b=$b jobscript.sbatch\n</code></pre> <p>However note in this example, the output file doesn't have the job ID, which is not available from the command line. The job ID is only defined inside the environment created when running the batch script.</p>","tags":["reference","slurm","job script"]},{"location":"Stata/","title":"Stata","text":"<p>Many versions of Stata are installed on the ICER HPC. When you log in, Stata is not available by default, but it may be load easily using this command\u00a0 (note you must type Stata with a capital 'S'</p> <pre><code>[hpc@dev-intel18 ~]$ module load Stata\n</code></pre> <p>This loads Stata SE version 15.\u00a0 This is equivalent to using the command</p> <pre><code>[hpc@dev-intel18 ~]$ module load Stata/15.0.SE\n</code></pre> <p>Stata has a command line version and a GUI (windowed) version.\u00a0To use the command line, type stata at the prompt.\u00a0You will see this :</p> <p>Using Stata command line</p> <pre><code>[hpc@dev-intel18 ~]$ module load Stata\n[hpc@dev-intel18 ~]$ stata\n\n  ___  ____  ____  ____  ____ (R)\n /__    /   ____/   /   ____/\n___/   /   /___/   /   /___/   15.0   Copyright 1985-2017 StataCorp LLC\n  Statistics/Data Analysis            StataCorp\n                                      4905 Lakeway Drive\n                                      College Station, Texas 77845 USA\n                                      800-STATA-PC        http://www.stata.com\n                                      979-696-4600        stata@stata.com\n                                      979-696-4601 (fax)\n\n15-user Stata network perpetual license:\n       Serial number:  401506213245\n         Licensed to:  iCER / HPCC at Michigan State University\n                       East Lansing, MI\n\nNotes:\n      1.  Unicode is supported; see help unicode_advice.\n\n.\n</code></pre> <p>In which you may type Stata commands.\u00a0Type 'exit' to quit this version.\u00a0</p> <p>To run a Stata do file from the command line in 'batch', you use the syntax</p> <pre><code>[hpc@dev-intel18 ~]$ stata -b do dofilename.do\n</code></pre>"},{"location":"Stata/#versions","title":"Versions","text":"<p>Stata comes in several versions: IC, SE, and MP; see https://www.stata.com/products/which-stata-is-right-for-me/ for details for the differences.\u00a0Stata/IC has limitations on the numbers of variables that affect most users but has no licensing restrictions (see below). While the default version of Stata available when you load the module is Stata/SE, you currently have to use the command 'stata-se' to start the 'SE' version</p> <pre><code>[hpc@dev-intel18 ~]$ module load Stata/15.0.SE\n[hpc@dev-intel18 ~]$ stata-se  # to run the interactive version\n[hpc@dev-intel18 ~]$ stata-se -b do dofilename.do\u00a0 # to run a do file \n</code></pre> <p>However, to use the \"MP\" Version, you must load it explicitly.\u00a0</p> <pre><code>[hpc@dev-intel18 ~]$ module load Stata/15.0.MP\n[hpc@dev-intel18 ~]$ stata-mp\n</code></pre> <p>Note that even if you load Stata/MP or SE module, as above, if you just use the command <code>stata</code> it will load the IC version.\u00a0To see which version of Stata you are current in, use the <code>about</code> command at the dot prompt.\u00a0To use these special versions to run a bach do file, use stata-se and stata-mp instead of plain stata</p> <p>For Stata/SE There are 15 licenses available, so 15 users may use it, for MP there are 5 user licenses of 8-cores each. Please exit the program when you are finished with it.\u00a0\u00a0</p>"},{"location":"Stata/#gui-version","title":"GUI version","text":"<p>To use the GUI version, you must first be connected to HPCC with X11 forwarding ( MobaXterm for Windows, XQuartz for Mac - see\u00a0 instructions on installing an SSH client )\u00a0 or using a web-base connection to HPCC (see\u00a0 instructions on Connecting via web site ). Once an X11 or remote desktop client is connected, you can run xstata on a dev node:</p> <pre><code>[hpc@dev-intel18 ~]$ module load Stata\n[hpc@dev-intel18 ~]$ xstata\n</code></pre> <p></p>"},{"location":"Stata/#variables-limits","title":"Variables Limits","text":"<p>Even if you load the MP or SE versions, Stata limits the number of variables to 5000 unless you tell it otherwise.\u00a0\u00a0\u00a0 For information use the help set_maxvar command at the dot prompt.\u00a0\u00a0 You can set the maxvar for your session or in your do file with (for example to 6000)</p> <pre><code>. set maxvar 6000\n</code></pre> <p>There are other settings related to memory usage which are important as Stata attempts to be very conservative.\u00a0 For more information use the Stata \"memory\" command</p>"},{"location":"Stata/#running-jobs","title":"Running Jobs","text":"<p>Note you must use the command line version inside a sb script when running jobs.\u00a0 To copy a working example of Stata job file into your\u00a0 home directory, you can use our getexample tool</p> <p>Getting Stata Example</p> <pre><code>module load powertools\ncd ~\ngetexample STATA_example\ncd STATA_example\n\n# look into the README file in this folder for details\ncat README\n</code></pre>"},{"location":"Stata/#more-help","title":"More help","text":"<p>For questions requiring deeper knowledge of statistics, \u00a0users could contact\u00a0CSTAT services at\u00a0 https://cstat.msu.edu/cstat-services\u00a0and use\u00a0the \"schedule a meeting\" link to submit an intake form.</p>"},{"location":"Submitting-multiple-jobs-simultaneously_40337501.html/","title":"Submitting Multiple Jobs Simultaneously","text":"<ul> <li>Make a copy of the job script multi_seq.sb and name it     multi_sim.sb</li> <li>Edit multi_sim.sb to simultaneously run python_script.py and     r_script.R Be sure to update the resources required to run     multi_sim.sb if necessary.</li> <li> <p>Submit job to compute node</p> <p>Answer </p> </li> </ul> <pre><code>    #Make a copy of the job script multi_seq.sb and name it multi-sim.sb\n    cp multi_seq.sb multi_sim.sb\n\n    #Edit multi_sim.sb to simultaneously run python_script.py and r_script.R\n    gedit multi_sim.sb\n    python3 python_script.py&amp;\n    Rscript r_script.R&amp;\n    wait\n\n    #Note: Be sure to use \u201c&amp;\u201d (otherwise run in sequential) and \u201cwait\u201d (otherwise job exit immediately)\n    #Be sure to update the resources required to run multi-sim.sb if necessary. Note, the number of nodes/cores requested should be the sum of the nodes/cores needed to run each job. In this case since each job uses one node and one core. Requesting one node is sufficient.\n\n    #Submit job to compute node. Type the following at the command line:\n    sbatch multi_sim.sb\n</code></pre>"},{"location":"Submitting_a_TensorFlow_job/","title":"Submitting a TensorFlow job","text":"<p>After you've installed TF in your conda environment, we can submit a TF job to the cluster. In order to make use of GPU computing, we'll need to request for a GPU node in the job script through the <code>--gpu</code> directive.</p> <p>As an example, the TF python code, <code>matmul.tf2.py</code>, is shown below:</p> <pre><code>import tensorflow as tf\ntf.debugging.set_log_device_placement(True)\ntf.config.set_soft_device_placement(True)\n\nwith tf.device('/device:GPU:2'):\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n    c = tf.matmul(a, b)\nprint(c)\n</code></pre> <p>Because <code>tf.config.set_soft_device_placement</code> is turned on, even if this code is assigned a CPU-only node, it will still run. The multiplication step will be carried out using the CPU.</p> <p>Now, let's write our SLURM job script, <code>testTF.sbatch</code>, which contains the following:</p> <pre><code>#!/bin/bash\n\n# Job name:\n#SBATCH --job-name=test_matmul\n#\n# Request GPU:\n#SBATCH --gpus=v100:1\n#\n# Memory:\n#SBATCH --mem-per-cpu=20G\n#\n# Wall clock limit (minutes or hours:minutes or days-hours):\n#SBATCH --time=20\n#\n# Standard out and error:\n#SBATCH --output=%x-%j.SLURMout\n\nexport PATH=/mnt/home/user123/anaconda3/bin:$PATH # this is just an example PATH; use your own conda installation\n\nconda activate tf_gpu_Feb2023 # again, activate your own TF env\n\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/lib/:/lib64/:$CONDA_PREFIX/lib/:$CONDA_PREFIX/lib/python3.9/site-packages/tensorrt_libs\nexport XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib\n\npython matmul.tf2.py\n\nconda deactivate\n</code></pre> <p>To submit it,  run <code>sbatch testTF.sbatch</code> from the command line.  The final result will be written to the file <code>test_matmul-&lt;jobid&gt;.SLURMout</code>.</p>"},{"location":"TF-GPU/","title":"TF GPU usage","text":"<p>Authorship</p> <p>This guide was written by Siddak Marwaha (ICER student intern from MSU Astrophysics and Data Science, Spring 2023).</p>","tags":["TensorFlow","GPU"]},{"location":"TF-GPU/#tensorflow-gpu-usage","title":"TensorFlow GPU Usage","text":"","tags":["TensorFlow","GPU"]},{"location":"TF-GPU/#introduction","title":"Introduction","text":"<p>HPCC provides GPU resources for machine learning tasks. GPUs can accelerate the training and inference of deep learning models, allowing for faster experimentation and better performance. TensorFlow is a popular open-source machine learning framework that supports GPU acceleration. This guide will walk you through the steps of utilizing GPU resources on HPCC using TensorFlow.</p>","tags":["TensorFlow","GPU"]},{"location":"TF-GPU/#setup","title":"Setup","text":"<p>Ensure you have the latest TensorFlow GPU release installed.</p> <pre><code>import tensorflow as tf\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n</code></pre> <p>TensorFlow can perform computations on different types of devices, including CPUs and GPUs. These devices are identified by specific names, such as <code>/device:CPU:0</code> for the CPU and <code>/GPU:0</code> for the first visible GPU, <code>CPU:1</code> and <code>GPU:1</code> for the second and so on.</p> <p>When running TensorFlow operations that have both CPU and GPU implementations, the GPU device is prioritized by default. For example, if you have both a CPU and a GPU available, the GPU will be used to run the operation, unless you specifically request to use the CPU instead. However, if an operation doesn't have a corresponding GPU implementation, then it will fall back to the CPU device. For example, if you have a CPU and a GPU, and you're running an operation that only has a CPU implementation, the operation will run on the CPU even if you requested to run it on the GPU.</p>","tags":["TensorFlow","GPU"]},{"location":"TF-GPU/#logging-device-placement","title":"Logging device placement","text":"<p>The following code sets a TensorFlow option to log the device used to run each operation, then creates two matrices (<code>a</code> and <code>b</code>) and multiplies them using TensorFlow's built-in matrix multiplication function (<code>tf.matmul</code>). The result of the multiplication is printed to the console. By setting the log option, we can see which device (CPU or GPU) is used to perform the computation:</p> <pre><code>tf.debugging.set_log_device_placement(True)\n\n# Create some tensors\na = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nb = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\nc = tf.matmul(a, b)\n\nprint(c)\n</code></pre> <p>The expected result for GPU used is:</p> <pre><code>Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\ntf.Tensor(\n[[22. 28.]\n [49. 64.]], shape=(2, 2), dtype=float32)\n</code></pre>","tags":["TensorFlow","GPU"]},{"location":"TF-GPU/#manual-device-placement","title":"Manual device placement","text":"<p>If you want to choose a specific device for an operation instead of letting TensorFlow automatically select it for you, you can use the tf.device function. This creates a context where all operations inside it will run on the same device you choose. However, by default TensorFlow will use a GPU if it is available and configured properly.</p> <p>Again, the following code sets <code>log_device_placement</code> to True, which will cause TensorFlow to print the assigned device for each operation. Then, it places two constant tensors 'a' and 'b' on the CPU using the <code>with tf.device('/CPU:0')</code> block. Finally, it multiplies 'a' and 'b'. This demonstrates how to explicitly place tensors on specific devices and how TensorFlow prioritizes GPU over CPU when both are available:</p> <pre><code>tf.debugging.set_log_device_placement(True)\n\n# Place tensors on the CPU\nwith tf.device('/CPU:0'):\n  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n\n# Run on the GPU\nc = tf.matmul(a, b)\nprint(c)\n</code></pre>","tags":["TensorFlow","GPU"]},{"location":"TF-GPU/#limiting-tf-to-certain-gpus","title":"Limiting TF to certain GPUs","text":"<p>If you want to use only certain GPUs, you can use the <code>tf.config.set_visible_devices</code> method to limit TensorFlow to those GPUs. This can help avoid memory fragmentation and ensure that the specific GPUs you want to use are available. </p> <p>The following code checks if there are any GPUs available on the system by listing the physical devices. If there are GPUs available, it restricts TensorFlow to only use the first GPU by setting it as the visible device. It also lists the logical devices to confirm the GPU usage. If there are any errors, such as the visible devices being set after the GPUs have already been initialized, it will catch the error and print it. The purpose of this code is to manage the available GPUs and ensure that TensorFlow uses them efficiently:</p> <pre><code>gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  # Restrict TensorFlow to only use the first GPU\n  try:\n    tf.config.set_visible_devices(gpus[0], 'GPU')\n    logical_gpus = tf.config.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n  except RuntimeError as e:\n    # Visible devices must be set before GPUs have been initialized\n    print(e)\n</code></pre> <p>Example output:</p> <pre><code>1 Physical GPUs, 1 Logical GPU\n</code></pre> <p>Note</p> <p>Physical vs Logical devices: physical devices refer to the actual hardware components such as a GPU or CPU that are present in the system. On the other hand, logical devices refer to the virtual representations of these physical devices that are exposed to TensorFlow for computation. When TensorFlow is initialized on a machine with GPUs, it detects the available physical devices and creates a logical device for each physical device. Each logical device can have multiple components, such as a GPU with multiple cores, and it may also have a subset of the memory of the physical device. Logical devices are used by TensorFlow to distribute and manage the computation across the available physical devices in an efficient manner.</p>","tags":["TensorFlow","GPU"]},{"location":"TF-GPU/#using-a-single-gpu-on-a-multi-gpu-system","title":"Using a single GPU on a multi-GPU system","text":"<p>If you have multiple GPUs, TensorFlow will use the one with the lowest ID number by default. If you want to use a different GPU, you need to tell TensorFlow which one to use specifically. The following code attempts to perform a matrix multiplication operation between two TensorFlow constant tensors using a non-existent GPU device <code>/device:GPU:4</code>. Since this device does not exist, it should raise a RuntimeError exception. The code also sets <code>tf.debugging.set_log_device_placement(True)</code> to log the placement of operations on devices. So, if the program runs successfully, it will log which device the operation ran on. If you try to run operations on a specific GPU device that does not exist, you will get a runtime error. </p> <pre><code>tf.debugging.set_log_device_placement(True)\n\ntry:\n  # Specify an invalid GPU device\n  with tf.device('/device:GPU:4'):\n    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n    c = tf.matmul(a, b)\nexcept RuntimeError as e:\n  print(e)\n</code></pre> <p>You can use <code>tf.config.set_soft_device_placement(True)</code> to instruct TensorFlow to automatically choose a supported device to run the operations in case the specified device is not available. This can help make your code more flexible and robust in case the availability of GPU devices changes over time.</p> <p>Note</p> <p>Eager vs Graph execution modes: since TensorFlow 2.0, the eager execution is the default and soft device placement is enabled by default when running in eager mode. Therefore, with the above code snippet running in the eager mode, you won't get an error even without having <code>tf.config.set_soft_device_placement(True)</code>. However, for complex model training, graph execution has the advantages of being faster, more flexible, and robust. If you opt to use it, you will need to enable soft device placement.</p> <p>The first two lines of the following code enable TensorFlow to choose a device to run operations on, and then log where each operation is executed:</p> <pre><code>tf.config.set_soft_device_placement(True)\ntf.debugging.set_log_device_placement(True)\n\n# Creates some tensors\na = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nb = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\nc = tf.matmul(a, b)\nprint(c)\n</code></pre>","tags":["TensorFlow","GPU"]},{"location":"TF-GPU/#using-multiple-gpus","title":"Using multiple GPUs","text":"<p>Developing machine learning models to work with multiple GPUs allows the model to use additional resources and potentially scale better. However, if you only have a single GPU available, you can still simulate multiple GPUs using virtual devices. This makes it easier to test and develop for multi-GPU setups without needing additional physical GPUs.</p> <p>The following code is creating two virtual GPUs with 1GB memory each. It first lists the physical GPUs available on the system using <code>tf.config.list_physical_devices('GPU')</code>. If there are GPUs available, it uses <code>tf.config.set_logical_device_configuration()</code> to create two logical devices with a memory limit of 1024 MB (1GB) each on the first physical GPU. The code then lists the logical GPUs created with <code>tf.config.list_logical_devices('GPU')</code> and prints the number of physical and logical GPUs. If there is an error setting up the virtual devices, the error message is printed:</p> <pre><code>gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  # Create 2 virtual GPUs with 1GB memory each\n  try:\n    tf.config.set_logical_device_configuration(\n        gpus[0],\n        [tf.config.LogicalDeviceConfiguration(memory_limit=1024),\n         tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n    logical_gpus = tf.config.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Virtual devices must be set before GPUs have been initialized\n    print(e)\n</code></pre> <p>Output:</p> <pre><code>1 Physical GPU, 2 Logical GPUs\n</code></pre> <p>This output indicates that there is one physical GPU available on the system, and two logical GPUs have been created on that physical GPU using virtual devices. Each logical GPU has a memory limit of 1GB. The code successfully created the virtual devices without any errors. </p> <p>Once there are multiple logical GPUs available to the runtime, you can utilize the multiple GPUs with <code>tf.distribute.Strategy</code> or with manual placement.</p>","tags":["TensorFlow","GPU"]},{"location":"TF-GPU/#using-tfdistributestrategy","title":"Using tf.distribute.Strategy","text":"<p>The best practice for using multiple GPUs is to use <code>tf.distribute.Strategy</code>. The next code sets up a mirrored strategy for training a neural network model on multiple GPUs. It first enables device placement logging, then lists the logical GPUs available to the runtime. It creates a <code>MirroredStrategy</code> object, which distributes the training across multiple GPUs. The <code>with strategy.scope()</code> block defines the model architecture and compiles it with a mean squared error loss and stochastic gradient descent optimizer:</p> <pre><code>tf.debugging.set_log_device_placement(True)\ngpus = tf.config.list_logical_devices('GPU')\nstrategy = tf.distribute.MirroredStrategy(gpus)\nwith strategy.scope():\n  inputs = tf.keras.layers.Input(shape=(1,))\n  predictions = tf.keras.layers.Dense(1)(inputs)\n  model = tf.keras.models.Model(inputs=inputs, outputs=predictions)\n  model.compile(loss='mse',\n                optimizer=tf.keras.optimizers.SGD(learning_rate=0.2))\n</code></pre> <p>The program is using multiple GPUs to process the data faster by splitting the input data between the GPUs and running a copy of the model on each GPU. This approach is called \"data parallelism\".</p>","tags":["TensorFlow","GPU"]},{"location":"TF-GPU/#manual-placement","title":"Manual placement","text":"<p><code>tf.distribute.Strategy</code> is a tool that allows you to replicate your model on multiple devices, which can improve performance. You can also achieve the same thing manually by building your model on each device. The following program demonstrates how to manually replicate computation across multiple GPUs. It creates copies of a matrix multiplication operation on each available GPU and then adds the results of those computations on the CPU to obtain the final result. It also uses <code>tf.debugging.set_log_device_placement(True)</code> to print the placement of each operation to the console for debugging purposes:</p> <pre><code>tf.debugging.set_log_device_placement(True)\ngpus = tf.config.list_logical_devices('GPU')\nif gpus:\n  # Replicate your computation on multiple GPUs\n  c = []\n  for gpu in gpus:\n    with tf.device(gpu.name):\n      a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n      b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n      c.append(tf.matmul(a, b))\n  with tf.device('/CPU:0'):\n    matmul_sum = tf.add_n(c)\n  print(matmul_sum)\n</code></pre> <p>Example Output:</p> <pre><code>Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op AddN in device /job:localhost/replica:0/task:0/device:CPU:0\ntf.Tensor(\n[[ 88. 112.]\n [196. 256.]], shape=(2, 2), dtype=float32)\n</code></pre>","tags":["TensorFlow","GPU"]},{"location":"TFcode/","title":"TensorFlow model training code for testing","text":"<p>This page assumes that you've followed the instructions to install TensorFlow using conda and successfully installed TF in your conda environment. Below we provide more TF model training code for you to fully test your installation. Remember to log onto <code>dev-amd20-v100</code>.</p> <p>All the code should be typed in (or copy-paste) to an interactive python interpreter, after running the first four lines below from your terminal. After you are done and have quit the python session, remember to deactivate your conda environment via <code>conda deactivate</code>.</p> <pre><code>export PATH=/mnt/home/user123/anaconda3/bin:$PATH\nconda activate tf_gpu_Feb2023\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/lib/:/lib64/:$CONDA_PREFIX/lib/:$CONDA_PREFIX/lib/python3.9/site-packages/tensorrt_libs\nexport XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib\n\npython\n\n# Here insert your python code after &gt;&gt;&gt; the prompt of the interactive Python interpreter\n\nconda deactivate\n</code></pre> <p>Code 1</p> <pre><code>import tensorflow as tf\nimport numpy as np\n\n# Generate random data\nx = np.random.rand(100, 10)\ny = np.random.randint(0, 2, size=(100,))\n\n# Define the model architecture\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n  tf.keras.layers.Dense(32, activation='relu'),\n  tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x, y, epochs=10, validation_split=0.2)\n\n# Evaluate the model on test data\ntest_loss, test_acc = model.evaluate(x, y, verbose=2)\nprint(f'Test accuracy: {test_acc}')\n</code></pre> <p>Code 2</p> <pre><code>import tensorflow as tf\nmnist = tf.keras.datasets.mnist\n\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10, activation='softmax')\n])\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=5)\nmodel.evaluate(x_test, y_test)\n</code></pre> <p>Code 3</p> <pre><code>import tensorflow as tf\nfrom tensorflow.keras.datasets import boston_housing\n\n(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Dense(64, activation='relu', input_shape=(13,)),\n  tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae'])\n\nhistory = model.fit(x_train, y_train, epochs=10, batch_size=16, validation_split=0.2)\n\ntest_loss, test_mae = model.evaluate(x_test, y_test)\n\nprint(f'Test MAE: {test_mae:.2f}')\n</code></pre> <p>Special thanks to Siddak Marwaha who provided the above code.</p>","tags":["how-to guide","TensorFlow"]},{"location":"Targeting_Cluster_Architectures/","title":"Targeting Cluster Architectures","text":"<p>While all HPCC nodes are the x86_64 architecture, some newer processors have features that are not supported by older processors. A program compiled on a newer processor may not run on an older processor and may result in an 'Illegal Instruction' error. This can be corrected by specifying compilers parameters that control which processor instruction are used.</p> <p>Use the following options for your compiler to ensure your programs will run on all HPCC nodes.</p> Compiler Type Min Version Max Version Arguments GCC 6.4 N/A -march=core-avx-i -mtune=skylake-avx512 GCC 4.9 \\&lt; 6.4 -march=core-avx-i -mtune=silvermont GCC 4.8 \\&lt; 4.9 -march=core-avx-i -mtune=core-avx2 GCC 4.6 \\&lt; 4.8 -march=core-avx-i GCC 4.3 \\&lt; 4.6 -march=core2 GCC 3.3 \\&lt; 4.3 -march=nocona Intel 2015.1 N/A -mAVX -axCORE-AVX-I,CORE-AVX2,CORE-AVX512 <p>The new amd20 cluster does not support AVX-512.</p>"},{"location":"Torque_vs._SLURM/","title":"Migrating to SLURM from TORQUE","text":"<p>While the HPCC uses the SLURM resource manager, users may be familiar with the TORQUE resource manager from  running jobs on other systems. </p> <p>SLURM handles resource requests slightly differently than TORQUE. TORQUE frames requests in terms of CPUs; for instance, <code>#PBS -l ppn=&lt;count&gt;</code> specifies the desired number of CPUs (processors) per node. On the other hand, SLURM frames requests in terms of tasks; e.g., <code>#SLURM --ntasks-per-node=&lt;count&gt;</code>. A task may then have multiple CPUs assigned to it for the purposes of threading: <code>#SBATCH --cpus-per-task=&lt;count&gt;</code>.</p> <p>We recommend reviewing our pages on writing and submitting job scripts, the list of SLURM resource specifications, and our example SLURM scripts to help with the transition to SLURM. Users may also benefit from this side by side comparison between TORQUE and SLURM options, environment variables, and commands.</p>","tags":["explanation","torque","slurm"]},{"location":"Transferring_Data_with_Google_using_Globus/","title":"Transferring Data with Google using Globus","text":""},{"location":"Transferring_Data_with_Google_using_Globus/#initial-setup","title":"Initial setup","text":"<p>Go to the Globus File Manager page and log in with your MSU credentials if necessary. For this example, we will transfer files between the ICER's HPCC and MSU Google endpoints. First, connect to your drive space on ICER's HPCC. Then, in the search box (black arrow), enter msu#google.</p> <p></p> <p>Select the endpoint msu#google.</p> <p></p> <p>At this point you may be asked for permission to allow Globus access to your Google drive.</p> <p></p> <p>Click Continue (orange oval):</p> <p></p> <p>Click the wrench under status heading (orange oval).</p> <p></p> <p>Click Allow (orange oval).</p> <p></p> <p>You will see your status changed (orange oval). You now have access to your Google Drive. Click File Manager in the upper left corner (black oval) to return to the File Manager main page.</p> <p></p> <p>You now can transfer files between your ICER HPCC drive space and your Google drive space, and you will see their respective contents in a split screen. You can transfer folders and files using the two methods, 1)\u00a0Drag and drop (not shown here), or 2)\u00a0Transfer &amp; Sync Options (orange oval and black arrow).</p> <p></p> <p>Please note that Google enforces a transfer limit per person per minute. If you see warnings that connection limit has been reached the transfer will still work but be slower. Also, by default, Globus does not allow access to your Google files, as seen in the message received if you attempt to share (purple arrow above) a file or folder from your Google Drive.</p> <p></p> <p>While it is possible to allow other users to access your collection from Google Drive if you change the permissions, it is not recommended for data security reasons.</p>"},{"location":"Transferring_Data_with_Google_using_Globus/#transferring-from-google-shared-filesdrives","title":"Transferring from Google shared files/drives.","text":"<p>It is also possible to transfer files/folders that are in the \"Shared with me\" section of Google Drive, or from \"Shared Drives\".   Globus by default shows your own Google drive files, but you can access those shared files/drives by clicking the \"up a level\" button in the file browser circled in the following screen shot.  The \"Path\" box should be just a single slash \"/\" and you will see \"My Drive\", \"Shared With Me\" and \"Team Drives.\"   \"Team Drives\" is the same as \"Shared Drives\" (Google's previous name for Shared Drives).</p> <p></p>"},{"location":"Transferring_data_with_Globus/","title":"Transferring data with Globus","text":"","tags":["how-to guide","globus"]},{"location":"Transferring_data_with_Globus/#what-is-globus","title":"What is\u00a0Globus?","text":"<p>Globus is a\u00a0free service to the MSU community\u00a0for secure, reliable research data management. Globus\u00a0gives\u00a0users\u00a0the ability to\u00a0move\u00a0and\u00a0share data\u00a0regardless of\u00a0user or\u00a0file location\u00a0through a single web browser-based interface.\u00a0Users\u00a0can manage data from any device (e.g.,\u202fsupercomputer, tape archive, lab cluster or equipment, public cloud, or personal computer/laptop) from anywhere in the world\u00a0using\u00a0their\u00a0existing\u00a0institutional\u00a0identities.\u00a0Use of Globus\u00a0removes data management roadblocks by providing unified access to all storage locations, making\u00a0it easy\u00a0to work with data while ensuring reliability\u00a0and security.\u00a0For more information on\u00a0Globus, check out  the Globus website here.</p>","tags":["how-to guide","globus"]},{"location":"Transferring_data_with_Globus/#why-should-i-use-globus","title":"Why\u00a0should\u00a0I use\u00a0Globus?","text":"<p>Globus is ideal for moving large files and data transfers between ICER\u2019s HPCC or external research collaborators\u00a0because of its truly fire-and-forget method of transferring data.\u00a0After you\u00a0initiate\u00a0a file transfer,\u00a0Globus will work on your behalf to optimize transfer performance, monitor for transfer completion and correctness, and recover from network errors, credential expiration, and collection downtime without restarting the transfer. This allows you to\u00a0navigate away from File Manager, close the browser window, and even logout.\u00a0</p>","tags":["how-to guide","globus"]},{"location":"Transferring_data_with_Globus/#transferring-data-with-globus_1","title":"Transferring data with Globus","text":"<p>The HPCC has a Globus data transfer endpoint, <code>msuhpcc</code>. This can be used to do large data transfers to/from your personal computer, to/from collaborators or to/from external HPC sites. \u00a0You can also use it to share data.</p> <p>With Globus, you can create a transfer (to or from) between your computer and a folder on the HPCC which you have access to. This could be a folder in your home, research or scratch spaces.</p> <p>You can also create Globus \"shares\" to your external colleagues. They can use your created link to access the HPCC directory. **You can not create a globus share on any folder in your home directory.\u00a0** You can only share folders in scratch space or any research space you have access to.\u00a0\u00a0 \u00a0</p> <p>To use Globus Online, please perform the following steps:</p> <ol> <li>If you do not have a globus account, create one     at\u00a0https://www.globus.org</li> <li>Log into the MSU Globus Online portal,     https://globus.msu.edu\u00a0and set up\u00a0a free Globus     Online account.</li> <li>If you wish to use Globus to transfer data to/from your local     computer, install the Globus Connect Personal tool.</li> <li>On the Globus\u00a0Start Transfer page,      enter <code>msuhpcc</code>\u00a0as the end point on one side. This will pull up an authentication window. Use your MSU NetID for the     username, and your MSU NetID password for the passphrase. This will     set up an authenticated session that will last as long as specified     in the Credential Lifetime field, up to a limit of 2 weeks.</li> <li>Select and authenticate with the other endpoint for your transfer,     and initiate your transfer. An example can be seen from     How To Transfer Files with Globus.</li> <li>To share data in a HPCC folder accessible to you with other     persons,\u00a0 please check     How To Share Data Using Globus.</li> </ol> <p>More information about using Globus can be found on\u00a0Data transfer and sharing using Globus or Globus support page.</p> <p>For further training on the Science DMZ and how to use Globus, please self-enroll in ICER's DMZ Globus Training\u00a0D2L course.</p>","tags":["how-to guide","globus"]},{"location":"Transferring_data_with_Globus/#icers-dmz-globus-training-globus-walkthrough-documents-optional","title":"ICER's DMZ Globus Training - Globus Walkthrough Documents (optional)","text":"<ol> <li> <p>Find and Connect to HPCC using Globus</p> </li> <li> <p>Transfer from PC to HPCC using Globus</p> </li> <li> <p>Transferring Data between Endpoints Using Globus</p> </li> <li> <p>Transferring Data with Google using Globus</p> </li> <li> <p>Sharing Data using Globus</p> </li> </ol>","tags":["how-to guide","globus"]},{"location":"Trimmomatic/","title":"Trimmomatic","text":"<p>Trimmomatic is a tool for trimming Illumina FASTQ data and removing adapters. \u00a0When data is sequenced on Illumina, adapters are added for the fragments to attach to the beads. \u00a0If these adapters are not removed they can result in false assembly or other issues. \u00a0Additionally, the quality of the sequences varies across the length of the read, and poorer quality regions can be trimmed using Trimmomatic. \u00a0Running Trimmomatic is a good first step in quality filtering your Illumina data.\u00a0</p> <p>To run it on the HPCC (for example trimming paired-end reads):</p> <pre><code>java -jar /opt/software/Trimmomatic/0.36-Java-1.8.0_92/trimmomatic-0.36.jar PE [-threads &lt;threads] [-phred33 | -phred64] [-trimlog &lt;logFile&gt;] &lt;input 1&gt; &lt;input 2&gt; &lt;paired output 1&gt; &lt;unpaired output 1&gt; &lt;paired output 2&gt; &lt;unpaired output 2&gt; &lt;step 1&gt; ...\n</code></pre> <p>Read\u00a0the manual\u00a0for how to use it. Note that\u00a0the adapter sequence files are in\u00a0<code>/opt/software/Trimmomatic/0.36-Java-1.8.0_92/adapters/</code>. If you couldn't find the adapters you need in that directory, you will need to obtain them from elsewhere (for example asking the person who ran the library prep for you).</p>"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/","title":"Trinity for RNA-seq de novo assembly","text":""},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#loading-module","title":"Loading module","text":"<p>Take loading Trinity 2.6.6 as an example, we run:</p> <pre><code>module purge\nmodule load icc/2017.4.196-GCC-6.4.0-2.28 impi/2017.3.196 Trinity/2.6.6\n</code></pre>"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#most-basic-run-transcript-assembly","title":"Most basic run (transcript assembly)","text":"<p>A typical Trinity command for assembling strand-specific paired-end RNA-seq data would look like:</p> <p>A typical run of Trinity</p> <pre><code>Trinity \\\n  --seqType fq \\\n  --max_memory 2G \\\n  --left reads.left.fq \\\n  --right reads.right.fq \\\n  --SS_lib_type RF \\\n  --CPU 10\n</code></pre> <p>This will generate output files in a new directory <code>trinity_out_dir</code> in the working directory. Among them, the assembled transcripts file is \"<code>Trinity.fasta</code>\". For more detail, check out\u00a0https://github.com/trinityrnaseq/trinityrnaseq/wiki.</p> <p>When you submit the above command as a job to the cluster, you need to request 10 CPUs in the sbatch script with the following lines (in addition to your other sbatch directives):</p> <p>sbatch code snippet</p> <pre><code>#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=10\n</code></pre>"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#transcript-quantification","title":"Transcript quantification","text":"<p>Trinity provides abundant utility scripts for post-assembly analysis, such as quality assessment, transcript quantification and differential expression tests. For some of them, external software tools need to be installed separately (that is, they are not bundled with Trinity). For example, for the transcript quantification step, we will need one of RSEM, eXpress, kalllisto and salmon (cf.\u00a0https://github.com/trinityrnaseq/trinityrnaseq/wiki/Trinity-Transcript-Quantification). We have made all these four available on the HPCC. As instructed by Trinity, \"the tools should be available via your PATH setting\". So, in the next example where we choose to use RSEM to align reads to the assembled transcript and then quantify transcript abundance, we first set the\u00a0<code>PATH</code>\u00a0variable so that RSEM can be automatically searched for by trinity.</p> <p>Using RSEM for transcript quantification</p> <pre><code># Assuming\n#    1) you've loaded Trinity module already and\n#    2) your current working directory is trinity_out_dir generated from the previous assembly step. \n\nexport PATH=/opt/software/RSEM/1.3.1-GCCcore-6.4.0/usr/local/bin:$PATH\n\n/opt/software/Trinity/2.6.6/util/align_and_estimate_abundance.pl --seqType fq --transcripts Trinity.fasta \\\n    --est_method RSEM \\\n    --left ../reads.left.fq \\\n    --right ../reads.right.fq \\\n    --SS_lib_type RF \\\n    --aln_method bowtie \\\n    --trinity_mode \\\n    --prep_reference \\\n    --thread_count 10 \\\n    --output_dir RSEM_out\n</code></pre> <p>The RSEM computation generates two primary output files containing estimated abundances in the subdirectory\u00a0<code>RSEM_out</code>\u00a0as specified in the command above:\u00a0<code>RSEM.isoforms.results</code>\u00a0(transcript level) and\u00a0<code>RSEM.genes.results</code>\u00a0(gene level).</p>"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#more-utilities","title":"More utilities","text":"<p>Please consult\u00a0https://github.com/trinityrnaseq/trinityrnaseq/wiki for detail.</p> <p>Note that a few R packages are needed for differential expression analysis (https://github.com/trinityrnaseq/trinityrnaseq/wiki/Trinity-Differential-Expression). These have been installed in\u00a0<code>R/4.0.2</code>\u00a0which can be loaded by</p> <pre><code>module purge; module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2\n</code></pre>"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#version-note","title":"Version note","text":"<p>The latest version is 2.91. After loading it, you may load R 4.0.2 for DE analysis.</p> <p><code>module purge</code> <code>module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 Trinity/2.9.1</code> <code>module load R/4.0.2</code></p>"},{"location":"User_Created_Modules/","title":"User Created Modules","text":"<p>Info</p> <p>This is for advanced usage, since creating your own module files for software access is usually not necessary.</p> <p>If you develop or install your own software, you might consider writing a modulefile to help manage your environment variables. HPCC presently uses the LMOD module package, developed at TACC. The following is a typical module file with comments. Name your files with a <code>.lua</code> extension.</p> <pre><code>-- -*- lua -*- \n help( \n [[ \n Describe your software here. \n ]]) \n\n -- comments are prefaced with two dashes \n\n whatis(\"Description: Name of software\") \n whatis(\"URL:  www.ucc.org \") \n\n local install_path = \"/mnt/home/ongbw/opt/mysoftware\" \n\n -- set an environment variable \n setenv(\"MYSOFTWARE_HOME\",install_path) \n\n -- add to PATH variable \n prepend_path('PATH', pathJoin(install_path,\"bin\")) \n\n -- add Library Paths \n prepend_path('LD_LIBRARY_PATH',pathJoin(install_path,\"lib\")) \n prepend_path('LIBRARY_PATH',pathJoin(install_path,\"lib\")) \n\n -- add include paths \n prepend_path('INCLUDE',pathJoin(install_path,\"include\"))\n</code></pre> <p>For more information, see the Lmod documentation</p>","tags":["reference","modules"]},{"location":"Using_Python_in_HPCC_with_virtualenv/","title":"Using Python in HPCC with virtualenv","text":"<p>Python applications usually use packages and modules that require specific versions of libraries. This means one installed application may conflict with another application due to using the same library but with different versions. It is difficult to meet the requirements of every application by one global Python installation. To resolve this issue, users can create an isolated virtual environment with a particular version of Python in a self-contained directory of their home or research space. Any package and the dependent libraries installed inside the directory can be available only through the virtual environment. Different applications can then use different virtual environments to avoid any conflict.</p>"},{"location":"Using_Python_in_HPCC_with_virtualenv/#create-and-use-virtual-environments","title":"Create and use virtual environments","text":"<p>To create python virtual environments, please make sure your preferred version of Python is loaded. It is also a good idea to create a directory of the python version to store different environments and their applications:</p> <pre><code>[UserName@dev-intel18 ~]$ module list Python\n\nCurrently Loaded Modules Matching: Python\n  1) Python/3.6.4\n\n[UserName@dev-intel18 ~]$ which python\n/opt/software/Python/3.6.4-foss-2018a/bin/python\n\n[UserName@dev-intel18 ~]$ mkdir Python3.6.4\n[UserName@dev-intel18 ~]$ cd Python3.6.4\n[UserName@dev-intel18 Python3.6.4]$\n</code></pre> <p>Currently, two common tools can be used to create Python virtual environments. Please use only one of them:</p> <ol> <li> <p>venv is available for Python 3.3 and later by default. The application <code>pip</code> and <code>setuptools</code> should be ready to use in HPCC system. A virtual environment can be created by running \"<code>python3 -m venv &lt;DIR&gt;</code>\", where <code>&lt;DIR&gt;</code> is the directory of the created environment. Following is an example of the command, and the directory <code>tutorial</code> is created for the virtual environment.</p> <pre><code>[UserName@dev-intel18 Python3.6.4]$ python3 -m venv tutorial  \n[UserName@dev-intel18 Python3.6.4]$ ls tutorial  \nbin  include  lib  lib64  pyvenv.cfg\n</code></pre> </li> <li> <p>virtualenv supports all Python versions. By default, HPCC system has     <code>pip</code>, <code>setuptools</code> and <code>wheel</code> installed and available. Similarly     to venv, a virtual environment can be created by executing     <code>virtualenv &lt;DIR&gt;</code>, where applications of\u00a0the virtual environment are installed in <code>tutorial</code> directory. <pre><code>    [UserName@dev-intel18 Python3.6.4]$ virtualenv tutorial\n    Using base prefix '/opt/software/Python/3.6.4-foss-2018a'\n    New python executable in /mnt/home/UserName/Python3.6.4/tutorial/bin/python\n    Installing setuptools, pip, wheel...done.\n    [UserName@dev-intel18 Python3.6.4]$ ls tutorial\n    bin  include  lib  lib64  pip-selfcheck.json\n</code></pre></p> </li> </ol> <p>Please use one of them to create the virtual environment.</p> <p>The created <code>tutorial</code> environment can now be used by sourcing the script file <code>activate</code> under the <code>bin</code> directory:</p> <pre><code>[UserName@dev-intel18 Python3.6.4]$ source tutorial/bin/activate\n(tutorial) [UserName@dev-intel18 Python3.6.4]$\n</code></pre> <p>where the name inside the\u00a0parentheses <code>(tutorial)</code> in front of the prompt line shows the current Python environment. To leave the environment, just run \"deactivate\":</p> <pre><code>(tutorial) [UserName@dev-intel18 Python3.6.4]$ deactivate\n[UserName@dev-intel18 Python3.6.4]$\n</code></pre> <p>and the parentheses disappear. Any time you want to use the <code>tutorial</code> environment. Simply source the file again: <code>source ~/Python3.6.4/tutorial/bin/activate</code> and the environment is back. More information can be found about venv or virtualenv.</p>"},{"location":"Using_Python_in_HPCC_with_virtualenv/#install-packages-from-pypi-using-pip","title":"Install packages from PyPI using pip","text":"<p>The most common usage of pip is to install python packages from the Python Package Index with a requirement specifier. Users can also check other usages with pip. Below, some of the common usage scenarios are introduced.</p> <p>To install the latest version of a python package, users can run <code>pip install &lt;Package Name&gt;</code>, for example, using <code>sympy</code> as <code>&lt;Package Name&gt;</code>:</p> <pre><code>(tutorial) [UserName@dev-intel18 Python3.6.4]$ pip install \"sympy\"\nCollecting sympy\nUsing cached https://files.pythonhosted.org/packages/21/21/f4105795ca7f35c541d82c5b06be684dd2f5cb4f508fb487cd7aea4de776/sympy-1.4-py2.py3-none-any.whl\nCollecting mpmath&gt;=0.19 (from sympy)\nInstalling collected packages: mpmath, sympy\nSuccessfully installed mpmath-1.1.0 sympy-1.4\n</code></pre> <p>To install a specific version of a python package, please run <code>pip install &lt;Package Name&gt;==&lt;Version Number&gt;</code>. For example, install <code>numpy</code> with <code>&lt;version number&gt;</code> as <code>1.16.2</code>:</p> <pre><code>(tutorial) [UserName@dev-intel18 Python3.6.4]$ pip install \"numpy==1.16.2\"\nCollecting numpy==1.16.2\nDownloading https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n    |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17.3MB 10.5MB/s\nInstalling collected packages: numpy\nSuccessfully installed numpy-1.16.2\n</code></pre> <p>To upgrade an already installed package to the latest from PyPI, users can run <code>pip install --upgrade</code>. For example, upgrade the installed package <code>numpy</code>:</p> <pre><code>(tutorial) [UserName@dev-intel18 Python3.6.4]$ pip install --upgrade numpy\nCollecting numpy\nUsing cached https://files.pythonhosted.org/packages/e5/e6/c3fdc53aed9fa19d6ff3abf97dfad768ae3afce1b7431f7500000816bda5/numpy-1.17.2-cp36-cp36m-manylinux1_x86_64.whl\nInstalling collected packages: numpy\nFound existing installation: numpy 1.16.2\n    Uninstalling numpy-1.16.2:\n    Successfully uninstalled numpy-1.16.2\nSuccessfully installed numpy-1.17.2\n</code></pre> <p>With pip, you can also list all installed packages and their versions with the command <code>pip freeze</code>:</p> <pre><code>(tutorial) [UserName@dev-intel18 Python3.6.4]$ pip freeze\nmpmath==1.1.0\nnumpy==1.17.2\nsympy==1.4\n</code></pre> <p>For more detail, see the pip docs, which includes a complete Reference Guide.</p>"},{"location":"Using_Python_in_HPCC_with_virtualenv/#pythonpath-environment-variable","title":"PYTHONPATH environment variable","text":"<p>You can use the environment variable <code>PYTHONPATH</code> to include the packages already installed by the same python version in other directories. By adding the <code>site-packages</code> paths to <code>PYTHONPATH</code> environment variable and separating them with the \"<code>:</code>\" symbol:</p> <pre><code>(tutorial) [UserName@dev-intel18 Python3.6.4]$ export PYTHONPATH=~/Python3.6.4/tutorial/lib/python3.6/site-packages:/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages\n(tutorial) [UserName@dev-intel18 Python3.6.4]$ pip freeze\nabsl-py==0.5.0\nalabaster==0.7.12\nappdirs==1.4.3\nartemis==0.1.4\n...\nnose==1.3.7\nnumpy==1.17.2\nnumpydoc==0.8.0\n...\nsuspenders==0.2.6\nsympy==1.4\ntensorboard==1.10.0\ntensorflow==1.10.1\n...\nvirtualenv==15.1.0\nwcwidth==0.1.7\nWerkzeug==0.14.1\nxopen==0.3.5\n</code></pre> <p>all packages inside the paths are now ready to use. Please make sure the <code>site-packages</code> path of the current environment is set the first in\u00a0<code>PYTHONPATH</code> variable. If a package is installed in more than one path (possibly with different versions), the package of the first path showing in the variable (<code>PYTHONPATH</code>) will be used.</p>"},{"location":"Using_conda/","title":"Using Conda","text":"<p>Attention</p> <p>This wiki serves as a very limited introduction. ICER strongly  recommendeds reading the conda user guide  specific to the version of interest before you start using conda  on the HPCC. </p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_conda/#introduction","title":"Introduction","text":"<p>Conda is an open-source  package management system that installs and updates packages and their dependencies.  Conda also easily creates, saves, loads, and switches between environments on the  HPCC. It was created for Python programs but it can package and distribute software  for any language as a collection of 1,000+ open-source packages with free community support. The conda package and environment manager is included in all  versions of Anaconda.  </p> <p>Anaconda\u00a0was built to complement the rich,  open-source Python community, the Anaconda platform provides an enterprise-ready  data analytics platform that empowers researchers to adopt a modern, open data science analytics architecture. </p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_conda/#conda-on-the-hpcc","title":"Conda on the HPCC","text":"<p>Using Conda on the HPCC requires both a default component installed and maintained by the HPCC administrators via the Software module system and a user installed and maintained component. This dual component configuration is required to ensure system wide compliance for the user's customized Conda environments. Hence, users must install Anaconda in the home or research space to have full control of their Conda managed environments and packages; and users must also load the HPCC administrated Conda/3 module to allow Conda managed environments and packages to run smoothly on the HPCC.</p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_conda/#installing-anaconda-for-users","title":"Installing Anaconda for Users","text":"","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_conda/#anaconda-installation-script","title":"Anaconda Installation Script","text":"<p>To install the user managed component of Anaconda on the HPCC visit  www.anaconda.com/download/#linux  and follow the instructions below:</p> <ol> <li> <p>Find the link for the 64-bit (x86) Installer for Linux and copy the URL associated with this link (right-click on the download link and select \"copy link address\" or \"copy link location\"); For example, https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh.</p> </li> <li> <p>Login to the HPCC, login to a development node, navigate to the desired installation directory if it is somewhere other than your homespace,\u00a0then run the command <code>curl -O &lt;copied link address&gt;</code>     where <code>&lt;copied link address&gt;</code> above is to be replaced with the URL obtained in step 1.</p> </li> <li> <p>Once the Anaconda file is downloaded, run the command <code>bash &lt;AnacondaFileName.sh&gt;</code>      where <code>&lt;AnacondaFileName.sh&gt;</code> above is replaced with the name of the Anaconda      file downloaded with the <code>curl</code> command. For example, <code>Anaconda3-2022.05-Linux-x86_64.sh</code>.  </p> <p>During Installation you will need to:  </p> <ul> <li>Accept the license terms; the output will display <code>Do you accept the license terms [yes|no]?</code>  Type <code>yes</code> to accept; you must agree to install Anaconda</li> <li>Choose the installation location; the output will display <code>Anaconda3 will now be installed into this location: $HOME/anaconda3</code> <code>- Press ENTER to confirm the location</code> <code>- Press CTRL-C to abort the installation</code> <code>- Or specify a different location below</code>  (note: it may take a long while to complete this step)</li> <li>Choose to initialize conda; the output will display <code>Do you wish the installer to initialize Anaconda3</code> <code>by running conda init? [yes|no]</code> <code>[no] &gt;&gt;&gt;</code> Please type <code>no</code>, a more careful initialization is described after the installation instructions.</li> </ul> </li> </ol> <p>Note</p> <p>Please remember the directory where Anaconda was installed. This installation path will be used next. </p> <p>Upon successful installation, the output will display   </p> output<pre><code>You have chosen to not have conda modify your shell scripts at all.\nTo activate conda's base environment in your current shell session:\n\neval \"$(/$HOME/anaconda3/bin/conda shell.YOUR_SHELL_NAME hook)\" \n\nTo install conda's shell functions for easier access, first activate, then:\n\nconda init\n\nIf you'd prefer that conda's base environment not be activated on startup, \n   set the auto_activate_base parameter to false: \n\nconda config --set auto_activate_base false\n\nThank you for installing Anaconda3!\n</code></pre> <p>Note</p> <p>Please disregard this output. Users must manually configure Anaconda as described in the next section.</p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_conda/#editing-bashrc","title":"Editing <code>.bashrc</code>","text":"<p>To avoid conflicts between the user-installed Anaconda distribution and system-installed Python distributions, a modification of the <code>$HOME/.bashrc</code> file is necessary. The <code>.bashrc</code> file in the user's home space can be modified to set a specified environment every time you login to an HPCC node. You can modify the <code>.bashrc</code> file with an editor such as <code>vim</code> or <code>nano</code>.  To modify the <code>.bashrc</code> file for Anaconda installations please follow these steps:</p> <ol> <li> <p>Navigate to your home space and open the <code>.bashrc</code> file with an editor e.g., run  <code>cd $HOME</code> followed by <code>vim .bashrc</code>. Once in the vim editor, press the [i] key to enter \"--insert--\" mode</p> </li> <li> <p>Set the variable \"<code>CONDA3PATH</code>\" to the Anaconda3 installation directory by adding the command line  <code>export CONDA3PATH=&lt;Anaconda3 installation path&gt;</code>. Make sure to include the final <code>/</code> at the end of the path.</p> </li> <li> <p>Save the modified <code>.bashrc</code> file by pressing the [esc] key to exit     \"--insert--\"     mode; followed by <code>:wq</code> to save and quit vim.</p> </li> </ol> <p>Warning</p> <p>If there was a block of code in your <code>.bashrc</code> file that begins with <code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;</code> and ends with <code># &lt;&lt;&lt; conda initialize &lt;&lt;&lt;</code> and the lines between are not commented out (i.e., they did not begin with <code>#</code>s), please run <code>conda init --reverse</code> to remove these lines. Alternatively, you may comment them out yourself.</p> <p>Run <code>logout</code> to exit your SSH session on the development node, then reconnect with SSH to enable your changes.</p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_conda/#loading-the-conda-module","title":"Loading the Conda Module","text":"<p>As stated in the section Conda on the HPCC users must load the HPCC administered Conda module to ensure system-wide compliance when using and managing Conda environments and packages. To load the Conda/3 module login to a dev node and run the command    <code>module load Conda/3</code>     .</p> <p>If you would like to automatically load the Conda/3 module upon login, add the command    <code>module load Conda/3 2&gt; /dev/null</code>  to the\u00a0 <code>.bashrc</code> file after the  <code>export CONDA3PATH=...</code> command  </p> <p>Note</p> <p>Loading the Conda/3 module will also replace any loaded previously Python module to avoid conflicts.</p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_conda/#managing-conda","title":"Managing Conda","text":"<p>To ensure conda is properly installed and determine the  installed version, use the command <code>$ conda --version</code>  If properly installed, the conda version will be output to the display. For example,   <code>$ conda 22.11.0</code>.</p> <p>To update conda to the most recent version, use the command <code>$ conda update conda</code> </p> <p>The output will display text similar to     </p> output<pre><code>Collecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /mnt/home/parvizim/anaconda3\n\n  added / updated specs:\n    - conda\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    conda-22.11.0              |   py39h06a4308_0         872 KB\n    ruamel.yaml-0.16.12        |   py39h27cfd23_1         184 KB\n    ruamel.yaml.clib-0.2.6     |   py39h7f8727e_0         137 KB\n    ------------------------------------------------------------\n                                           Total:         1.2 MB\n\nThe following NEW packages will be INSTALLED:\n\n  ruamel.yaml        pkgs/main/linux-64::ruamel.yaml-0.16.12-py39h27cfd23_1\n  ruamel.yaml.clib   pkgs/main/linux-64::ruamel.yaml.clib-0.2.6-py39h7f8727e_0\n\nThe following packages will be UPDATED:\n\n  conda                               4.13.0-py39h06a4308_0 --&gt; 22.11.0-py39h06a4308_0\n\n\nProceed ([y]/n)? y\n\n\nDownloading and Extracting Packages\nruamel.yaml-0.16.12  | 184 KB    | ######################################################## | 100% \nruamel.yaml.clib-0.2 | 137 KB    | ######################################################## | 100% \nconda-22.11.0        | 872 KB    | ######################################################## | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n</code></pre> <p>Type <code>y</code> to continue with the update.</p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_conda/#managing-environments","title":"Managing Environments","text":"<p>To create a conda environment, use the command  </p> <p><code>conda create --name &lt;environment_name&gt;</code> </p> <p>where the text <code>&lt;environment_name&gt;</code> is to be replaced with the  name you choose. For example, to create an environment named \"myenv\" use the command    </p> <p><code>conda create --name myenv</code> </p> <p>The output will display    </p> output<pre><code>Collecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: $HOME/anaconda3/envs/myenv\n\n\n\nProceed ([y]/n)?    \n</code></pre> <p>Type <code>y</code> to create the environment. No packages have been installed in  this environment yet. </p> <p>To create a conda environment with pre-specified packages and/or versions  of Python, use the command above with additional arguments. Here are a  few examples that demonstrate the syntax:     </p> <p>Create an environment named \"bioenv\" with the \"biopython\" package  </p> <pre><code>conda create --name bioenv biopython\n</code></pre> <p>Create an environment named \"scienv\" with Python version 3.9 and version 1.6.0  of the \"scipy\" packages  </p> <pre><code>conda create --name scienv python=3.9 scipy=1.6.0\n</code></pre> <p>Create an environment named \"astroenv\" with version 1.6.0  of the \"scipy\" packages, the current \"asteroid\" package, and the current  \"bable\" packages  </p> <pre><code>conda create --name astroenv scipy=1.6.0 asteroid bable\n</code></pre> <p>To copy an existing environment, use the command    </p> <p><code>conda create --name &lt;new environment name&gt; --clone &lt;existing environment name&gt;</code> </p> <p>For example, the command    </p> <p><code>conda create --name newenv --clone myenv</code> </p> <p>will create a new environment named \"newenv\" that contains the same packages as the existing environment \"myenv\".    </p> <p>To display a list of all conda environments, use the command  </p> <p><code>conda info --envs</code> </p> <p>The output will display    </p> output<pre><code># conda environments:\n#\nastroenv                 $HOME/anaconda3/envs/astroenv\nbase                  *  $HOME/anaconda3\nbioenv                   $HOME/anaconda3/envs/bioenv\nmyenv                    $HOME/anaconda3/envs/myenv\nnewenv                   $HOME/anaconda3/envs/newenv\nscienv                   $HOME/anaconda3/envs/scienv\n</code></pre> <p>where an active environment is denoted with the <code>*</code> symbol.  </p> <p>To activate a conda environment, use the command     </p> <p><code>conda activate &lt;environment_name&gt;</code> </p> <p>The current environment should be in (parentheses) in front of the command prompt. For example, the command    </p> <p><code>conda activate astroenv</code> </p> <p>will result in the new command prompt <code>(astroenv) $</code>.    </p> <p>To switch to another environment, just use the <code>conda activate</code> command with the new environment name.    </p> <p>To deactivate the current conda environment, use the command  </p> <p><code>conda deactivate</code> </p> <p>To remove an environment, use the command</p> <p><code>conda remove --name &lt;name of environment&gt; --all</code></p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_conda/#managing-packages","title":"Managing Packages","text":"<p>To list all packages currently installed in an environment, first activate the environment then use the command     </p> <p><code>conda list</code> </p> <p>For example, after running the commands    </p> <p><code>conda activate scienv</code> <code>conda list</code> </p> <p>the output will display    </p> output<pre><code># packages in environment at $HOME/anaconda3/envs/scienv:\n#\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                        main  \n_openmp_mutex             5.1                       1_gnu  \nblas                      1.0                         mkl  \nca-certificates           2022.07.19           h06a4308_0  \ncertifi                   2022.6.15        py39h06a4308_0  \n... ...\n... ...\nzlib                      1.2.12               h7f8727e_2\n</code></pre> <p>To search for a package in the Anaconda repository that you would like to install, use the command    </p> <p><code>conda search &lt;package name&gt;</code>  where the <code>&lt;package name&gt;</code> is to be replaced with the name of package to search for. For example, the command    </p> <p><code>conda search beautifulsoup4</code> </p> <p>results in the output display    </p> output<pre><code>Loading channels: done\n# Name                       Version           Build  Channel             \nbeautifulsoup4                 4.6.0          py27_1  pkgs/main           \nbeautifulsoup4                 4.6.0  py27h3f86ba9_1  pkgs/main           \n... ...\n... ...          \nbeautifulsoup4                4.11.1  py38h06a4308_0  pkgs/main           \nbeautifulsoup4                4.11.1  py39h06a4308_0  pkgs/main\n</code></pre> <p>To install a package in the active environment, use the command    </p> <p><code>conda install &lt;package name&gt;</code> </p> <p>where <code>&lt;package name&gt;</code> is to be replaced with the name of package to install. For example, the command    </p> <p><code>(scienv) $ conda install beautifulsoup4</code> </p> <p>will output the display    </p> output<pre><code>Collecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: $HOME/anaconda3/envs/scienv\n\n  added / updated specs:\n    - beautifulsoup4\n\n\nThe following NEW packages will be INSTALLED:\n\n  beautifulsoup4     pkgs/main/linux-64::beautifulsoup4-4.11.1-py39h06a4308_0\n  soupsieve          pkgs/main/noarch::soupsieve-2.3.1-pyhd3eb1b0_0\n\n\nProceed ([y]/n)?\n</code></pre> <p>Type \u2018y\u2019 to install the package.</p> <p>To install a package in another environment, use the command    </p> <p><code>conda install --name &lt;environment name&gt; &lt;package name&gt;</code> </p> <p>where <code>&lt;environment name&gt;</code> is to be replaced with the name of the target environment and <code>&lt;package name&gt;</code> is to be replaced with the name of package to install. For example, the command    </p> <p><code>(scienv) $ conda install --name myenv beautifulsoup4</code> </p> <p>will install the package \"beautifulsoup4\" in the inactive environment \"myenv\".</p> <p>Note</p> <p>It is best to install all packages at once, so that all dependencies are installed at the same time.</p> <p>**Not all packages can be installed with the simple command ** <code>conda install</code>. Some packages may reside in a private package repository hosted by  Anaconda.org. As an example, we will illustrate the process using a public package called \u2018bottleneck\u2019.</p> <p>Use a web browser to visit the webpage anaconda.org and enter the text \"bottleneck\" into the search bar.</p> <p>Note</p> <p>To search for packages in a private repository you will have to 'Sign Up' and  'Sign In'.</p> <p></p> <p></p> <p>Choose the appropriate version, here we choose the most frequently downloaded, and click on the text \u2018bottleneck\u2019. </p> <p></p> <p>This will display all the information available on the package, including the commands used to install it. In this case, we want to install the standard \"bottleneck\" package via the \"conda-forge\" channel so we choose the command</p> <p><code>conda install -c conda-forge bottleneck</code></p> <p>where the <code>-c</code> flag designates the channel.</p> <p>If packages you are interested in are not available from conda or Anaconda.org use the 'pip' package manger within a conda environment via the command</p> <p><code>pip install &lt;package name&gt;</code></p> <p>where <code>&lt;package name&gt;</code> is to be replaced by the name of the desired package. For example, the commands   <pre><code>    $ conda activate scienv\n   (scienv) $ pip install see\n</code></pre></p> <p>will install the package \"see\" in the active environment \"scienv\".</p> <p>To update a package use the command</p> <p><code>conda update &lt;package name&gt;</code>.</p> <p>To remove a package from the active environment, use the command</p> <p><code>conda remove &lt;package name&gt;</code></p> <p>where <code>&lt;package name&gt;</code> is to be replaced by the name of the package to be removed.</p> <p>To remove a package from another environment, use the command    </p> <p><code>conda remove --name &lt;environment name&gt; &lt;package name&gt;</code> </p> <p>where <code>&lt;environment name&gt;</code> is to be replaced with the name of the target environment and <code>&lt;package name&gt;</code> is to be replaced with the name of package to be removed. For example, the command    </p> <p><code>(scienv) $ conda remove --name myenv beautifulsoup4</code> </p> <p>will remove the package \"beautifulsoup4\" in the inactive environment \"myenv\".</p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_conda/#using-conda-with-slurm","title":"Using conda with SLURM","text":"<p>You can activate a conda environment from within a SLURM Job Script. Include the <code>conda activate &lt;environment name&gt;</code> and <code>conda deactivate</code> commands in the 'bash command' portion of the SLURM job script. Ensure to first navigate into the directory where Anaconda is installed e.g., <code>cd $HOME</code>.</p> <pre><code>#!/bin/bash\n#SBATCH -t 60 # Runtime in minutes\n.\n.\n.\n#SBATCH --output=conda_%j.out # Standard out goes to this file\n\n# Load Conda module\nmodule load Conda/3\n# Activate conda environment and run job commands\ncd $HOME\nconda activate &lt;environment name&gt;\n&lt;bash command&gt;\n&lt;bash command&gt;\n.\n.\n.\n&lt;bash command&gt;\nconda deactivate\n</code></pre>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_the_Data_Machine/","title":"Using the Data Machine","text":"<p>This page acts as a reference for using some of the features of the Data Machine. For more general information on what the Data Machine can offer, please see the Data Machine overview.</p>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#table-of-data-machine-resources","title":"Table of Data Machine resources","text":"Node CPUs Memory Local NVME storage GPU GPU memory acm-048 acm-049 acm-070 acm-071 128 2 TB 32 TB nal-004 nal-005 nal-006 nal-007 128 512 GB 32 TB 4 NVIDIA A100 GPUs eachsplit into 7 allocatable units 10 GB (per unit) <p>Each GPU node has four GPUs each split into seven units that can be requested. Each of these units has 10 GB of memory. These units are requested similarly to normal GPUs. See the examples below.</p>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#running-code-on-the-data-machine","title":"Running code on the Data Machine","text":"<p>Though the Data Machine is not a buy-in node, the same procedures are used behind the scenes to run on Data Machine nodes. Therefore, users must be added to the <code>data-machine</code> buy-in account to run jobs on the Data Machine. To be added to this account please submit a request using ICER's contact form.</p> <p>Note</p> <p>The <code>data-machine</code> account is limited to Data Machine nodes only.</p>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#ondemand-data-machine-access","title":"OnDemand Data Machine access","text":"<p>Each OnDemand app has an \"Advanced Options\" checkbox. This opens additional form entries. To use the Data Machine nodes, enter <code>data-machine</code> in the SLURM Account  text box. Your job will queue onto a Data Machine node. Other resources (time,  CPU and memory) can be requested as usual by filling out the \"Number of hours\",  \"Number of cores per task\" and \"Amount of memory\" boxes.</p>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#gpu-access","title":"GPU access","text":"<p>To use GPUs with your OnDemand session, enter a number into the Number of GPUs  box. This will allocate the requested number of GPU units.</p>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#slurm-scripting-data-machine-access","title":"SLURM scripting Data Machine access","text":"<p>Below are some examples of SLURM resource requests for the Data Machine.</p>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#partial-data-machine-node","title":"Partial Data Machine node","text":"<pre><code>#!/bin/bash\n#SBATCH --account=data-machine  # Run under the data machine buy-in\n#SBATCH --nodes=1  # Reserve only one node\n#SBATCH --time=4:00:00  # Reserve for four hours (or your desired amount of time)\n#SBATCH --mem=256GB  # Set to your desired amount of memory\n#SBATCH --cpus-per-task=32  # Set to your desired number of CPUs\n</code></pre>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#full-data-machine-node-with-no-gpu-with-large-memory","title":"Full Data Machine node with no GPU with large memory","text":"<pre><code>#!/bin/bash\n#SBATCH --account=data-machine  # Run under the data machine buy-in\n#SBATCH --nodes=1  # Reserve only one node\n#SBATCH --time=4:00:00  # Reserve for four hours (or your desired amount of time)\n#SBATCH --mem=2TB  # Uses all memory on a large memory node\n#SBATCH --cpus-per-task=128  # Uses all CPUs on a node\n</code></pre>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#one-gpu-unit-on-a-single-node","title":"One GPU unit on a single node","text":"<pre><code>#!/bin/bash\n#SBATCH --account=data-machine  # Run under the data machine buy-in\n#SBATCH --nodes=1  # Reserve only one node\n#SBATCH --time=4:00:00  # Reserve for four hours (or your desired amount of time)\n#SBATCH --mem=256GB  # Set to your desired amount of memory\n#SBATCH --cpus-per-task=32  # Set to your desired number of CPUs\n#SBATCH --gpus=a100_1g.10gb  # Request one GPU unit on the reserved node\n</code></pre>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#two-gpu-units-on-a-single-node","title":"Two GPU units on a single node","text":"<pre><code>#!/bin/bash\n#SBATCH --account=data-machine  # Run under the data machine buy-in\n#SBATCH --nodes=1  # Reserve only one node\n#SBATCH --time=4:00:00  # Reserve for four hours (or your desired amount of time)\n#SBATCH --mem=256GB  # Set to your desired amount of memory\n#SBATCH --cpus-per-task=32  # Set to your desired number of CPUs\n#SBATCH --gpus=a100_1g.10gb:2  # Request two GPU units on the reserved node\n</code></pre>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#using-the-fast-nvme-storage","title":"Using the fast NVME storage","text":"<p>You can preload your data into local NVME storage using \"burst buffers\". SLURM will move the data you want to use into NVME storage before your job starts.</p>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#requesting-a-node","title":"Requesting a node","text":"<p>At the moment, burst buffers work best when requesting one specific node in the data machine. This ensures that the time SLURM takes to move your data does not count against the time you reserve the node for.</p> <p>However, be careful which node you pick. If this node is busy, SLURM will wait until it is available to assign it to you. You can use the </p> <pre><code>buyin_status --account data-machine\n</code></pre> <p>to see the current usage of the Data Machine nodes.</p>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#example-burst-buffer-resource-specification","title":"Example burst buffer resource specification","text":"<p>In this example, we'll assume that we don't need a GPU and choose <code>acm-048</code>.</p> <pre><code>#!/bin/bash\n#SBATCH --account=data-machine  # Run under the data machine buy-in\n#SBATCH --nodelist=acm-048  # Restrict to a specific data machine node\n#SBATCH --nodes=1  # Reserve only one node\n#SBATCH --time=4:00:00  # Reserve for four hours (or your desired amount of time)\n#SBATCH --memory=256GB  # Set to your desired amount of memory\n#SBATCH --cpus-per-task=128  # Set to your desired number of CPUs\n#BB source=/mnt/home/&lt;username&gt;/important/data/here\n</code></pre>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#using-the-local-data","title":"Using the local data","text":"<p>SLURM sets an environment variable <code>BB_DATA</code> with the location of your data on the local NVME storage. Use this directory to access your data with less latency than the home, research, or scratch space where it originally came from.</p>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#saving-data-written-to-local-storage","title":"Saving data written to local storage","text":"<p>Usually, if you edit data on local storage, your changes will be lost after the job ends. However, if you add the specification <code>resync=true</code> to the <code>#BB</code> line in your submission script, that data will be copied to the location it was originally taken from after the job ends.</p> <p>Example:</p> <pre><code>#!/bin/bash\n#SBATCH --account=data-machine  # Run under the data machine buy-in\n#SBATCH --nodelist=acm-048  # Restrict to a specific data machine node\n#SBATCH --nodes=1  # Reserve only one node\n#SBATCH --time=4:00:00  # Reserve for four hours (or your desired amount of time)\n#SBATCH --memory=256GB  # Set to your desired amount of memory\n#SBATCH --cpus-per-task=128  # Set to your desired number of CPUs\n#BB source=/mnt/home/&lt;username&gt;/important/data/here resync=true\n</code></pre>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#debugging-burst-buffer-issues","title":"Debugging burst buffer issues","text":"<p>To check on the status of your submitted jobs, use the command</p> <pre><code>squeue --me\n</code></pre> <p>The <code>NODELIST(REASON)</code> column of the output may give information relevant to burst buffer steps, e.g.,</p> <pre><code>   JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n24411390 data-mach data_mac grosscra  PENDING       0:00   1:00:00      1 (BurstBufferResources)\n24411197 data-mach data_mac grosscra  PENDING       0:00   1:00:00      1 (burst_buffer/lua: slurm_bb_data_in: )\n</code></pre> <p>A job with the <code>BurstBufferResources</code> reason is waiting for a node to run on and begin transferring resources. In the example above, a job is running the <code>slurm_bb_data_in</code>, i.e., it is transferring the data to the node.</p> <p>For more information or if there are problems with the burst buffer specification, use the command</p> <pre><code>scontrol show job &lt;jobid&gt;\n</code></pre> <p>For example, running <code>scontrol show job 24411197</code> while the job above was transferring data, the output ends with</p> <pre><code>...\nBurstBuffer=#BB source=/mnt/home/grosscra/scripts\nBurstBufferState=staging-in\n...\n</code></pre> <p>Often the <code>Comment</code> field in the <code>scontrol show job &lt;jobid&gt;</code> output can give helpful burst buffer information.</p>","tags":["reference","data machine"]},{"location":"Variables_I/","title":"Variables - Part I","text":"<p>The shell is a program that takes commands from the input device (usually, a keyboard) and gives them to the operating system to perform.\u00a0On most Linux system including HPC at MSU, <code>sh</code> works as the shell. Besides <code>sh</code>, other shells are available, but here, we will focus on <code>sh</code>.</p> <p>This tutorial assumes you have:</p> <ul> <li>minimal programming knowledge</li> <li>minimal Linux shell knowledge (see Linux Comand Line for Beginners I for an introduction)</li> </ul>","tags":["tutorial","command line"]},{"location":"Variables_I/#writing-a-script","title":"Writing a script","text":"<p>Let's create a file <code>first.sh</code> on the terminal using your favorite editor. If you rarely use any editor on Linux, this is a good chance to start using one of them (Linux text editors). A popular one for which is already installed on the HPCC is <code>nano</code>.</p> first.sh<pre><code>#!/bin/sh\n# This is a comment!\necho Hello World         # This is a comment, too!\n</code></pre> <p>The first line tells Linux that the file is to be executed by <code>/bin/sh</code>. <code>#!</code> will be explained later. The second line begins with <code>#</code>. This special character makes the line as a comment, and it is ignored by the shell. The only exception is when the first\u00a0line of the file starts with <code>#!</code>.</p> <p>The third line runs a command <code>echo</code>, with two parameters/arguments 'Hello' and 'World'. The symbol <code>#</code> on line 3 makes the rest of the line a comment.</p> <p>Now, after exiting the text editor, run <code>chmod u+x first.sh</code> on the command line to make the text file executable, then run <code>./first.sh</code>.</p> <pre><code>$ chmod u+x first.sh\n$ ./first.sh\nHello World\n</code></pre>","tags":["tutorial","command line"]},{"location":"Variables_I/#using-variables","title":"Using variables","text":"<p>Next, let's expand on <code>first.sh</code> by using variables.\u00a0Create a new script called <code>var1.sh</code> with the following content:</p> var1.sh<pre><code>#!/bin/bash\nMY_MESSAGE=\"Hello World\"\necho $MY_MESSAGE\n</code></pre> <p>This assigns the string <code>Hello World</code> to the variable <code>MY_MESSAGE</code> then <code>echo</code> command prints the value of the variable. Note that you need the quotes around the string.</p> <p>To use variables, <code>$</code> is required in front of variables. If you use <code>echo MY_MESSAGE</code> in the above, it will \u00a0print <code>MY_MESSAGE</code> instead of <code>Hello World</code>. The scope of the variable <code>MY_MESSAGE</code> is only inside of the script, and when the script finished the variable is empty (don't forget to use <code>chmod u+x var1.sh</code> to make a script executable).</p> <pre><code>$ ./var1.sh\nHello World\n$ echo $MY_MESSAGE\n\n$\n</code></pre> <p>In addition, if you use a variable without declaration, it returns empty string. There is no warning or error message. </p>","tags":["tutorial","command line"]},{"location":"Variables_I/#exploring-variable-scope","title":"Exploring variable scope","text":"<p>Let's create a shell script <code>var2.sh</code>.</p> var2.sh<pre><code>#!/bin/sh \necho \"MYVAR is: $MYVAR\"\nMYVAR=\"hi there\"\necho \"MYVAR is: $MYVAR\"\n</code></pre> <p>Then run the script. You can use <code>chmod u+x</code> to make <code>var2.sh</code> executable and run it as the previous examples or use the <code>sh</code> command:</p> <pre><code>$ sh var2.sh\nMYVAR is:\nMYVAR is: hi there\n</code></pre> <p>The first <code>MYVAR</code> is empty because it is not declared. The second <code>MYVAR</code> has the value we expected. The scope of the variables in a script is only inside the script. For example, <code>MYVAR</code> is only valid inside <code>var2.sh</code> and when the script finishes, <code>MYVAR</code> is empty again.</p> <pre><code>$ ./var2.sh\nMYVAR is:\nMYVAR is: hi there\n$ echo $MYVAR\n\n$\n</code></pre>","tags":["tutorial","command line"]},{"location":"Variables_I/#saving-variables","title":"Saving variables","text":"<p>You can declare variables with <code>export</code> command in a shell. Check the scope of variables.</p> <pre><code>$ MYVAR=\"hello there\"\n$ export MYVAR\n$ ./var2.sh\nMYVAR is: hello there\nMYVAR is: hi there\n$ echo $MYVAR\nhello there\n</code></pre>","tags":["tutorial","command line"]},{"location":"Variables_I/#extended-example","title":"Extended example","text":"<p>You can use variables in many ways. Here is one example.</p> var3.sh<pre><code>#!/bin/sh\necho \"What is your name?\"\nread USER_NAME\necho \"Hello $USER_NAME\"\necho \"I will create a file called ${USER_NAME}_file\"\ntouch ${USER_NAME}_file\n</code></pre> <p>Let's run the script.</p> <pre><code>$ chmod u+x var3.sh\n$ ./var3.sh\nWhat is your name?\nICER\nHello ICER\nI will create a file called ICER_file\n$ls -l ICER_file\n-rw-r--r--  1 choiyj  staff  0 Jan  5 14:08 ICER_file\n</code></pre> <p>Notice that we use curly braces for a file name. If you use <code>$USER_NAME_file</code> instead of <code>${USER_NAME}_file</code>, the shell returns the empty string because there is no variable called <code>USER_NAME_file</code> in the script.</p>","tags":["tutorial","command line"]},{"location":"Variables_II/","title":"Variables - Part II","text":"<p>Linux offers a set of pre-defined variables. These pre-defined variables contain useful information.</p> <p>The first set of variables are <code>$0</code>, <code>$1</code>, ..., <code>$9</code>, and <code>$#</code>.</p> <p>The variable <code>$0</code> is the name of the program as it was called. For example, if you run <code>example.sh</code> which uses the variable <code>$0</code>, it will return <code>example.sh</code>. <code>$1</code>, ..., <code>$9</code> are the first 9 additional parameters the script was called with. The total number of parameters that the script is called with is stored in <code>$#</code>.</p> <p>To access all parameters at once, we can use <code>$@</code> and <code>$*</code>. <code>$@</code> is a special variable takes the entire list of parameters and separates them into a list of parameters. Thus, the variable <code>$@</code> is all parameters <code>$1</code>, ..., <code>$any_number</code>. \u00a0The variables <code>$*</code> is similar but does not preserve any whitespace or quoting, so \"File with spaces\" becomes \"File\" \"with\" \"spaces\". This is similar to the echo command.</p> <p>Let's do a hands on example.</p> var4.sh<pre><code>#!/bin/sh\necho \"Number of parameters from input: $# parameters\"\necho \"My name is $0\"\necho \"My first parameter is $1\"\necho \"My second parameter is $2\"\necho \"All parameters are $@\"\n</code></pre> <p>Here is a sample run for the above script.</p> <pre><code>$ sh ./var4.sh\nNumber of parameters from input: 0 parameters\nMy name is ./var4.sh\nMy first parameter is\nMy second parameter is\nAll parameters are\n\n$ sh ./var4.sh My lazy fox\nNumber of parameters from input: 3 parameters\nMy name is ./var4.sh\nMy first parameter is My\nMy second parameter is lazy\nAll parameters are My lazy fox\n</code></pre> <p><code>$#</code> and <code>$1</code>, ..., <code>$9</code> are set by the shell. We can take more than 9 parameters by using the <code>shift</code> command. Look at the next example.</p> test.sh<pre><code>#!/bin/sh\nwhile [ \"$#\" -gt \"0\" ]\ndo\n  echo \"\\$1 is $1\"\n  shift\ndone    \n</code></pre> <p>The backslash character <code>\\</code> is used to \"escape\" <code>$</code> so that it is not interpreted by the shell. This script uses <code>shift</code> to move all parameters down one slot (removing <code>$1</code>) until <code>$#</code> is down to zero.</p> <p>Here is a sample run for the above script.</p> <pre><code>$ sh ./test.sh The quick brown fox jumps over the lazy dog.\n$1 is The\n$1 is quick\n$1 is brown\n$1 is fox\n$1 is jumps\n$1 is over\n$1 is the\n$1 is lazy\n$1 is dog.\n</code></pre> <p>We can write a script using <code>$*</code> to get the same result.</p> <pre><code>#!/bin/sh\n\nfor TOKEN in $*\ndo\n    echo \\$1 is $TOKEN\ndone\n</code></pre> <p>In the previous scripts, <code>while</code>, <code>for</code>, and <code>do ... done</code>\u00a0are loop commands which will be covered later.</p> <p>The\u00a0<code>$?</code>\u00a0variable represents the exit status of the previous command.\u00a0Exit status is a numerical value returned by every command upon its completion. Most commands return 0 if they were successful, and 1 if they were unsuccessful.</p> test.sh<pre><code>#!/bin/sh\n\nfor TOKEN in $*\ndo\n    echo \\$1 is $TOKEN\ndone\necho $?\n</code></pre> <p>Here is the result of the sample run.</p> <pre><code>$ sh ./test.sh The quick brown fox jumps over the lazy dog.\n$1 is The\n$1 is quick\n$1 is brown\n$1 is fox\n$1 is jumps\n$1 is over\n$1 is the\n$1 is lazy\n$1 is dog.\n0\n</code></pre>","tags":["tutorial","command line"]},{"location":"Variables_II/#see-also","title":"See also","text":"<ul> <li>How To Read and Set Environmental and Shell Variables on Linux by Justin Ellingwood</li> </ul>","tags":["tutorial","command line"]},{"location":"Virtual_Terminals/","title":"Virtual Terminals","text":""},{"location":"Virtual_Terminals/#gnu-screen","title":"GNU Screen","text":"<p>GNU Screen is a program that allows you to create a virtual terminal session inside a single terminal window. It is useful for dealing with multiple programs from a command line interface and for separating programs from the Unix shell that started the program.</p> <p>To start screen, simply type</p> <pre><code>screen\n</code></pre> <p>at the command prompt. Even if it looks like nothing has happened, you are now in a new window within screen.\u00a0\u00a0 Describing the details of how screen works is beyond the scope of this entry, but it allows you to leave a session running even after you've logged out (or disconnected because of network issues).\u00a0\u00a0</p> <p>One challenge with the 'screen' command is that by default you can't load any modules or other system commands.\u00a0 That's because 'screen' does not run the shell profile commands (in /etc/profile and /etc/profile.d).\u00a0 HPCC configures the modules system and several other system variables/settings in these profile scripts. Without running them, the module system won't work.</p> <p>However, you can ask screen to run this profile by including the line shell=-$SHELL in the screen config file \".screenrc\" in your home directory.\u00a0 To make a screen config file, try this (provided you don't already have a .screenrc file)</p> <p>create default .screenrc</p> <pre><code>echo 'shell -$SHELL' &gt;&gt; ~/.screenrc\n</code></pre> <p>You'll have to exit and restart screen to see the changes.</p>"},{"location":"Virtual_Terminals/#screen-commands","title":"Screen Commands","text":"<p>To send commands to screen (instead of the window you're working in), you preface them with Ctrl-a, i.e., you type the Ctrl key and \"a\" together, release both keys, then type the next letter to invoke the command.</p> <ul> <li> <p>Ctrl-A then:</p> ? Display available screen commands \" List running screen windows N Display number of current window c Open a new window [number] Switch to window [number] [ Copy (so you can paste to another window) ] Paste k Kill the current window Ctrl-\\ Quit and close all screen windows A Label the screen window M Monitor for activity d Detach the current screen session </li> <li> <p>Startup commands:</p> <code>screen -ls</code> List detached screen sessions on the server screen -S (name for new screen) Create a screen with specified name <code>screen --r [id]</code> Reattach the specified screen </li> </ul>"},{"location":"Virtual_Terminals/#detachingreattaching-screen-sessions","title":"Detaching/Reattaching Screen Sessions","text":"<p>To detach a running screen session, type <code>Ctrl-A d</code> This will detach the screen session with all of its windows but leave all of the related processes running. You can start and detach as many screen sessions as you wish, each with its own windows and processes. You can even logout of the server, and your screen sessions will continue running while you're gone.</p> <p>To reattach a screen session, find the id of the one you want with <code>screen -ls</code> then reattach it with <code>screen -r id</code></p> <p>If you don't remember which screen sessions you have opened on our cluster, you can type:</p> <pre><code>module load powertools\nuserinfo &lt;uid&gt;\n</code></pre> <p>If you are running a licensed software (such as MATLAB) within a screen session, please remember to kill it when you're done developing for the day. This will help us better manage licenses for the MSU research community.</p>"},{"location":"Virtual_Terminals/#screen-customizations","title":"Screen customizations","text":"<p>The attached file (click here) includes customizations to screen that places an information bar at the bottom of the terminal. Many people find it very usefu.\u00a0\u00a0 View the file and copy selected or all lines to\u00a0 your \".screenrc\" file.\u00a0</p>"},{"location":"Virtual_Terminals/#tmux","title":"Tmux","text":"<p>tmux is intended to be the modern, BSD-licensed alternative to screen. Both programs are available on HPCC. It allows splitting a window horizontally and vertically, and copying and pasting between multiple buffers. More information is available at http://tmux.sourceforge.net.</p>"},{"location":"accessHPCC_overview/","title":"Overview of HPCC access","text":"<p>Accessing the HPCC resources requires a user account and a proper login method. In this section, you will find out information about</p> <p>1)  How to apply for an HPCC account</p> <p>2)  How to use SSH to connect to the HPCC</p> <p>3)  How to use your web browser to connect to the HPCC</p> <p>4)  Information for users from other universities in Michigan</p> <p>For details, please click on the relevant links in the navigation menu under the current section.</p>"},{"location":"bi/","title":"bi","text":"<p>bi is a powertools program that provides innformation about user buy-in accounts on SLURM. For more information on buy-in accounts, see here. Running bi on its own will display your user name, any SLURM account your belong to and you default account:</p> <pre><code>$ bi\n\nUser:     panchyni\nAccounts: bioinformaticscore   [ MaxJobs=520, MaxSubmit=1000 ]\n          scavenger            [ MaxJobs=520, MaxSubmit=1000 ]\n          general              [ MaxJobs=520, MaxSubmit=1000 ]\nDefault:  general\n</code></pre> <p>A full description of arguments for bi can be found by acccessing the help for the program by runnig \"bi -h\" on the command line (this information is also reproduced below)</p> <pre><code>$ bi -h\n\n  Usage:\n    bi\n    bi -h\n    bi [ -a &lt;account&gt; | -u &lt;user&gt; ]\n    bi -d [ -u &lt;user&gt; ]\n    bi -l [ -u &lt;user&gt; ]\n\n    -h | --help               Display this help message\n    -a | --account &lt;account&gt;  Display users and nodes of a buyin account\n    -u | --user &lt;user&gt;        Display buyin accounts for this user\n    -d | --default            Display only the user's default buyin account\n    -l | --list               List jobs and usage status of buyin nodes\n</code></pre>","tags":["reference"]},{"location":"development_nodes/","title":"Development nodes","text":"<p>Warning</p> <p>Any long running (total CPU run time <code>&gt; 2hrs</code>) jobs on dev nodes will be killed automatically without advance notice.</p> <p>The HPCC has several development nodes that are available for users to compile their code and do short runs (less than 2 hours) to estimate run-time and memory usage.\u00a0</p> <p>These development nodes run the latest operating system and have similar configurations and environment setups as the compute nodes of the same clusters.\u00a0 Please use these development nodes to compile your program and test the work flow of your job script. For running long-time or large-resource computations, please submit jobs to use compute nodes.</p> <p>Warning</p> <p>Code compiled on older development nodes (dev-intel14 and dev-intel14-k20) may have errors when running on the latest clusters due to an outdated instruction set. To resolve this, compile your code on a newer development node.</p> <p>Users may <code>ssh</code> to the development nodes after connecting to the gateway via SSH. To access a certain development node, for example dev-amd20, please run\u00a0<code>ssh dev-amd20</code> from the gateway.  Users may also directly connect to development nodes by setting up SSH Tunneling Alternatively, they may be accessed through the \"Development Nodes\" tab on OnDemand.</p> <p>Nodes with  -k80 or -v100 suffixes have GPU cards required by GPU-enabled software, but may be used for any software.\u00a0Note there is not a development node containing the AMD20 A100 GPUs.\u00a0</p> Node Hostname Cores Memory Notes dev-amd20 128 960GB AMD EPYC 7H12 64-Core Processor @ 2.6GHz dev-amd20-v100 48 187GB Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz and 4 Tesla V100S dev-intel18 40 377GB Two 2.4Ghz 20-core Intel Xeon Gold 6148 CPU (40 cores total) dev-intel16 28 128GB Two 2.4Ghz 14-core Intel Xeon E5-2680v4 (28 cores total) dev-intel16-k80 28 256GB Intel16 node with 4 Nvidia Tesla K80 GPUs <p>Once your program is compiled and job script is tested, users can submit it to the SLURM queue by specifying various constraints such as\u00a0job duration, memory usage, number of CPUs, software license reservations and so on.</p>","tags":["reference"]},{"location":"filesystem_overview/","title":"Overview of HPCC file systems","text":"<p>The HPCC is comprised of several different file systems:</p> <ul> <li>The Home Space</li> <li>Research Space</li> <li>Scratch Space</li> <li>Node-local storage space</li> </ul> <p>Home, Research and Scratch are network file systems, meaning that each node in the cluster must go through the network switch to access these spaces. This can potentially slow down file I/O for jobs. Alternatively, there are several options for local file storage which either uses hard drives connected to each development and compute node or the node's RAM. See each respective page above for more information. We also have Guidelines for Choosing File Storage and I/O.</p> <p>Data that are subject to external security requirements are considered sensitive data. These requirements may be imposed by regulations or contractual obligations. In either case, users that intend to store sensitive data on HPCC systems must work with us to ensure data security. For more information, check out our Sensitive data hosting wiki page.</p>"},{"location":"getexample/","title":"getexample","text":"<p>getexample is powertools program for downloading user examples for various programs installed on HPCC. Using getexample requires specifying an examplename a list of which can be found by running getexample without any arguments (see below):</p> <pre><code>$ getexample\nDownload an HPC example:\nusage:\n   getexample &lt;examplename&gt;\n\nWhere &lt;examplename&gt; is the name of the example you want to\ndownload.  This will create a directory named examplename which\nyou can cd into and read the README file for details (if one is\navaliable) or read .qsub or .sb file for how the example is run.\nYou may submit the example with 'qsub *.qsub' or 'sbatch *.sb'.\n\nFor Example:\n  getexample helloworld\n\nPossible example names:\nabaqus_example        cuda_hybrid     Intro2Linux_Oct_2019    MATLAB_threadPool  PETSc_example\nABySS             DDT_examples    Job_dependency      MKL_benchmark      Python_MPI\nADMB_example          dmtcp_longjob   LAPACK_example      MKL_c_eigenvalues  Python_numpy\naffinity          espresso_benchmark  magma_example       MKL_Example        QuantumESPRESSO\nAmber_example         FFTW        makefile_example    MKL_FFTW       R_parallel_examples\nbasic_array_job       FI491       maker_MPI       MKL_parallel       SAS_example\nblast             fluentMPI       MATLAB_basic        mothur_example     ScaLAPACK\nblender_farm          fortran_openmp      MATLAB_compiler     MPI_OpenMP_GPU     simpleMatlab\nBOOST_example         FreeSurfer      MATLAB_GPU          MPI_pi         STATA_array\nbowtie            gmp_mpfr        MATLAB_many_jobs    multi_variable     STATA_example\nbrother_test          helloHPCC       MATLAB_parallel     NAMD_example       tbb_example\nClang_example         helloMPI        MATLAB_parameter_sweep  OpenACC_example    TotalView_MPI_example\nCMakePackageExamples  helloworld      MATLAB_parfor       openmp_exercise    VASP_example\ncuda              intro2hpcc      MATLAB_patternsearch    PC2HPC         XSEDE_MPI_WORKSHOP\n</code></pre>","tags":["reference"]},{"location":"install_ssh_client/","title":"Install SSH Client","text":"<p>To use the HPCC via a command line interface (as opposed to the webinterface provided by OnDemand), you will need an SSH client.  To launch interactive GUI programs over the command line, you will additionally need an X Windows (aka X11) server. Please see the following recommendations for your operating system.</p> WindowsMac OSLinux <p>We recommend installing\u00a0MobaXterm Home Edition. This program provides both an SSH client and an X Windows server. Follow this tutorial to set up MobaXterm.</p> <p>Warning</p> <p>Make sure you keep MobaXTerm up to date. If you use a new version of PuTTYgen to create SSH keys  (see here) an older version of MobaXTerm may not be able to read your generated keys.</p> <p>You can use Terminal program that comes installed with your operating system as your SSH Client. However, for running\u00a0graphical user interface (GUI) programs on HPCC, you will need to install the X Windows server program XQuartz. See https://www.xquartz.org/ for download instructions.</p> <p>Similar to Mac OS, you can use the built-in Terminal as your SSH client. There is no need to download additional X Windows server software.</p>","tags":["reference","ssh"]},{"location":"job_policies/","title":"Job Policies","text":"<p>The following limits apply generally to all MSU users of the HPCC. Those at affiliate institutions may be working under slightly different policies. The limits are in place to help our large user community share the HPCC. However, if these policies are an impediment to completing your research, please contact us.</p>","tags":["reference","slurm"]},{"location":"job_policies/#cpu-and-gpu-usage-limits","title":"CPU and GPU usage limits","text":"<ul> <li> <p>HPCC users who do not have a buy-in account are given a 'general'     SLURM account. The general account is limited to\u00a0500,000 CPU hours     (30,000,000 minutes) and 10,000\u00a0GPU hours (600,000 minutes)\u00a0every     year (from January 1st to December 31st) starting from 2021. </p> <ul> <li>A CPU hour is the walltime of your job multiplied by the number of CPUs used.   The same applies for GPU hours.</li> </ul> </li> <li> <p>There is no yearly usage limit on CPU or GPU time with a buy-in     account. If you have a buy-in account, your jobs will be run under that     account by     default, unless     the manager of the buy-in account has chosen to opt-in (requiring jobs to     be submitted with the <code>-A</code>     flag) instead of     opt-out.</p> </li> <li> <p>Users with general accounts can use the powertools command <code>SLURMUsage</code>     to check their used CPU and GPU time (in minutes) and remaining CPU and     GPU time (in hours):</p> <p><code>$ ml powertools # run this command if powertools not loaded</code></p> <p><code>$ SLURMUsage</code></p> </li> <li> <p>If users without a buy-in account need more CPU or GPU time due to     reaching the limits, they can request additional CPU/GPU hours     by filling out the CPU/GPU Increase Request online form.</p> </li> </ul>","tags":["reference","slurm"]},{"location":"job_policies/#limits-on-job-resource-requests","title":"Limits on job resource requests","text":"<ul> <li>Time: Users can schedule jobs and run for at most 7 days (168     hours)\u00a0 ( <code>--time=168:00:00</code>)</li> <li>CPU: Users can utilize up to a total of 1040 cores and have at most 520 jobs     running at any one time.\u00a0The core usage value is reflected in the SLURM     variable <code>QOSMaxCpuPerUserLimit</code>(Buyin groups who have purchased more than     1040 cores can exceed this limit)</li> <li>Queue: The maximum number of jobs that can be queued or running per user is     1000 jobs.</li> </ul>","tags":["reference","slurm"]},{"location":"job_policies/#buy-in-program","title":"Buy-in program","text":"<p>Faculty can purchase nodes via our buy-in program. The program guarantees jobs submitted with a buy-in group will start running on their buy-in nodes in 4 hours. However, due to contention between buy-in group jobs, the guarantee might not be fulfilled if requested resources are occupied or reserved by other jobs of the buy-in group.</p>","tags":["reference","slurm"]},{"location":"job_policies/#policy-summary","title":"Policy summary","text":"<ul> <li>Jobs that run under 4 hours are able to run on the largest set of     nodes (the combination of community + specialized hardware + buy-in     nodes.\u00a0 See below for details)\u00a0</li> <li>Jobs that request more resources (processors or RAM) have priorities     over smaller jobs because these jobs are more difficult to schedule.</li> <li>Jobs accrue priority based on how long they have been queued.</li> <li>The scheduler will attempt to balance usage among all users. (See     Fairshare Policy below.)</li> <li>It is against our fair use policy to artificially increase the     priority of a job in the queue (e.g. by requesting more resources     which will not be used). Jobs found to be manipulating the scheduler     will be canceled, and users continuing to attempt this will be     suspended.</li> </ul>","tags":["reference","slurm"]},{"location":"job_policies/#more-about-queue-time","title":"More about queue time","text":"<p>This section gives a brief overview of the factors that affect how long your job sits in the SLURM queue. For more information, see the page on how jobs are scheduled by SLURM as well as other pages under \"Understanding the Scheduler.\"</p>","tags":["reference","slurm"]},{"location":"job_policies/#fairshare","title":"Fairshare","text":"<p>As jobs wait in the queue, they accrue priority to run. Another factor that contributes to a job's priority value is Fairshare. The scheduler will attempt to ensure fair resource utilization of all HPCC users by adjusting the initial priorities of the users who have recently used HPCC resources. Due to the policy, if users had jobs running with many resources recently, their current pending jobs might wait longer than before. Users can find the Fairshare contribution to a job priority by running command \"<code>sprio -u $USER</code>\":</p> <pre><code>[UserID@dev-intel18 UserID]$ sprio -u $USER\n          JOBID PARTITION     USER   PRIORITY       SITE        AGE  FAIRSHARE        QOS                 TRES\n       53381467 general-l   UserID      49432          0          0      49318          0       cpu=100,mem=15\n       53381467 general-s   UserID      49432          0          0      49318          0       cpu=100,mem=15\n</code></pre> <p>where it is found under <code>FAIRSHARE</code> column and the values are between 60,000 (highest priority contribution) and 0 (lowest priority contribution).  The more resources your jobs used recently, the less your Fairshare value will become, resulting in lower overall priority for your jobs. For other contributions of <code>sprio</code> results, please check Job Priority Factors.</p>","tags":["reference","slurm"]},{"location":"job_policies/#shorter-jobs-can-run-on-more-nodes","title":"Shorter jobs can run on more nodes","text":"<p>Jobs that request a total running (wall-clock) time of four hours or less can run on any available buy-in and specialized nodes. Because they can access any nodes, they are likely to start running more quickly than the jobs which have to wait for the general-long partition nodes.</p>","tags":["reference","slurm"]},{"location":"job_policies/#bigger-jobs-are-prioritized-small-jobs-are-backfilled","title":"Bigger jobs are prioritized &amp; small jobs are backfilled","text":"<p>The scheduler attempts to gather resources for large jobs and then backfill smaller jobs around them. The size of the job is determined by the number of CPUs and amount of memory requested.</p> <p>The scheduler packs small jobs together to allow more resources to be gathered for multi-core jobs.\u00a0 Resource requests are monitored. Abusive resource requests may violate MSU policy.</p>","tags":["reference","slurm"]},{"location":"js/","title":"js","text":"<p>The SLURM command sacct can be used to show the job steps of a job and the resource usages after it finished running.:</p> <pre><code>sacct -j 40410\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n40410               job classres-+   classres         28  COMPLETED      0:0\n40410.batch       batch              classres         28  COMPLETED      0:0\n40410.extern     extern              classres         28  COMPLETED      0:0\n40410.0            pw.x              classres         28  COMPLETED      0:0\n40410.1            ph.x              classres         28  COMPLETED      0:0\n</code></pre> <p>However, to display your desired results, it might take you some time to look into the web site and learn how to use the command. Here we  introduce the powertools command \"<code>js</code>\" to display the resource usages of your jobs.</p>","tags":["reference"]},{"location":"js/#display-usage-info-of-a-job","title":"Display Usage Info of a Job","text":"<p>Users can simply run the powertools command \"<code>js</code>\" and it gives you most of the useful resource usages. To see the resource usages of a job, just use the command \"<code>js -j &lt;JobID&gt;</code>\", e.g.,</p> <pre><code>$ js -j 45251                      # powertools command\n\nSLURM Job ID: 45251\nWrkDir=/mnt/home/changc81/GetExample/GaAs\nstdout=/mnt/home/changc81/GetExample/GaAs/slurm-45251.out\n=========================================================================================================\n          JobID |               45251 |         45251.batch |        45251.extern |             45251.0 |\n        JobName |                 job |               batch |              extern |                pw.x |\n           User |            UserName |                     |                     |                     |\n       NodeList |             lac-421 |             lac-421 |             lac-421 |             lac-421 |\n         NNodes |                   1 |                   1 |                   1 |                   1 |\n         NTasks |                     |                   1 |                   1 |                  28 |\n          NCPUS |                  28 |                  28 |                  28 |                  28 |\n         ReqMem |               112Gn |               112Gn |               112Gn |               112Gn |\n      Timelimit |            04:00:00 |                     |                     |                     |\n        Elapsed |            00:00:16 |            00:00:16 |            00:00:16 |            00:00:14 |\n       TotalCPU |           05:39.544 |           00:00.999 |           00:00.001 |           05:38.543 |\n     AveCPULoad |             21.2215 |           0.0624375 |            6.25e-05 |             24.1816 |\n         MaxRSS |                     |                     |                 20K |              60296K |\n      MaxVMSize |                     |             189200K |               4184K |             605104K |\n          Start | 2018-08-28T20:27:40 | 2018-08-28T20:27:40 | 2018-08-28T20:27:40 | 2018-08-28T20:27:41 |\n            End | 2018-08-28T20:27:56 | 2018-08-28T20:27:56 | 2018-08-28T20:27:56 | 2018-08-28T20:27:55 |\n       ExitCode |                 0:0 |                 0:0 |                 0:0 |                 0:0 |\n          State |           COMPLETED |           COMPLETED |           COMPLETED |           COMPLETED |\n=========================================================================================================\n</code></pre> <p>If you would like to show more data of a job, you can also use the specification -F:</p> <pre><code>$ js -j &lt;Job ID&gt; -F                   # powertools command\n</code></pre> <p>to list all stored data of the job steps.</p>","tags":["reference"]},{"location":"js/#display-a-list-of-jobs","title":"Display a List of Jobs","text":"<p>If users would like to know a list of jobs submitted before, they can use\u00a0 \"<code>js -z</code>\" command. Simply provide a period of time when job was running with -S (start time of the period) and -E (end time of the period) options:</p> <pre><code>$ js -z -S &lt;Start Time&gt; -E &lt;End Time&gt;\n</code></pre> <p>and a list of the jobs with their properties and resource usages is displayed. For example, user can run the command:</p> <pre><code>$ js -z -S 2021-04-12 -E 2021-04-19\n       JobID    JobName NNo NTas NCPU   Timelimit     Elapsed  AveCPU     MaxRSS Stat Exit               Start          NodeList\n------------ ---------- --- ---- ---- ----------- ----------- ------- ---------- ---- ---- ------------------- -----------------\n    21043834 ondemand/+   1    1    1    01:00:00    01:00:16 0.05487    467.41M TIM+  0:0 2021-04-12T08:59:10           css-033\n    21127831    fi_info   1    1    2    00:05:00    00:03:19 1.18321  58191.45M COM+  0:0 2021-04-16T10:14:33           skl-033\n    21158898  hello.exe   1    8    1    00:20:00    00:01:09   0.236    644.61M COM+  0:0 2021-04-17T20:17:57           amr-133\n    21158916 interacti+   1    1    1    03:00:00    02:00:04 0.77325   1244.61M COM+  0:0 2021-04-18T20:18:29           css-033\n    21158973     SPAdes   1    4    8    09:30:00    09:00:04   7.254     13.20G FAI+  1:0 2021-04-19T20:20:44           lac-421\n</code></pre> <p>to see a list of jobs running between April 12th 2021 and April 19th 2021. If any one of the options -S or -E is not specified, the time will be considered as the current time of \"<code>js</code>\" execution.</p>","tags":["reference"]},{"location":"js/#more-selections-of-js-command","title":"More Selections of js Command","text":"<p>To see all possible usages of the command, please use the specification -h:</p> <pre><code>$ js -h\n\njs [&lt;OPTION&gt;]\n     Valid &lt;OPTION&gt; values are:\n     -a, --allusers:\n                   Display jobs for all users. By default, only the\n                   current user's jobs are displayed.  If ran by user root\n                   this is the default.\n     -A, --accounts:\n                   Use this comma separated list of accounts to select jobs\n                   to display.  By default, all accounts are selected.\n     -b, --brief:\n                   Equivalent to '--format=jobstep,state,error'.\n     -c, --completion: Use job completion instead of accounting data.\n         --delimiter:\n                   ASCII characters used to separate the fields when\n                   specifying the  -p  or  -P options. The default delimiter\n                   is a '|'. This options is ignored if -p or -P options\n                   are not specified.\n     -C:\n                   Display results in columns rather than rows. Each\n                   column shows all data of a job step. A number can\n                   be specified after -C for how many columns in a row.\n     -D, --duplicates:\n                   If Slurm job ids are reset, some job numbers may\n                   appear more than once referring to different jobs.\n                   Without this option only the most recent jobs will be\n                   displayed.\n     -e, --helpformat:\n                   Print a list of fields that can be specified with the\n                   '--format' option\n     -E, --endtime:\n                   Select jobs eligible before this time.  If states are\n                   given with the -s option return jobs in this state before\n                   this period.\n         --federation: Report jobs from federation if a member of a one.\n     -f, --file=file:\n                   Read data from the specified file, rather than Slurm's\n                   current accounting log file. (Only appliciable when\n                   running the filetxt plugin.)\n     -F:\n                   Display data of all fields (--format=ALL) in columns.\n                   By default, three columns are shown in a row. See -C\n                   to change the default column number.\n     -g, --gid, --group:\n                   Use this comma separated list of gids or group names\n                   to select jobs to display.  By default, all groups are\n                   selected.\n     -h, --help:   Print this description of use.\n     -i, --nnodes=N:\n                   Return jobs which ran on this many nodes (N = min[-max])\n     -I, --ncpus=N:\n                   Return jobs which ran on this many cpus (N = min[-max])\n     -j, --jobs:\n                   Format is &lt;job(.step)&gt;. Display information about this\n                   job or comma-separated list of jobs. The default is all\n                   jobs. Adding .step will display the specific job step of\n                   that job. (A step id of 'batch' will display the\n                   information about the batch step.)\n     -k, --timelimit-min:\n                   Only send data about jobs with this timelimit.\n                   If used with timelimit_max this will be the minimum\n                   timelimit of the range.  Default is no restriction.\n     -K, --timelimit-max:\n                   Ignored by itself, but if timelimit_min is set this will\n                   be the maximum timelimit of the range.  Default is no\n                   restriction.\n         --local   Report information only about jobs on the local cluster.\n                   Overrides --federation.\n     -l, --long:\n                   Equivalent to specifying\n                   '--format=jobid,jobname,partition,maxvmsize,maxvmsizenode,\n                             maxvmsizetask,avevmsize,maxrss,maxrssnode,\n                             maxrsstask,averss,maxpages,maxpagesnode,\n                             maxpagestask,avepages,mincpu,mincpunode,\n                             mincputask,avecpu,ntasks,alloccpus,elapsed,\n                             state,exitcode,avecpufreq,reqcpufreqmin,\n                             reqcpufreqmax,reqcpufreqgov,consumedenergy,\n                             maxdiskread,maxdiskreadnode,maxdiskreadtask,\n                             avediskread,maxdiskwrite,maxdiskwritenode,\n                             maxdiskwritetask,avediskread,allocgres,reqgres\n     -L, --allclusters:\n                   Display jobs ran on all clusters. By default, only jobs\n                   ran on the cluster from where sacct is called are\n                   displayed.\n     -M, --clusters:\n                   Only send data about these clusters. Use \"all\" for all\n                   clusters.\n     -n, --noheader:\n                   No header will be added to the beginning of output.\n                   The default is to print a header.\n     --noconvert:\n                   Don't convert units from their original type\n                   (e.g. 2048M won't be converted to 2G).\n     -N, --nodelist:\n                   Display jobs that ran on any of these nodes,\n                   can be one or more using a ranged string.\n     --name:\n                   Display jobs that have any of these name(s).\n     -o, --format:\n                   Comma separated list of fields. (use \"--helpformat\"\n                   for a list of available fields).\n     -p, --parsable: output will be '|' delimited with a '|' at the end\n     -P, --parsable2: output will be '|' delimited without a '|' at the end\n     -q, --qos:\n                   Only send data about jobs using these qos.  Default is all.\n     -r, --partition:\n                   Comma separated list of partitions to select jobs and\n                   job steps from. The default is all partitions.\n     -s, --state:\n                   Select jobs based on their current state or the state\n                   they were in during the time period given: running (r),\n                   completed (cd), failed (f), timeout (to), resizing (rs),\n                   deadline (dl) and node_fail (nf).\n     -S, --starttime:\n                   Select jobs eligible after this time.  Default is\n                   00:00:00 of the current day, unless '-s' is set then\n                   the default is 'now'.\n     -T, --truncate:\n                   Truncate time.  So if a job started before --starttime\n                   the start time would be truncated to --starttime.\n                   The same for end time and --endtime.\n     -u, --uid, --user:\n                   Use this comma separated list of uids or user names\n                   to select jobs to display.  By default, the running\n                   user's uid is used.\n     --units=[KMGTP]:\n                   Display values in specified unit type. Takes precedence\n                   over --noconvert option.\n     --usage:      Display brief usage message.\n     -v, --verbose:\n                   Primarily for debugging purposes, report the state of\n                   various variables during processing.\n     -V, --version: Print version.\n     -W, --wckeys:\n                   Only send data about these wckeys.  Default is all.\n     --whole-hetjob=[yes|no]:\n                   If set to 'yes' (or not set), then information about all\n                   the heterogeneous components will be retrieved. If set\n                   to 'no' only the specific filtered components will be\n                   retrieved.\n     -x, --associations:\n                   Only send data about these association id.  Default is all.\n     -X, --allocations:\n                   Only show statistics relevant to the job allocation\n                   itself, not taking steps into consideration.\n     -z:           Show simple summary data only.\n\n     Note, valid start/end time formats are...\n                   HH:MM[:SS] [AM|PM]\n                   MMDD[YY] or MM/DD[/YY] or MM.DD[.YY]\n                   MM/DD[/YY]-HH:MM[:SS]\n                   YYYY-MM-DD[THH:MM[:SS]]\n</code></pre>","tags":["reference"]},{"location":"make/","title":"<code>make</code>","text":"<p>The program <code>make</code> and Makefiles are a simple way to organize code compilation. This tutorial offers a very basic idea of what is possible using <code>make</code>.</p>","tags":["tutorial","make","compilers"]},{"location":"make/#prerequisites","title":"Prerequisites","text":"<p>For this tutorial, please run</p> <pre><code>module load powertools\ngetexample makefile_example2\n</code></pre> <p>This will create a directory <code>makefile_example2</code> with a main program <code>hello.c</code>, a function <code>hellofunc.c</code>, and an include file <code>hello.h</code>.</p>","tags":["tutorial","make","compilers"]},{"location":"make/#compiling-by-hand","title":"Compiling by hand","text":"<p>To compile this code, you would use the following command:</p> <pre><code>gcc -o hello hello.c hellofunc.c -I.\n</code></pre> <p>This command compiles the two C files, and names the executable hello. With the <code>-I.</code> flag, <code>gcc</code> will look in the current directory for the include file <code>hello.h</code>. For future steps, please remove the executable with <code>rm hello</code>.</p> <p>With only two C files, it is easy to compile with the above approach, but with more files, it is harder to keep track of everything. In addition, if you are only making changes to one C file, the above approach recompiles all of C files every time which is time-consuming and inefficient.</p>","tags":["tutorial","make","compilers"]},{"location":"make/#transferring-to-a-makefile","title":"Transferring to a Makefile","text":"<p>A Makefile will be helpful for such cases. A simple Makefile is included in the <code>getexample</code>, so before we start, please move it to <code>makefile.bak</code> with <code>mv makefile makefile.bak</code>.</p> <p>Now, in the <code>makefile_example2</code> directory, create a file called <code>makefile</code> which has the following two lines:</p> makefile<pre><code>hello: hello.c hellofunc.c\n    gcc -o hello hello.c hellofunc.c -I.\n</code></pre> <p>Here, the tab must actually be a tab character, not spaces.</p> <p>Now, type <code>make</code> on the terminal and check if the executable is created. The <code>make</code> command will execute the compile command as you have written it in the Makefile.</p> <p>Note that invoking <code>make</code> with no arguments executes the first rule in the file. Furthermore, by putting the list of files on which the command depends on the first line after the ':', <code>make</code> knows that the rule <code>hello</code> needs to be executed if any of those files change.</p>","tags":["tutorial","make","compilers"]},{"location":"make/#useful-makefile-variables","title":"Useful Makefile variables","text":"<p>Can we make it a little bit more efficient? Let's modify <code>makefile</code> like so:</p> makefile<pre><code>CC=gcc\nCFLAGS=-I.\nhello: hello.o hellofunc.o\n    $(CC) -o hello hello.o hellofunc.o\n</code></pre> <p>In this Makefile, we define the variables <code>CC</code> and <code>CFLAGS</code>, which are special macros communicating to <code>make</code> how we want to compile the files <code>hello.c</code> and <code>hellofunc.c</code>. In particular, <code>CC</code> is for the C compiler, and <code>CFLAGS</code> is the list of flags to pass to C compiler.</p> <p>By putting the object files (<code>hello.o</code> and <code>hellofunc.o</code>) in the dependency list and in the rule, <code>make</code> will automatically compile the <code>.c</code> files individually into object files, and then build the executable <code>hello</code>. If your project is small (just a few files), this form of Makefile is enough.</p>","tags":["tutorial","make","compilers"]},{"location":"make/#setting-up-dependencies","title":"Setting up dependencies","text":"<p>However, there is a problem. This Makefile misses the include files. For example, if you made a change to <code>hello.h</code>, <code>make</code> would not recompile the <code>.c</code> files, even though the change in <code>hello.h</code> may affect them. In order to fix this problem, we need to tell <code>make</code> that all <code>.c</code> files depend on certain <code>.h</code> files. This can be done by writing a simple rule and adding it to the Makefile.</p> makefile<pre><code>CC=gcc\nCFLAGS=-I.\nDEPS = hello.h\n\n%.o: %.c $(DEPS)\n    $(CC) -c -o $@ $&lt; $(CFLAGS)\n\nhello: hello.o hellofunc.o\n    $(CC) -o hello hello.o hellofunc.o\n</code></pre> <p>This addition first creates the macro <code>DEPS</code> (the macro name can be anything), which is the set of <code>.h</code> files on which the <code>.c</code> files depend.</p> <p>Then we define a rule for all <code>.o</code> files. The rule says that each <code>.o</code> file depends on </p> <ul> <li>the <code>.c</code> file with the same name, and </li> <li>the <code>.h</code> files which are included in <code>DEPS</code>.</li> </ul> <p>Next, the rule says that to generate the <code>.o</code> file, <code>make</code> needs to compile the <code>.c</code> file using the compiler defined in <code>CC</code>. The components are described as follows:</p> <ul> <li>the <code>-c</code> flag says to generate the object file, </li> <li>the <code>-o $@</code> says to put the output of the compilation in the file named on   the left side of the <code>:</code> (in this case, the <code>.o</code> file), </li> <li>the <code>$&lt;</code> is the first item in the dependencies list (in this case, the <code>.c</code>   file), </li> <li>and the <code>CFLAGS</code> macro is defined on the second line.</li> </ul>","tags":["tutorial","make","compilers"]},{"location":"make/#generalizing","title":"Generalizing","text":"<p>To simplify the final rule, you can use special macros <code>$@</code> and <code>$^</code>, which are the left and right sides of the <code>:</code>, respectively. This also generalizes the rule to work for multiple files at once.</p> <p>In the example below, all of the include files should be listed as part of the macro <code>DEPS</code>, and all of the object files should be listed as part of the macro <code>OBJ</code>.</p> makefile<pre><code>CC=gcc\nCFLAGS=-I.\nDEPS = hello.h\nOBJ = hello.o hellofunc.o\n\n%.o: %.c $(DEPS)\n    $(CC) -c -o $@ $&lt; $(CFLAGS)\n\nhello: $(OBJ)\n    $(CC) -o $@ $^ $(CFLAGS)\n</code></pre>","tags":["tutorial","make","compilers"]},{"location":"make/#further-resources","title":"Further resources","text":"<p>Now you have a good sense of the Makefile. For more information on Makefiles and the <code>make</code> function, check out the GNU Make Manual. </p> <p>You can download some other Makefile examples on the HPCC using getexample.</p>","tags":["tutorial","make","compilers"]},{"location":"modules_changelog/","title":"Module changelog","text":"<ul> <li>2024-03-19 - Installation of moduel DFTB+/21.1 from source (Xiaoge Wang)</li> <li>2024-03-04 - Installed Module: DMTCP Version: 3.0.0 (Craig Gross)</li> <li>2024-03-04 - Installed Module: PAML Version: 4.10.5 (Craig Gross)</li> <li>2024-02-21 - Manually added an enviroment variable that sets the path to the plugin folder (Nicholas Louis Panchy)</li> <li>2024-02-20 - Installed Apache Spark and dependencies (Andrew Giles Fullard)</li> <li>2024-02-16 - Installed Module: htop Version: 2.0.0 and NO dependencies (Craig Gross)</li> <li>2024-01-31 - Installed Module: CGAL 5.5.2 (Andrew Giles Fullard)</li> <li>2024-01-29 - installation of Trilinos/13.4.1-GCC-12.2.0. (Xiaoge Wang)</li> <li>2024-01-26 - Partial install of Trilinos/13.4.1 by Xiaoge, work in progress (Nicholas Louis Panchy)</li> <li>2024-01-26 - Updated BCFtools, GSL, and HTSlib for BCFtools 1.17 (Nicholas Louis Panchy)</li> <li>2024-01-19 - Installed Module: Blender Version 3.4.1 and 1 dependency (Craig Gross)</li> <li>2024-01-11 - added abaqus 2023.lua (Jim Leikert)</li> <li>2024-01-04 - Julia 1.10 (Andrew Giles Fullard)</li> <li>2024-01-03 - Change EasyBuild Version 4.8.1 configuration (Craig Gross)</li> <li>2023-12-19 - Installed new version of Nextflow, 23.10.0 (Nicholas Louis Panchy)</li> <li>2023-12-19 - Installed Module: Molpro versio 2023.2.0, source build and binaries (Nicholas Louis Panchy)</li> <li>2023-12-19 - Installed Module: ESMF Version: 8.3.0 and 2 dependencies (Xiaoge Wang)</li> <li>2023-12-18 - Installed Module: M4 Version: 1.4.19 and 1 dependency (Craig Gross)</li> <li>2023-12-18 - Installed Module: VoyantServer Version: 2.6.10-Java-11.0.2 and NO dependencies (Craig Gross)</li> <li>2023-12-14 - Updaed breseq(0.38.1) for a BioinformaticsCore Project (Nicholas Louis Panchy)</li> <li>2023-12-14 - Added new versions of ulia (1.8.0,1.8.2,1.9.3) (Nicholas Louis Panchy)</li> <li>2023-12-06 - Installed Module: htop Version: 2.0.0 and NO dependencies (Craig Gross)</li> <li>2023-12-01 - Installation of GDB/13.2 and its dependency. (Xiaoge Wang)</li> <li>2023-11-30 - Fix a path error of .lua file for module OpenMPI/4.1.1-gcccuda-11.1.0-11.4.0 (Xiaoge Wang)</li> <li>2023-11-30 - added schrodinger 2023-4 module (Jim Leikert)</li> <li>2023-11-21 - added mathematica 13.3.1.lua modulefile (Jim Leikert)</li> <li>2023-11-16 - Installation of GROMACS/2023.1-CUDA-11.8.0 and its dependencies. (Xiaoge Wang)</li> <li>2023-11-16 - Installation of OpenBLAS/0.3.23 (Xiaoge Wang)</li> <li>2023-11-14 - reinstall PHP/8.2.12 with more extentions. (Xiaoge Wang)</li> <li>2023-11-13 - Install module phoronix-test-suite and its dependency php/8.2.12. (Xiaoge Wang)</li> <li>2023-11-06 - Add a warning message to the module file. This version only works on intel18 nodes. (Xiaoge Wang)</li> <li>2023-11-02 - Installation of VASP/6.2.1 revised version. See RT #74184 for details. (Xiaoge Wang)</li> <li>2023-10-30 - Remove the module HDF5 built with GCC toolchain (Xiaoge Wang)</li> <li>2023-10-30 - Install HDF5/1.14.0 with toolchain gompi/2023a (Xiaoge Wang)</li> <li>2023-10-30 - Create a toolchain gompi/2023a (Xiaoge Wang)</li> <li>2023-10-30 - Installation of HDF5/1.14.0. (Xiaoge Wang)</li> <li>2023-10-30 - Install libevent/2.1.12 and PMIx/4.2.4 for dependency of OpenMPI/5.0.0-CUDA-12.3.0. But there is some issue. Use bundled module instead. (Xiaoge Wang)</li> <li>2023-10-30 - Installation of libpciaccess/0.17, a dependency of OpenMPI/5.0.0-CUDA-12.3.0. (Xiaoge Wang)</li> <li>2023-10-30 - Installation of hwloc/2.9.1, a dependency of OpenMPI/5.0.0-CUDA-12.3.0. (Xiaoge Wang)</li> <li>2023-10-30 - Installation of libfabri/1.18.0, a dependency of OpenMPI/5.0.0-CUDA-12.3.0. (Xiaoge Wang)</li> <li>2023-10-30 - Installation of UCX/1.14.1, a dependency of OpenMPI/5.0.0-CUDA-12.3.0. (Xiaoge Wang)</li> <li>2023-10-30 - Installation of libxml2/2.11.4, dependency of OpenMPI/5.0.0-CUDA-12.3.0. (Xiaoge Wang)</li> <li>2023-10-30 - Installation of module numactl/2.0.16, which is a dependency of OpenMPI/5.0.0-CUDA-12.3.0 (Xiaoge Wang)</li> <li>2023-10-30 - Installation of module OpenMPI/5.0.0-CUDA-12.3.0 (Xiaoge Wang)</li> <li>2023-10-29 - Installation of module pkg-config/0.29.2 (Xiaoge Wang)</li> <li>2023-10-29 - Installation of module GCC/12.3.0 (Xiaoge Wang)</li> <li>2023-10-29 - installation of CUDA/12.3.0 (Xiaoge Wang)</li> <li>2023-10-27 - Modify the lua file to fix the issue of installation paths of CUDA/8.0.44 with intel tool chain. (Xiaoge Wang)</li> <li>2023-10-26 - Finished Rust 1.70 install (Nicholas Louis Panchy)</li> <li>2023-10-26 - Finished Trinity 2.15.1 install (Nicholas Louis Panchy)</li> <li>2023-10-25 - New EasyBuild with updated easyblocks for recent software builds (Nicholas Louis Panchy)</li> <li>2023-10-25 - New alphafold singularity image and support scripts (Nicholas Louis Panchy)</li> <li>2023-10-25 - Parts of the GCC 12.3.0 tool chain for the incomplete build of Rust 1.70 (Nicholas Louis Panchy)</li> <li>2023-10-25 - Additional parts of the Trinit 2.51.1 build (Nicholas Louis Panchy)</li> <li>2023-10-25 - Updated bioinformatics software for incompleted build of Trinity 2.15.1 (Nicholas Louis Panchy)</li> <li>2023-10-18 - install elbencho/2.0-3 with CUDA support (Xiaoge Wang)</li> <li>2023-10-13 - testing push (Joel)</li> <li>2023-10-13 - testing push (Joel)</li> <li>2023-10-11 - Installation of Nektar++ and OrthoFinder (Xiaoge Wang)</li> <li>2023-10-06 - Add module gffcompare/0.10.6 (Xiaoge Wang)</li> <li>2023-10-04 - reinstall OrthoFinder/2.5.4 with explicitly add MAFF as dependency (Xiaoge Wang)</li> <li>2023-10-03 - Flow3d 2023R1 file cleanup (Jim Leikert)</li> <li>2023-10-02 - Build OrthoFinder module verion 2.5.4 with Scipy-boundle version 2019.10-Python-2.7.16 to fix the issue in the version built with Scipy-boundle version 2019.10-Python-3.7.4 (Xiaoge Wang)</li> <li>2023-09-27 - libaio/0.3.112 is installed as the dependcy of module elbencho/2.0-3 (Xiaoge Wang)</li> <li>2023-09-27 - elbencho is a distributed storage benchmark for files, objects &amp; blocks with support for GPUs (Xiaoge Wang)</li> <li>2023-09-27 - added flow3d 2023R1 (Jim Leikert)</li> <li>2023-09-22 - Module file of LAMMPS/2Aug2023-Python-3.9.6 (Xiaoge Wang)</li> <li>2023-09-22 - Module file of LAMMPS/2Aug2023-kokkos. (Xiaoge Wang)</li> <li>2023-09-22 - Added two new versions of RSEM/1.3.3 and associated software. New builds support updated versions of the STAR aligner. (Nicholas Louis Panchy)</li> <li>2023-09-21 - changed license server info for ansys v. 16,18,19 (Jim Leikert)</li> <li>2023-09-21 - added converge 3.0.28 ompi/impi module files (Jim Leikert)</li> <li>2023-09-14 - Install JAGS/4.3.1 (Compatible with R/4.2.1) (Craig Gross)</li> <li>2023-09-09 - New install (Xiaoge Wang)</li> <li>2023-09-09 - New install (Xiaoge Wang)</li> <li>2023-09-09 - New install (Xiaoge Wang)</li> <li>2023-09-09 - New install (Xiaoge Wang)</li> <li>2023-09-08 - New install (Xiaoge Wang)</li> <li>2023-09-08 - New install (Xiaoge Wang)</li> <li>2023-09-08 - new install. (Xiaoge Wang)</li> <li>2023-08-31 - Added WPS module, companion to WRF (Nicholas Louis Panchy)</li> <li>2023-08-31 - Add a path to fix the error. (Xiaoge Wang)</li> <li>2023-08-29 - new install (Nanye Long)</li> <li>2023-08-28 - Install RStudio-Server/2022.07.02 (Craig Gross)</li> <li>2023-08-22 - add NextDenovo lua (Nanye Long)</li> <li>2023-08-22 - VASP built with VTST tools. (Xiaoge Wang)</li> <li>2023-08-21 - Add hwloc dependency to OpenMPI/2.1.2 (Craig Gross)</li> <li>2023-08-17 - New installation of a dependent (Xiaoge Wang)</li> <li>2023-08-17 - added tecplot360 2023R1 (Jim Leikert)</li> <li>2023-08-09 - SciPy-bundle built with foss-2022b and associated tools for unfinished R/4.3.1 (Nicholas Louis Panchy)</li> <li>2023-08-09 - More of the foss-2022b install fromthe the unfinished R.4.3.1 install (Nicholas Louis Panchy)</li> <li>2023-08-09 - Installed AnsysEM 2023R2 (Andrew Giles Fullard)</li> <li>2023-08-09 - Installed (some) of foss-2022b as part of unfinished R.4.3.1 install (Nicholas Louis Panchy)</li> <li>2023-08-07 - Initial commit (Andrew Giles Fullard)</li> </ul>","tags":["reference","software"]},{"location":"obtain_an_hpcc_account/","title":"Obtaining an HPCC account","text":"<p>Every user needs to have an account to use the HPCC. Below, we provide directions for different situations.</p>","tags":["accounts"]},{"location":"obtain_an_hpcc_account/#current-msu-affiliated-employees","title":"Current MSU-affiliated employees","text":"<p>HPCC accounts are free for all MSU researchers. To obtain an account, a Principal Investigator (PI) needs to complete an online New Account Request Form for themselves and their personnel (staff, students, staff, post-docs) who have an existing MSU NetID. Information needed includes:</p> <ul> <li>a list of the names and MSU NetIDs of personnel requiring HPCC accounts</li> <li>a statement on whether or not export controlled software or data will be used</li> <li>an abstract describing your research </li> </ul> <p>By applying for an HPCC account, the PI agrees that all group members will abide by MSU's Acceptable Use Policy.</p> <p>If you are going to attend an ICER workshop, you may get a temporary HPCC account. Please contact ICER for further instruction.</p>","tags":["accounts"]},{"location":"obtain_an_hpcc_account/#previous-msu-affiliated-hpcc-users","title":"Previous MSU-affiliated HPCC users","text":"<p>If you were previously affiliated with MSU and used to have an HPCC account, your account can be disabled by our yearly departed user cleanup. In order to reactivate your old account, a PI (e.g., your collaborator) needs to sponsor the renewal for you. They need to fill out the HPCC Sponsored Renewal Form on an annual basis.</p> <p>Note: All MSU Net IDs used to access the HPCC must be associated with a valid email address. If your previous MSU email no longer works, please provide us with an alternative email.</p>","tags":["accounts"]},{"location":"obtain_an_hpcc_account/#external-collaborators","title":"External collaborators","text":"<p>To request an HPCC account for an external collaborator, the PI must obtain a login-only, non-email, MSU Guest ID for their collaborator first. Either contact MSU ID Office at idoffice@msu.edu  or visit their website. Once the Guest ID is acquired for the collaborator, the PI can follow the above instructions, by filling out the New Account Request Form. Since the Guest ID is not associated with a valid email address, the PI is responsible for providing a working email of the collaborator when completing the request form.</p>","tags":["accounts"]},{"location":"obtain_an_hpcc_account/#other-affiliated-universities-in-michigan","title":"Other affiliated universities in Michigan","text":"<p>If you are affiliated with one of the universities listed below, please review their Documentation and email the Contact shown in the table below.</p> University Contact Documentation Oakland University Mario Nowak: nowak@oakland.eduTomas Hajek:hajek@oakland.edu University Technology Services Western Michigan University Leonard Peirce:leonard.peirce@wmich.edu Research and Innovation HPCHub Central Michigan University Mel Taylor:taylo1ml@cmich.edu Office of Information Technology <p>For MSU PIs who want to sponsor HPCC accounts for users from one of these affiliated universities, please follow the following instructions:</p> <ul> <li> <p>The affiliated user must first register for an MSU Guest Account. Make sure that an official .edu email address from the affiliated university is used for the registration; otherwise, the HPCC account will not be created.</p> </li> <li> <p>Once the MSU Guest Account has been activated for the affiliated user, they will need to complete the ICER Community ID Form, choosing MSU to authenticate, and using the credentials established while registering for the MSU Guest Account.</p> </li> <li> <p>Once the ICER Community ID Form is received by HPCC staff, we will reach out to the MSU PI for approval and create the account.</p> </li> </ul>","tags":["accounts"]},{"location":"orthomcl-pipeline/","title":"orthomcl-pipeline","text":"<p>OrthoMCL Pipeline (https://github.com/apetkau/orthomcl-pipeline) is a wrapper that automates running of OrthoMCL. If you prefer to run OrthoMCL from scratch, please skip this tutorial.</p>"},{"location":"orthomcl-pipeline/#installation-guide","title":"Installation guide","text":"<p>You could install orthomcl pipeline to your home directory (or research space), following the instruction of installing OrthoMCL pipeline. All the Perl dependencies have been installed by iCER staff, and you only need to run a couple of commands to complete the installation. Importantly, it's assumed that you have already prepared your MySQL configuration file.</p>"},{"location":"orthomcl-pipeline/#sample-installation","title":"Sample installation","text":"<p>I am going to install the pipeline in a subdirectory under my home <code>~/Software/</code>.</p> <p>Installing OrthoMCL Pipeline</p> <pre><code>ssh dev-intel18\n\n# Load necessary modules\nmodule purge\nmodule load icc/2016.3.210-GCC-5.4.0-2.26 impi/5.1.3.181\nmodule load OrthoMCL/2.0.9-custom-Perl-5.24.0\nmodule load BLAST/2.2.26-Linux_x86_64\nmodule load GCCcore/5.4.0 libxml2/2.9.4\n\n\n# Download source and configure\ncd Software\ngit clone https://github.com/apetkau/orthomcl-pipeline.git\ncd orthomcl-pipeline\nperl scripts/orthomcl-pipeline-setup.pl # set paths to dependencies\ncat etc/orthomcl-pipeline.conf # parameters in this file can be adjusted; consult the instruction linked above\n    # ---\n    # blast:\n    #   F: 'm S'\n    #   b: '100000'\n    #   e: '1e-5'\n    #   v: '100000'\n    # filter:\n    #   max_percent_stop: '20'\n    #   min_length: '10'\n    # mcl:\n    #   inflation: '1.5'\n    # path:\n    #   blastall: /opt/software/BLAST/2.2.26-Linux_x86_64/bin/blastall\n    #   formatdb: /opt/software/BLAST/2.2.26-Linux_x86_64/bin/formatdb\n    #   mcl: /opt/software/MCL/14.137-intel-2016b/bin/mcl\n    #   orthomcl: /opt/software/OrthoMCL/orthomclsoftware-custom/bin\n    # scheduler: fork\n    # split: '4'\n\n\n# Testing\nexport PATH=~/Software/orthomcl-pipeline/bin:~/Software/orthomcl-pipeline/scripts:$PATH\nperl t/test_pipeline.pl -m ~/Practice/general_test/orthomcl/my_orthomcl_dir/orthomcl.config -s fork -t ~/tmp # replace the path to orthomcl.config with your own\n</code></pre>"},{"location":"orthomcl-pipeline/#example-ortholog-identification","title":"Example: ortholog identification","text":"<p>The tutorial is adapted from a tutorial hosted at https://github.com/apetkau/microbial-informatics-2014/tree/master/labs/orthomcl. We strongly recommend that you read it fully before starting the hands-on practice below, which is a much simplified version of the original one and serves as a demo only. The datasets containing a set of V. Cholerae genomes are located in <code>mnt/research/common-data/Bio/orthomcl-data/</code>.</p> <pre><code>ssh dev-intel18\n\n# Then go to your orthomcl working directory\n\n# Load necessary modules\nmodule purge\nmodule load icc/2016.3.210-GCC-5.4.0-2.26  impi/5.1.3.181\nmodule load OrthoMCL/2.0.9-custom-Perl-5.24.0\nmodule load BLAST/2.2.26-Linux_x86_64\nmodule load GCCcore/5.4.0 libxml2/2.9.4\nexport PATH=~/Software/orthomcl-pipeline/bin:~/Software/orthomcl-pipeline/scripts:$PATH\n\n# Run orthomcl pipeline (replace the path to orthomcl.config with your own)\northomcl-pipeline -i /mnt/research/common-data/Bio/orthomcl-data -o orthomcl_out_tmp -m ~/Practice/general_test/orthomcl/my_orthomcl_dir/orthomcl.config --nocompliant\n\n# Visualize the results by drawing a Venn Diagram using a pipeline utility script\nnml_parse_orthomcl.pl -i orthomcl_out_tmp/groups/groups.txt -g /mnt/research/common-data/Bio/orthomcl-data/genome-groups.txt -s --draw -o orthomcl-stats.txt --genes\n\n# View the Venn Diagram plot (just for demo; you should transfer the svg to your local computer for better display effect)\njava -jar /opt/software/batik/batik-1.9/batik-squiggle-1.9.jar genome-groups.txt.svg\n</code></pre> <p>Note</p> <p>As mentioned in the full tutorial, you need to answer \"yes\" to the database removal question in the course of the run.</p>"},{"location":"parallel_computing/","title":"Parallel Computing","text":"<p>Here we introduce three basic parallel models: Shared Memory, Distributed Memory and Hybrid.   Images are taken from the Lawrence Livermore National Lab's Parallel Computing Tutorial. Visit their site to learn more.</p>"},{"location":"parallel_computing/#shared-memory-with-threads","title":"Shared Memory with Threads","text":"<ul> <li>A main program loads and acquires all of the necessary resources to run the \"heavy weight\" process.</li> <li>It performs some serial work, and then creates a number of threads (\"light weight\") run by CPU cores concurrently.</li> <li>Each thread can have local data, but also, shares the entire resources, including memory of the main program.</li> <li>Threads communicate with each other through global memory (RAM, Random Access Memory). This requires synchronization operations to ensure that no than one thread is updating the same RAM address at any time.</li> <li>Threads can come and go, but the main program remains present to provide the necessary shared resources until the application has completed.</li> </ul> <p>Examples: POSIX Threads, OpenMP, CUDA threads for GPUs</p>"},{"location":"parallel_computing/#distributed-memory-with-tasks","title":"Distributed Memory with Tasks","text":"<ul> <li> <p>A main program creates a set of tasks that use their own local memory during computation. Multiple tasks can reside on the same physical machine and/or across an arbitrary number of machines.</p> </li> <li> <p>Tasks exchange data through communications by sending and receiving messages through fast network (e.g. infinite band).</p> </li> <li> <p>Data transfer usually requires cooperative operations to be performed by each process. For example, a send operation must have a matching receive operation.</p> </li> <li> <p>Synchronization operations are also required to prevent a race condition. Example: Message Passing Interface (MPI)</p> </li> </ul>"},{"location":"parallel_computing/#hybrid-parallel","title":"Hybrid Parallel","text":"<ul> <li>A hybrid model combines more than one of the previously described programming models.</li> <li> <p>A simple example is the combination of the message passing model (MPI) with the threads model (OpenMP).</p> <ul> <li>Threads perform computationally intensive kernels using local, on-node data</li> <li>Communications between processes on different nodes occurs over the network using MPI</li> </ul> </li> <li> <p>Works well to the most popular hardware environment of clustered multi/many-core machines.</p> </li> <li> <p>Other example: MPI with CPU-GPU (Graphics Processing Unit)</p> </li> </ul> <p>Hybrid OpenMP-MPI Parallel Model:</p> <p></p> <p>Hybrid CUDA-MPI Parallel Model:</p> <p></p>"},{"location":"qs/","title":"qs","text":"<p>qs is a powertools program that will display list of jobs currently on the SLURM queue for a giver user account. For more infomration about scheduling jobs on SLURM, see here. Running qs on its own will generate the following table listing your jobs that are currently on the queue:</p> <pre><code>$ qs\n\nTue Feb  7 12:29:28 EST 2023\n                                                                                            Start_Time/\n          JobID         User    Account      Name  Node CPUs  TotMem    GPU    WallTime  ST  Elapsed_Time  NodeList(Reason)\n---------------------------------------------------------------------------------------------------------------------------\n         4304840     panchyni   general test_SLURM    1    1    750M    N/A        10:00  R         0:05   css-118\n         4304842     panchyni   general test_SLURM    1    1    750M    N/A        10:00  R         0:05   css-121\n</code></pre> <p>A full description of arguments for qs can be found by acccessing the help for the program by runnig \"qs -h\" on the command line (this information is also reproduced below)</p> <pre><code>$ qs -h\n\nUsage:   -a  --&gt;  all jobs\n         -F  --&gt;  all (52) fields\n         -E  --&gt;  every (107) fields\n         -j  --&gt;  specific job\n         -u  --&gt;  specific user\n         -r  --&gt;  all job array elements\n\nDefault: -u $USER\n</code></pre>","tags":["reference"]},{"location":"virtual_help_desk/","title":"Virtual Help Desk by Microsoft Teams and Zoom","text":"<p>ICER offers virtual helpdesk office hours (every Monday and Thursday 1:00-2:00pm) online without walk-in. Users can reach us either through Microsoft Teams App or just a web browser.</p> <p>Please click on the ICER Help Desk link . It will take you to the launcher web site of Microsoft Teams:</p> <p></p> <p>You can now choose to use a web browser or Microsoft Teams to access our Help Desk channel.</p> <p>If you do not want to install and use Microsoft Teams, you can click on Use the web app instead to enter ICER Help Desk channel: </p> <p>If you would like to use Microsoft Teams but have not installed one in your computer yet, please click on Get the Teams app. If Microsoft Teams is installed already, you can click on Launch it now. If a \"Launch Appliction\" window pops out, choose Microsoft Teams  to open our Help Desk channel link:</p> <p></p> <p>Once you are in the channel, please ask your questions in the text bar located at the bottom of the window:</p> <p></p> <p>Click on the  button so we are able to see the message and help you. We can start a conversation and arrange a Zoom\u00a0   Zoom's Microsoft Teams integration has been set up to start or join an instant meeting right from our conversation. You may enter \"@zoom help\" or type \"@zoom\" in the text bar and click on\u00a0  Zoom to see a list of commands. To find out how to download or use Zoom, please visit the MSU Zoom page.</p>"},{"location":"workshop_slides/","title":"Workshop materials","text":"<p>Note</p> <p>These materials are made available for reference by attendees of past ICER workshops. Though they contain useful information, they are not meant as standalone documents that will be regularly updated. If would like to participate in a future workshop covering these materials or something similar, please visit ICER's Upcoming Seminars and Workshops page.</p> <ul> <li>R for the HPCC (last update 09/19/2023, by Craig Gross): website</li> <li>Introduction to Linux for High-Performance Computing (last update 01/31/2023, by Mahmoud Parvizi): pdf</li> <li>Introduction to HPCC (last update 7/2022, by Nanye Long): pdf</li> <li>Writing SLURM job scripts (last update 7/2022, by Nanye Long): pdf</li> <li>From PC to HPC (by Xiaoge Wang): pdf</li> </ul>"},{"location":"tags/","title":"Tags","text":"<p>Docs pages organized by tag.</p>"},{"location":"tags/#accounts","title":"accounts","text":"<ul> <li>Buy-in and account management</li> <li>Obtaining an HPCC account</li> </ul>"},{"location":"tags/#alphafold","title":"AlphaFold","text":"<ul> <li>(2023-07-18) Lab Notebook: AlphaFold 2.3.1 DEPRECATED</li> <li>(2023-11-10) Lab Notebook: AlphaFold 2.3.2 WorkInProgress</li> <li>AlphaFold on HPCC</li> <li>AlphaFold via Singularity</li> </ul>"},{"location":"tags/#antismash","title":"AntiSMASH","text":"<ul> <li>(2022-06-27) Lab Notebook: AntiSMASH installation with Conda</li> </ul>"},{"location":"tags/#bactopia","title":"Bactopia","text":"<ul> <li>(2022-10-03) Lab Notebook: Bactopia installaion with Conda and Mamba</li> </ul>"},{"location":"tags/#bioinformatics","title":"bioinformatics","text":"<ul> <li>Mothur</li> <li>QIIME 2</li> </ul>"},{"location":"tags/#buyin","title":"buyin","text":"<ul> <li>Buy-in and account management</li> <li>SLURM queueing and partitions</li> </ul>"},{"location":"tags/#centos-8","title":"CentOS 8","text":"<ul> <li>(2022-09-23) Lab Notebook: Singularity --- Using an alternate OS</li> </ul>"},{"location":"tags/#checkpointing","title":"checkpointing","text":"<ul> <li>Checkpoint with DMTCP</li> <li>Overview</li> <li>Powertools <code>longjob</code> by DMTCP</li> </ul>"},{"location":"tags/#clean-up","title":"clean up","text":"<ul> <li>(2022-12-20) Lab Notebook: Anaconda --- Cleanning out your cache on HPCC</li> </ul>"},{"location":"tags/#cloud","title":"cloud","text":"<ul> <li>Cloud storage file transfer</li> </ul>"},{"location":"tags/#command-line","title":"command line","text":"<ul> <li>Conditional statements</li> <li>Expansion</li> <li>Linux Command Line for Beginners I</li> <li>Linux Command Line for Beginners II</li> <li>Loops</li> <li>Regular expressions</li> <li>Variables I</li> <li>Variables II</li> </ul>"},{"location":"tags/#compilers","title":"compilers","text":"<ul> <li>Compilers and Libraries</li> <li>Compiling for GPUs</li> <li>make</li> </ul>"},{"location":"tags/#conda","title":"Conda","text":"<ul> <li>(2022-06-27) Lab Notebook: AntiSMASH installation with Conda</li> <li>(2022-09-20) Lab Notebook: PyMesh installation with Conda</li> <li>(2022-09-24) Lab Notebook: Singularity Overlays</li> <li>(2022-10-03) Lab Notebook: Bactopia installaion with Conda and Mamba</li> <li>(2022-10-08) Lab Notebook: Praat installation and execution using Singularity Overlays on CentOS8</li> <li>(2022-10-19) Lab Notebook: OpenVDB installaion with Conda</li> <li>(2022-10-19) Lab Notebook: TractSeg installation with Conda</li> <li>(2022-12-20) Lab Notebook: Anaconda --- Cleanning out your cache on HPCC</li> <li>Installing TF using anaconda</li> <li>Installing Pytorch with Anaconda</li> <li>Using Conda</li> </ul>"},{"location":"tags/#containers","title":"containers","text":"<ul> <li>Overview</li> <li>Docker</li> <li>Singularity Advanced Topics</li> <li>Singularity Introduction</li> </ul>"},{"location":"tags/#cryosparc","title":"CryoSPARC","text":"<ul> <li>(2023-02-16) Lab Notebook: CryoSPARC Installation on HPCC</li> </ul>"},{"location":"tags/#crystal","title":"Crystal","text":"<ul> <li>(2023-03-14) Lab Notebook: CRYSTAL23 --- Parallel submission script on the MSU HPCC</li> </ul>"},{"location":"tags/#csh","title":"csh","text":"<ul> <li>(2023-08-02) Lab Notebook: Alternative Linux Shells on HPCC</li> </ul>"},{"location":"tags/#dashboard","title":"dashboard","text":"<ul> <li>Dashboard</li> </ul>"},{"location":"tags/#data-machine","title":"data machine","text":"<ul> <li>Data Machine overview</li> <li>Using the Data Machine</li> </ul>"},{"location":"tags/#ddd","title":"DDD","text":"<ul> <li>Data Display Debugger</li> </ul>"},{"location":"tags/#debugging","title":"Debugging","text":"<ul> <li>(2022-12-07) Lab Notebook: OnDemand --- Bad Request from browser</li> <li>Data Display Debugger</li> </ul>"},{"location":"tags/#docker","title":"Docker","text":"<ul> <li>Overview</li> <li>Docker</li> <li>Singularity Advanced Topics</li> </ul>"},{"location":"tags/#drive-mapping","title":"drive mapping","text":"<ul> <li>Mapping drives using SSHFS</li> <li>Mapping drives using Samba</li> </ul>"},{"location":"tags/#easybuild","title":"EasyBuild","text":"<ul> <li>(2023-02-07) Lab Notebook: WRF installation with EasyBuild</li> <li>(2023-05-18) Lab Notebook: EasyBuild PostgreSQL Installation</li> <li>EasyBuild Reference</li> <li>EasyBuild Tutorial</li> </ul>"},{"location":"tags/#explanation","title":"explanation","text":"<ul> <li>Overview</li> <li>Data Machine overview</li> <li>File permissions</li> <li>HPCC layout</li> <li>Home space</li> <li>Overview</li> <li>Overview</li> <li>Python on HPCC</li> <li>Research space</li> <li>Overview</li> <li>Scratch space</li> <li>Singularity Advanced Topics</li> <li>Migrating from Torque</li> </ul>"},{"location":"tags/#faq","title":"FAQ","text":"<ul> <li>FAQ</li> </ul>"},{"location":"tags/#files","title":"files","text":"<ul> <li>(2023-02-09) Lab Notebooks: Rclone --- Example of copying folder from HPCC to MSU OneDrive</li> <li>(2024-01-13) Lab Notebook: Sharing directories with other HPCC users</li> <li>File count</li> <li>File permissions</li> <li>Guidelines for choosing file systems and I/O</li> <li>Home space</li> <li>Managing file permissions</li> <li>Research space</li> <li>Scratch space</li> <li>Sensitive data storage</li> </ul>"},{"location":"tags/#git","title":"git","text":"<ul> <li>(2022-09-28) Lab Notebook: HPCC via RStudio and GitHub</li> <li>(2023-11-21) Lab Notebook: Git in Research Spaces</li> </ul>"},{"location":"tags/#globus","title":"globus","text":"<ul> <li>Overview</li> </ul>"},{"location":"tags/#gpu","title":"GPU","text":"<ul> <li>Compiling for GPUs</li> <li>GPU resources</li> <li>Requesting GPUs</li> <li>TF GPU usage</li> </ul>"},{"location":"tags/#groups","title":"groups","text":"<ul> <li>Change primary group</li> <li>File permissions</li> <li>Managing file permissions</li> </ul>"},{"location":"tags/#how-to-guide","title":"how-to guide","text":"<ul> <li>SSH tunneling to directly access development nodes</li> <li>Backwards compatibility</li> <li>Buy-in and account management</li> <li>Change primary group</li> <li>SSH connection via VS Code</li> <li>Connect to the HPCC</li> <li>Connections to compute nodes</li> <li>Editing text with nano</li> <li>File transfer</li> <li>Installing TF using anaconda</li> <li>Installing Pytorch with Anaconda</li> <li>Jupyter Notebooks in VS Code</li> <li>Managing file permissions</li> <li>Preparing for the OS upgrade</li> <li>Cloud storage file transfer</li> <li>SLURM resource request guide</li> <li>SSH keys</li> <li>Scavenger Queue</li> <li>Tensorflow Model Training Code Examples</li> <li>Overview</li> <li>Using Conda</li> </ul>"},{"location":"tags/#hrldas","title":"HRLDAS","text":"<ul> <li>(2023-02-27) Lab Notebook: HRLDAS</li> </ul>"},{"location":"tags/#io","title":"I/O","text":"<ul> <li>(2023-08-30) Lab Notebooks: Incident Report on System Slowdown due to I/O</li> <li>Guidelines for choosing file systems and I/O</li> </ul>"},{"location":"tags/#incident-report","title":"incident report","text":"<ul> <li>(2023-08-30) Lab Notebooks: Incident Report on System Slowdown due to I/O</li> </ul>"},{"location":"tags/#interactive-desktop","title":"interactive desktop","text":"<ul> <li>(2022-10-04) Lab Notebook: OnDemand --- Unable to click icons on the Interactive Desktop</li> </ul>"},{"location":"tags/#jags","title":"JAGS","text":"<ul> <li>Some packages and other information</li> </ul>"},{"location":"tags/#job-script","title":"job script","text":"<ul> <li>Example job scripts</li> <li>Writing and submitting job scripts</li> <li>List of job specifications</li> <li>SLURM environment variables</li> </ul>"},{"location":"tags/#jupyter","title":"Jupyter","text":"<ul> <li>(2023-06-30) Lab Notebooks: Jupyter --- Port-Forwarding Servers</li> <li>(2023-08-11) Lab Notebooks: Jupyter and MobaXterm--- Port-Forwarding Servers</li> <li>Jupyter Notebooks in VS Code</li> <li>Python on HPCC</li> </ul>"},{"location":"tags/#lab-notebook","title":"lab notebook","text":"<ul> <li>(2022-06-27) Lab Notebook: AntiSMASH installation with Conda</li> <li>(2022-09-20) Lab Notebook: PyMesh installation with Conda</li> <li>(2022-09-21) Lab Notebook: VisIt on HPCC</li> <li>(2022-09-23) Lab Notebook: Singularity --- Using an alternate OS</li> <li>(2022-09-24) Lab Notebook: Singularity Overlays</li> <li>(2022-09-28) Lab Notebook: HPCC via RStudio and GitHub</li> <li>(2022-09-30) Lab Notebook: ROS on HPCC</li> <li>(2022-10-03) Lab Notebook: Bactopia installaion with Conda and Mamba</li> <li>(2022-10-04) Lab Notebook: OnDemand --- Unable to click icons on the Interactive Desktop</li> <li>(2022-10-08) Lab Notebook: Praat installation and execution using Singularity Overlays on CentOS8</li> <li>(2022-10-10) Lab Notebook: OnDemand Interactive Desktop Error</li> <li>(2022-10-19) Lab Notebook: OpenVDB installaion with Conda</li> <li>(2022-10-19) Lab Notebook: TractSeg installation with Conda</li> <li>(2022-10-27) Lab Notebook: SFTP Mapping on HPCC File Systems</li> <li>(2022-12-07) Lab Notebook: OnDemand --- Bad Request from browser</li> <li>(2022-12-20) Lab Notebook: Anaconda --- Cleanning out your cache on HPCC</li> <li>(2022-12-20) Lab Notebook: SSHFS mapping on Windows</li> <li>(2023-02-07) Lab Notebook:  NAMD --- Running on Multile Nodes</li> <li>(2023-02-07) Lab Notebook: WRF installation with EasyBuild</li> <li>(2023-02-09) Lab Notebooks: Rclone --- Example of copying folder from HPCC to MSU OneDrive</li> <li>(2023-02-16) Lab Notebook: CryoSPARC Installation on HPCC</li> <li>(2023-02-27) Lab Notebook: HRLDAS</li> <li>(2023-03-14) Lab Notebook: CRYSTAL23 --- Parallel submission script on the MSU HPCC</li> <li>(2023-03-14) Lab Notebook: VASP</li> <li>(2023-06-30) Lab Notebooks: Jupyter --- Port-Forwarding Servers</li> <li>(2023-03-27) Lab Notebook: Mathematica --- Fixing Startup Errors</li> <li>(2023-04-04) Lab Notebook: OnDemand RunningMPIJobs</li> <li>(2023-05-04) Lab Notebook: Changes to srun for jobs with multiple CPUs per task</li> <li>(2023-05-18) Lab Notebook: EasyBuild PostgreSQL Installation</li> <li>(2023-07-13) Lab Notebook: Ignoring Local Python Libraries</li> <li>(2023-07-18) Lab Notebook: AlphaFold 2.3.1 DEPRECATED</li> <li>(2023-07-18) Lab Notebook: MATLAB offload from localPC to HPCC</li> <li>(2023-08-02) Lab Notebook: Alternative Linux Shells on HPCC</li> <li>(2023-08-11) Lab Notebooks: Jupyter and MobaXterm--- Port-Forwarding Servers</li> <li>(2023-08-30) Lab Notebooks: Incident Report on System Slowdown due to I/O</li> <li>(2023-09-21) Lab Notebooks: Example home directory software install (with links to videos)</li> <li>(2023-11-03) Lab Notebook: Connecting to a Singularity container with VS Code</li> <li>(2023-11-09) Lab Notebook: Molpro Parallel</li> <li>(2023-11-10) Lab Notebook: AlphaFold 2.3.2 WorkInProgress</li> <li>(2023-11-21) Lab Notebook: Git in Research Spaces</li> <li>(2024-01-13) Lab Notebook: Sharing directories with other HPCC users</li> </ul>"},{"location":"tags/#linux","title":"Linux","text":"<ul> <li>(2023-08-02) Lab Notebook: Alternative Linux Shells on HPCC</li> </ul>"},{"location":"tags/#make","title":"make","text":"<ul> <li>make</li> </ul>"},{"location":"tags/#many-files","title":"many files","text":"<ul> <li>(2022-12-20) Lab Notebook: Anaconda --- Cleanning out your cache on HPCC</li> </ul>"},{"location":"tags/#mathematica","title":"Mathematica","text":"<ul> <li>(2023-03-27) Lab Notebook: Mathematica --- Fixing Startup Errors</li> </ul>"},{"location":"tags/#matlab","title":"MATLAB","text":"<ul> <li>(2023-07-18) Lab Notebook: MATLAB offload from localPC to HPCC</li> <li>Matlab</li> </ul>"},{"location":"tags/#mobaxterm","title":"MobaXterm","text":"<ul> <li>(2023-08-11) Lab Notebooks: Jupyter and MobaXterm--- Port-Forwarding Servers</li> </ul>"},{"location":"tags/#modules","title":"modules","text":"<ul> <li>User created modules</li> </ul>"},{"location":"tags/#molpro","title":"Molpro","text":"<ul> <li>(2023-11-09) Lab Notebook: Molpro Parallel</li> </ul>"},{"location":"tags/#mothur","title":"Mothur","text":"<ul> <li>Mothur</li> </ul>"},{"location":"tags/#mpi","title":"MPI","text":"<ul> <li>(2023-04-04) Lab Notebook: OnDemand RunningMPIJobs</li> </ul>"},{"location":"tags/#namd","title":"NAMD","text":"<ul> <li>(2023-02-07) Lab Notebook:  NAMD --- Running on Multile Nodes</li> </ul>"},{"location":"tags/#nano","title":"nano","text":"<ul> <li>Editing text with nano</li> </ul>"},{"location":"tags/#ondemand","title":"OnDemand","text":"<ul> <li>(2022-10-04) Lab Notebook: OnDemand --- Unable to click icons on the Interactive Desktop</li> <li>(2022-10-08) Lab Notebook: Praat installation and execution using Singularity Overlays on CentOS8</li> <li>(2022-10-10) Lab Notebook: OnDemand Interactive Desktop Error</li> <li>(2022-12-07) Lab Notebook: OnDemand --- Bad Request from browser</li> <li>(2023-04-04) Lab Notebook: OnDemand RunningMPIJobs</li> <li>File transfer</li> <li>Quick start - Open OnDemand</li> </ul>"},{"location":"tags/#openbugs","title":"OpenBUGS","text":"<ul> <li>Some packages and other information</li> </ul>"},{"location":"tags/#openvdb","title":"OpenVDB","text":"<ul> <li>(2022-10-19) Lab Notebook: OpenVDB installaion with Conda</li> </ul>"},{"location":"tags/#os-upgrade","title":"OS upgrade","text":"<ul> <li>Backwards compatibility</li> <li>Nodes available for testing the new OS</li> <li>Overview</li> <li>Preparing for the OS upgrade</li> </ul>"},{"location":"tags/#parallel-comuting-toolbox","title":"Parallel Comuting Toolbox","text":"<ul> <li>(2023-07-18) Lab Notebook: MATLAB offload from localPC to HPCC</li> </ul>"},{"location":"tags/#partitions","title":"partitions","text":"<ul> <li>Buy-in and account management</li> <li>SLURM queueing and partitions</li> <li>Scavenger Queue</li> </ul>"},{"location":"tags/#perl","title":"Perl","text":"<ul> <li>Installing Local Perl Modules with CPAN</li> </ul>"},{"location":"tags/#postgresql","title":"PostgreSQL","text":"<ul> <li>(2023-05-18) Lab Notebook: EasyBuild PostgreSQL Installation</li> </ul>"},{"location":"tags/#powertools","title":"powertools","text":"<ul> <li>Powertools <code>longjob</code> by DMTCP</li> </ul>"},{"location":"tags/#pymesh","title":"PyMesh","text":"<ul> <li>(2022-09-20) Lab Notebook: PyMesh installation with Conda</li> </ul>"},{"location":"tags/#python","title":"Python","text":"<ul> <li>(2023-07-13) Lab Notebook: Ignoring Local Python Libraries</li> <li>Python on HPCC</li> <li>Using Conda</li> </ul>"},{"location":"tags/#pytorch","title":"PyTorch","text":"<ul> <li>Installing Pytorch with Anaconda</li> </ul>"},{"location":"tags/#qiime-2","title":"QIIME 2","text":"<ul> <li>QIIME 2</li> </ul>"},{"location":"tags/#queue","title":"queue","text":"<ul> <li>SLURM resource request guide</li> </ul>"},{"location":"tags/#quota","title":"quota","text":"<ul> <li>File count</li> <li>Home space</li> <li>Research space</li> <li>Scratch space</li> </ul>"},{"location":"tags/#r","title":"R","text":"<ul> <li>(2023-05-18) Lab Notebook: EasyBuild PostgreSQL Installation</li> <li>General information</li> <li>Some packages and other information</li> </ul>"},{"location":"tags/#rclone","title":"rclone","text":"<ul> <li>(2023-02-09) Lab Notebooks: Rclone --- Example of copying folder from HPCC to MSU OneDrive</li> <li>Cloud storage file transfer</li> </ul>"},{"location":"tags/#reference","title":"reference","text":"<ul> <li>Cluster resources</li> <li>Compilers and Libraries</li> <li>Compiling for GPUs</li> <li>Conditional statements</li> <li>SLURM - node status and job partition</li> <li>EasyBuild Reference</li> <li>Example job scripts</li> <li>External Resources</li> <li>File count</li> <li>FAQ</li> <li>GPU resources</li> <li>Guidelines for choosing file systems and I/O</li> <li>How jobs are scheduled</li> <li>Searching software modules</li> <li>Classroom support</li> <li>Interactive jobs</li> <li>Linux shells</li> <li>List of job specifications</li> <li>Local file system</li> <li>Loops</li> <li>Nodes available for testing the new OS</li> <li>Quick start - Open OnDemand</li> <li>Regular expressions</li> <li>Requesting GPUs</li> <li>SLURM queueing and partitions</li> <li>SLURM commands</li> <li>Sensitive data storage</li> <li>SLURM environment variables</li> <li>User created modules</li> <li>Using the Data Machine</li> <li>SLURM - buyin information</li> <li>Development nodes</li> <li>Get software usage examples</li> <li>Install an SSH client</li> <li>Job policies</li> <li>SLURM - display job steps and their resource usages</li> <li>Module changelog</li> <li>SLURM - display job list</li> </ul>"},{"location":"tags/#ros","title":"ROS","text":"<ul> <li>(2022-09-30) Lab Notebook: ROS on HPCC</li> </ul>"},{"location":"tags/#rstudio","title":"RStudio","text":"<ul> <li>(2022-09-28) Lab Notebook: HPCC via RStudio and GitHub</li> </ul>"},{"location":"tags/#rsync","title":"rsync","text":"<ul> <li>File transfer</li> <li>Cloud storage file transfer</li> </ul>"},{"location":"tags/#samba","title":"samba","text":"<ul> <li>Mapping drives using Samba</li> </ul>"},{"location":"tags/#scp","title":"scp","text":"<ul> <li>File transfer</li> </ul>"},{"location":"tags/#sensitive-data","title":"sensitive data","text":"<ul> <li>Sensitive data storage</li> </ul>"},{"location":"tags/#sftp","title":"sftp","text":"<ul> <li>(2022-10-27) Lab Notebook: SFTP Mapping on HPCC File Systems</li> </ul>"},{"location":"tags/#shell","title":"shell","text":"<ul> <li>(2023-08-02) Lab Notebook: Alternative Linux Shells on HPCC</li> </ul>"},{"location":"tags/#singularity","title":"Singularity","text":"<ul> <li>(2022-09-23) Lab Notebook: Singularity --- Using an alternate OS</li> <li>(2022-09-24) Lab Notebook: Singularity Overlays</li> <li>(2022-10-08) Lab Notebook: Praat installation and execution using Singularity Overlays on CentOS8</li> <li>(2023-07-13) Lab Notebook: Ignoring Local Python Libraries</li> <li>(2023-07-18) Lab Notebook: AlphaFold 2.3.1 DEPRECATED</li> <li>(2023-11-03) Lab Notebook: Connecting to a Singularity container with VS Code</li> <li>(2023-11-10) Lab Notebook: AlphaFold 2.3.2 WorkInProgress</li> <li>AlphaFold via Singularity</li> <li>Overview</li> <li>Singularity Advanced Topics</li> <li>Singularity Introduction</li> </ul>"},{"location":"tags/#slurm","title":"slurm","text":"<ul> <li>(2023-05-04) Lab Notebook: Changes to srun for jobs with multiple CPUs per task</li> <li>Example job scripts</li> <li>How jobs are scheduled</li> <li>Interactive jobs</li> <li>Writing and submitting job scripts</li> <li>List of job specifications</li> <li>SLURM commands</li> <li>Overview</li> <li>SLURM resource request guide</li> <li>Scavenger Queue</li> <li>Showing job steps</li> <li>SLURM environment variables</li> <li>Migrating from Torque</li> <li>Job policies</li> </ul>"},{"location":"tags/#software","title":"software","text":"<ul> <li>(2023-09-21) Lab Notebooks: Example home directory software install (with links to videos)</li> <li>Module changelog</li> </ul>"},{"location":"tags/#srun","title":"srun","text":"<ul> <li>(2023-05-04) Lab Notebook: Changes to srun for jobs with multiple CPUs per task</li> <li>Showing job steps</li> </ul>"},{"location":"tags/#ssh","title":"ssh","text":"<ul> <li>(2023-06-30) Lab Notebooks: Jupyter --- Port-Forwarding Servers</li> <li>(2023-08-11) Lab Notebooks: Jupyter and MobaXterm--- Port-Forwarding Servers</li> <li>SSH tunneling to directly access development nodes</li> <li>SSH connection via VS Code</li> <li>Connect to the HPCC</li> <li>Connections to compute nodes</li> <li>SSH keys</li> <li>Install an SSH client</li> </ul>"},{"location":"tags/#sshfs","title":"sshfs","text":"<ul> <li>(2022-12-20) Lab Notebook: SSHFS mapping on Windows</li> <li>Mapping drives using SSHFS</li> </ul>"},{"location":"tags/#tcsh","title":"tcsh","text":"<ul> <li>(2023-08-02) Lab Notebook: Alternative Linux Shells on HPCC</li> </ul>"},{"location":"tags/#tensorflow","title":"TensorFlow","text":"<ul> <li>Installing TF using anaconda</li> <li>Some packages and other information</li> <li>TF GPU usage</li> <li>Tensorflow Model Training Code Examples</li> </ul>"},{"location":"tags/#text-editors","title":"text editors","text":"<ul> <li>Editing text with nano</li> </ul>"},{"location":"tags/#torque","title":"torque","text":"<ul> <li>Migrating from Torque</li> </ul>"},{"location":"tags/#tractseg","title":"TractSeg","text":"<ul> <li>(2022-10-19) Lab Notebook: TractSeg installation with Conda</li> </ul>"},{"location":"tags/#training","title":"training","text":"<ul> <li>External Resources</li> </ul>"},{"location":"tags/#tutorial","title":"tutorial","text":"<ul> <li>AlphaFold on HPCC</li> <li>AlphaFold via Singularity</li> <li>Checkpoint with DMTCP</li> <li>Data Display Debugger</li> <li>Docker</li> <li>EasyBuild Tutorial</li> <li>Expansion</li> <li>Installing Local Perl Modules with CPAN</li> <li>Introduction to the module system</li> <li>Writing and submitting job scripts</li> <li>Linux Command Line for Beginners I</li> <li>Linux Command Line for Beginners II</li> <li>Mapping drives using SSHFS</li> <li>Mapping drives using Samba</li> <li>Matlab</li> <li>Mothur</li> <li>Powertools <code>longjob</code> by DMTCP</li> <li>QIIME 2</li> <li>General information</li> <li>Some packages and other information</li> <li>Showing job steps</li> <li>Singularity Introduction</li> <li>Using Conda</li> <li>Variables I</li> <li>Variables II</li> <li>make</li> </ul>"},{"location":"tags/#vasp","title":"VASP","text":"<ul> <li>(2023-03-14) Lab Notebook: VASP</li> </ul>"},{"location":"tags/#visit","title":"VisIt","text":"<ul> <li>(2022-09-21) Lab Notebook: VisIt on HPCC</li> </ul>"},{"location":"tags/#vs-code","title":"VS Code","text":"<ul> <li>(2023-11-03) Lab Notebook: Connecting to a Singularity container with VS Code</li> <li>SSH connection via VS Code</li> <li>Jupyter Notebooks in VS Code</li> </ul>"},{"location":"tags/#vscode","title":"VScode","text":"<ul> <li>(2023-11-03) Lab Notebook: Connecting to a Singularity container with VS Code</li> <li>SSH connection via VS Code</li> <li>Jupyter Notebooks in VS Code</li> </ul>"},{"location":"tags/#windows","title":"Windows","text":"<ul> <li>(2022-12-20) Lab Notebook: SSHFS mapping on Windows</li> </ul>"},{"location":"tags/#wrf","title":"WRF","text":"<ul> <li>(2023-02-07) Lab Notebook: WRF installation with EasyBuild</li> </ul>"},{"location":"tags/#zsh","title":"zsh","text":"<ul> <li>(2023-08-02) Lab Notebook: Alternative Linux Shells on HPCC</li> </ul>"},{"location":"Lab_Notebooks/","title":"Lab Notebooks","text":"<p>Warning</p> <p>The following Lab Notebooks are intended as a record of how particular problems were solved at a particular time and not updated or maintained in any way to reflect current system setting or versions of installed software. If you are having a problem with software/topic addressed by one of these notebooks, they may provide a solution or at least a starting point, but there is no guarantee that worked here will work again.</p>","tags":["lab notebook"]},{"location":"Lab_Notebooks/#notebooks","title":"Notebooks","text":"","tags":["lab notebook"]},{"location":"Lab_Notebooks/#lab-notebook","title":"lab notebook","text":"<ul> <li>(2022-06-27) Lab Notebook: AntiSMASH installation with Conda</li> <li>(2022-09-20) Lab Notebook: PyMesh installation with Conda</li> <li>(2022-09-21) Lab Notebook: VisIt on HPCC</li> <li>(2022-09-23) Lab Notebook: Singularity --- Using an alternate OS</li> <li>(2022-09-24) Lab Notebook: Singularity Overlays</li> <li>(2022-09-28) Lab Notebook: HPCC via RStudio and GitHub</li> <li>(2022-09-30) Lab Notebook: ROS on HPCC</li> <li>(2022-10-03) Lab Notebook: Bactopia installaion with Conda and Mamba</li> <li>(2022-10-04) Lab Notebook: OnDemand --- Unable to click icons on the Interactive Desktop</li> <li>(2022-10-08) Lab Notebook: Praat installation and execution using Singularity Overlays on CentOS8</li> <li>(2022-10-10) Lab Notebook: OnDemand Interactive Desktop Error</li> <li>(2022-10-19) Lab Notebook: OpenVDB installaion with Conda</li> <li>(2022-10-19) Lab Notebook: TractSeg installation with Conda</li> <li>(2022-10-27) Lab Notebook: SFTP Mapping on HPCC File Systems</li> <li>(2022-12-07) Lab Notebook: OnDemand --- Bad Request from browser</li> <li>(2022-12-20) Lab Notebook: Anaconda --- Cleanning out your cache on HPCC</li> <li>(2022-12-20) Lab Notebook: SSHFS mapping on Windows</li> <li>(2023-02-07) Lab Notebook:  NAMD --- Running on Multile Nodes</li> <li>(2023-02-07) Lab Notebook: WRF installation with EasyBuild</li> <li>(2023-02-09) Lab Notebooks: Rclone --- Example of copying folder from HPCC to MSU OneDrive</li> <li>(2023-02-16) Lab Notebook: CryoSPARC Installation on HPCC</li> <li>(2023-02-27) Lab Notebook: HRLDAS</li> <li>(2023-03-14) Lab Notebook: CRYSTAL23 --- Parallel submission script on the MSU HPCC</li> <li>(2023-03-14) Lab Notebook: VASP</li> <li>(2023-06-30) Lab Notebooks: Jupyter --- Port-Forwarding Servers</li> <li>(2023-03-27) Lab Notebook: Mathematica --- Fixing Startup Errors</li> <li>(2023-04-04) Lab Notebook: OnDemand RunningMPIJobs</li> <li>(2023-05-04) Lab Notebook: Changes to srun for jobs with multiple CPUs per task</li> <li>(2023-05-18) Lab Notebook: EasyBuild PostgreSQL Installation</li> <li>(2023-07-13) Lab Notebook: Ignoring Local Python Libraries</li> <li>(2023-07-18) Lab Notebook: AlphaFold 2.3.1 DEPRECATED</li> <li>(2023-07-18) Lab Notebook: MATLAB offload from localPC to HPCC</li> <li>(2023-08-02) Lab Notebook: Alternative Linux Shells on HPCC</li> <li>(2023-08-11) Lab Notebooks: Jupyter and MobaXterm--- Port-Forwarding Servers</li> <li>(2023-08-30) Lab Notebooks: Incident Report on System Slowdown due to I/O</li> <li>(2023-09-21) Lab Notebooks: Example home directory software install (with links to videos)</li> <li>(2023-11-03) Lab Notebook: Connecting to a Singularity container with VS Code</li> <li>(2023-11-09) Lab Notebook: Molpro Parallel</li> <li>(2023-11-10) Lab Notebook: AlphaFold 2.3.2 WorkInProgress</li> <li>(2023-11-21) Lab Notebook: Git in Research Spaces</li> <li>(2024-01-13) Lab Notebook: Sharing directories with other HPCC users</li> </ul>","tags":["lab notebook"]}]}